{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/00_Logo.jpg\"   style=\"width:116px;height:218px;\" >\n",
    "</figure>\n",
    "\n",
    "# **MACHINE LEARNING SPECIALIZATION**\n",
    "# Curso 1: Supervised Machine Learning: Regression and Classification\n",
    "# Semana 2: REGRESSION WITH MULTIPLE INPUT VARIABLES\n",
    "\n",
    "*A continuación se presentan los apuntes tomados de los videos de la Especialización de Machine Learning dictada por DeepLearning.AI y la Universidad de Stanford que se encuentra disponible en Cursera.org*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTIPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "    \n",
    "https://drive.google.com/file/d/1Y1ioFIGMFv2CyK1HFuo_cy0RtsfE6RmX/view?usp=sharing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta semana aprenderemos cómo hacer la regresión lineal mas rápida y mucho mas poderosa. Veremos la regresión lineal que usa no solo una característica, sino muchas características.  \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_03.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "En la versión original de la regresión lineal, tenías una sola característica $x$ (el tamaño de la casa) y eras capaz de predecir $y$ (el precio de la casa), así que el modelo era $f_{w,b}(x) = wx + b$.\n",
    "\n",
    "Pero ahora, ¿Qué pasa si no tenemos solo el tamaño de la casa para predecir el precio? Si también tenemos el número de habitaciones, el número de pisos, la edad de la casa en años, sugiere mas información para predecir el precio de la casa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_04.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Para introducir un poco de notación, vamos a usar las variables $x_1$, $x_2$, $x_3$, $x_4$ para denotar a las **características**. Vamos a introducir un poco mas de notación. Escribiremos $x_j$ para denotar las $j^{th}$ características que van de $j$ va de 1 a 4 en este caso. Vamos a usar $n$ para denotar el **número total de caracteristicas**. Por ejemplo en este ejemplo $n$=4. Cómo antes, $x^{(i)}$ se usa para denotar el $i^{th}$ **ejemplo de entrenamiento**. En este caso $x^{(i)}$ será una lista de 4 numeros, u otras veces lo llamaré **vectores** que incluyen todas las caracteristicas del $i^{th}$ ejemplo. Por ejemplo $x^{(2)}$ se referirá a las características del segundo ejemplo de entrenamiento, lo que será [1416, 3, 2, 40]. Tecnicamente escribí estos números en una fila, por lo que algunas veces lo llaman **vector fila**. Ya para referir a una característica especifica de $n$ ejemplos de entrenamiento usaré $x_j^{(i)}$. Por ejemplo, $x_3^{(2)}$ será el valor del segundo ejemplo para la tercer característica, lo que será igual a 2. Algunas veces, en orden de enfatizar que $x^{(i)}$ no un número sino un vector de características lo demarcaré con una flecha, pero es un signo opcional y no es necesario colocarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_05.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Ahora que tenemos múltiples características, veamos como luce el modelo. Previamente así definimos el modelo cuando $x$ era una sola característica, o sea un solo número, pero ahora con multiples características tenemos que definirla diferente. En su lugar, el modelo será $f_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b$. Concretamente, para la predicción de precios de las casas, un ejemplo sería como muestra la figura. Veamos como podemos interpretar estos parámetros. Si el modelo intenta predecir el precio de las casas en miles de dolares, podrías pensar en esto es igual a la base del precio de la casa, por ejemplo 80.000 dolares, sin tamaño, sin habitaciones, sin pisos y sin edad. Y puedes pensar el 0,1 como incrementar el precio en 100 dolares según los metros cuadrados. Y quizas, por cada habitación incrementa 4000 dolares, o aumeta 10000 según el piso o decrece de precio con la antigüedad. Y cuando se tienen n catacterísticas el modelo se verá como $f_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_06.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Aquí tenemos una vez mas la definición del modelo para $n$ caratceristicas. Lo que debemos hacer a continuación es introducir un poco mas de notación para escribir esta ecuación pero con una notación mas simple. Definamo $w$ (row) como una lista de números $w_1$, $w_2$, $...$ hasta $w_n$, que en matemáticas esto se llama un *vector*. Y algunas veces para denotar que es un vector, una lista de números, voy a dibujar una flecha arriba como simbolo opcional. A continuación, al igual que antes, $b$ es un único número y no un vector. Y entonces, este vector $w$ y el número $b$ son los **parámetros del modelo**. Dejeme también escribir $x$ como una lista, un vector fila, que contiene todas las características, $x_1$, $x_2$, $...$ hasta $x_n$. La flecha es opcional para denotar que se refiere a vectores. Entonces, con esta notación el modelo ahora puede ser escrito mas simplemente como $f_{w,b}(x)$ igual al vector $w$ por (donde este por significa dot product, producto escalar en algebra lineal) el vector $x + b$. \n",
    "\n",
    "¿Qué signica este producto escalar? Bueno, el producto escalar de dos vectores, de dos listas de números, $w$ y $x$, se calcula como tomando los correspondientes par de números $w_1$, $x_1$, multiplicando eso, mas el siguiente par de números $w_2$, $x_2$, multiplicando eso, hasta $w_n$, $x_n$ y finalmente sumamos el escalar $b$. Lo que nos da exactamente la expresión anterior. Así la notación del producto escalar, permite escribir el modelo de forma mas compacta y con menos caracteres. El nombre de este modelo de regresión de multiples carácterísticas se denomina **Regresión Lineal Múltiple**. Esto en contraste con la regresión lineal que solo tenía una caracteristica. Y por cierto, podrías pensar que este algoritmo se refiere a la regresión multivariable, pero eso se refiere a algo mas, que no vamos a usar aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA </b></font>\n",
    "\n",
    "En el conjunto de entrenamiento siguiente, ¿cuál es $x_1^{(4)}$?\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./imagenes/C1_W2_Página_04.jpg\"   style=\"width:250px;height:140px;\" >\n",
    "</figure>\n",
    " \n",
    "Escriba el número que aparece a continuación (se trata de un número entero como 123, sin puntos decimales).\n",
    "***\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta: 852.\n",
    "    \n",
    "$x_1^{(4)}$ es la primera característica (primera columna de la tabla) del cuarto ejemplo de entrenamiento (cuarta fila de la tabla).\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso es todo para regresión lineal para multiples caracteristicas, tambien llamada regresión lineal múltiple. Para implementar esto es necesario conocer la vectorización, que hace mas simple esta implementación y muchos otros algoritmos. En el próximo video veremos que es la vectorización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "    \n",
    "https://drive.google.com/file/d/1UkTf4DCVYD3dpZYrKN4RDjgUH6UU9wo9/view?usp=sharing    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este vídeo, verás una idea muy útil llamada **vectorización**. Cuando estás implementando un algoritmo de aprendizaje, el uso de la vectorización hará que tu código sea más corto y también que se ejecute de manera mucho más eficiente. Aprender a escribir código vectorizado te permitirá también aprovechar las modernas librerías de álgebra lineal numérica, así como quizás incluso el hardware GPU, que significa unidad de procesamiento gráfico. Se trata de un hardware diseñado objetivamente para acelerar los gráficos de tu ordenador, pero resulta que puede utilizarse cuando escribes código vectorizado para ayudarte también a ejecutar tu código mucho más rápidamente. Veamos un ejemplo concreto de lo que significa la vectorización.\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_08.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Aquí tenemos un ejemplo con los parámetros $w$ y $b$, donde $w$ es un vector con tres números, y también tienes un vector de características $x$ con también tres números. Aquí $n$ es igual a 3. Fíjate que en el álgebra lineal, el índice o el conteo comienza desde 1 y por eso el primer valor tiene el subíndice $w_1$ y $x_1$. En el código de Python, puedes definir estas variables $w$, $b$ y $x$ usando arrays como este. Aquí, en realidad estoy utilizando una biblioteca de álgebra lineal numérica en Python llamada **NumPy**, que es, con mucho, la biblioteca de álgebra lineal numérica más utilizada en Python y en el aprendizaje automático. Debido a que en Python, la indexación de los arrays mientras se cuenta en arrays comienza desde 0, accederías al primer valor de $w$ usando $w[0]$. El segundo valor usando $w[1]$, y el tercero y usando $w[2]$. La indexación aquí, va de 0, 1 a 2 en lugar de 1, 2 a 3. Del mismo modo, para acceder a las características individuales de $x$, utilizarás $x[0]$, $x[1]$ y $x[2]$. Muchos lenguajes de programación, incluyendo Python, comienzan a contar desde 0 en lugar de 1. \n",
    "\n",
    "Ahora, veamos una implementación sin vectorización para calcular la predicción del modelo. En los códigos, se verá así. Tomas cada parámetro $w$ y lo multiplicas por su característica asociada. Ahora, usted podría escribir su código como este, pero lo que si $n$ no es tres pero en cambio $n$ es un 100 o un 100.000 es tanto ineficiente para usted el código e ineficiente para su computadora para calcular. \n",
    "\n",
    "Aquí hay otra manera. Sin usar la vectorización pero usando un bucle for. En matemáticas, puedes usar un operador de suma para sumar todos los productos de $w_j$ y $x_j$ para $j$ igual a 1 hasta $n$. Entonces citaré la suma que agregas $b$ al final. Para sumar va desde $j$ igual a 1 hasta $n$ inclusive. Para $n$ igual a 3, $j$ por lo tanto va de 1, 2 a 3. En el código, puedes inicializar después de 0. Entonces para $j$ en el rango de 0 a $n$, esto realmente hace que $j$ vaya de 0 a n menos 1. De 0, 1 a 2, puedes entonces añadir a $f$ el producto de $w_j$ por $x_j$. Finalmente, fuera del bucle for, se añade $b$. Observa que en Python, el rango 0 a $n$ significa que $j$ va de 0 a n menos 1 y no incluye NSL. Esto se escribe rango $n$ en Python. Pero en este vídeo, he añadido un 0 aquí sólo para enfatizar que empieza desde 0. Aunque esta implementación es un poco mejor que la primera, sigue sin utilizar la factorización, y no es eso eficiente. \n",
    "\n",
    "Ahora, veamos cómo se puede hacer esto usando la vectorización. Esta es la expresión matemática de la función $f$, que es el producto punto de $w$ y $x$ más $b$, y ahora puedes implementar esto con una sola línea de código calculando $f$ es igual a np punto punto, dije punto punto porque el primer punto es el periodo y el segundo punto es la función o el método llamado dot. Pero es $f = np.dot(w,x)$ y esto implementa los productos matemáticos de punto entre los vectores $w$ y $x$. Entonces, finalmente, puedes agregarle $b$ al final. Esta función NumPy dot es una implementación vectorizada de la operación de producto punto entre dos vectores y especialmente cuando n es grande, esto se ejecutará mucho más rápido que los dos ejemplos de código anteriores. \n",
    "\n",
    "Quiero enfatizar que la vectorización tiene dos beneficios distintos. \n",
    "* Primero, hace que el código sea más corto, ahora es sólo una línea de código. ¿No es genial? \n",
    "* En segundo lugar, también hace que su código se ejecute mucho más rápido que cualquiera de las dos implementaciones anteriores que no utilizaron la vectorización. \n",
    "\n",
    "La razón por la que la implementación vectorizada es mucho más rápida está detrás de las escenas. La función NumPy dot es capaz de utilizar el hardware paralelo en tu ordenador y esto es cierto tanto si estás ejecutando esto en un ordenador normal, es decir, en una CPU de ordenador normal o si estás utilizando una GPU, una unidad de procesador gráfico, que a menudo se utiliza para acelerar los trabajos de aprendizaje automático. La capacidad de la función NumPy punto para utilizar el hardware paralelo hace que sea mucho más eficiente que el bucle for o el cálculo secuencial que vimos anteriormente.  \n",
    "\n",
    "Ahora, esta versión es mucho más práctica cuando n es grande porque no estás escribiendo $w_0$ veces $x_0$ más $w_1$ veces $x_1$ más pensamientos de términos adicionales como habrías tenido para la versión anterior. Pero aunque esto ahorra mucho en el tecleo, todavía no es tan eficiente computacionalmente porque todavía no utiliza la vectorización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA </b></font>\n",
    "\n",
    "¿Cuál de las siguientes es una implementación vectorizada para calcular la predicción de un modelo de regresión lineal?\n",
    " \n",
    "* f = np.dot(w,x) + b\n",
    "* f = 0\n",
    "  Para j en el rango(n):\n",
    "  f = f + w[j] * x[j]\n",
    "  f=f + b\n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta: f = np.dot(w,x) + b.\n",
    "    \n",
    "Esta función de numpy utiliza el hardware paralelo para calcular eficientemente el producto punto.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para recapitular, la vectorización hace que tu código sea más corto, por lo que es de esperar que sea más fácil de escribir y más fácil de leer para ti o para otros, y también hace que se ejecute mucho más rápido. Pero honestamente, esta magia detrás de la vectorización que hace que esto se ejecute mucho más rápido. Echemos un vistazo a lo que tu ordenador está haciendo realmente entre bastidores para hacer que el código vectorizado se ejecute mucho más rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "    \n",
    "https://drive.google.com/file/d/1DphE8KeknmjV5j8QxMClJ32ZrCbpgtAC/view?usp=sharing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerdo que cuando aprendí por primera vez sobre la vectorización, pasé muchas horas en mi ordenador tomando una versión no vectorizada de un algoritmo ejecutándolo, para ver cuánto tiempo se ejecutaba, y luego ejecutando una versión vectorizada del código y viendo cuánto más rápido se ejecutaba, y simplemente pasé horas jugando con eso. Francamente, por mi mente que el mismo algoritmo vectorizado se ejecutaría mucho más rápido. Se sentía casi como un truco de magia para mí. En este video, vamos a averiguar cómo funciona realmente este truco de magia. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_10.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Vamos a echar un vistazo más profundo a cómo una implementación vectorizada puede funcionar en su computadora detrás de las escenas. Veamos este bucle for. El bucle for así se ejecuta sin vectorización. Si $j$ va de 0 a, digamos, 15, este trozo de código realiza las operaciones una tras otra. En la primera marca de tiempo que voy a escribir como $t_0$. Primero opera sobre los valores del índice 0. En el siguiente paso de tiempo, calcula los valores correspondientes al índice 1 y así sucesivamente hasta el paso 15, donde calcula eso. En otras palabras, calcula estos cálculos paso a paso, un paso tras otro.  \n",
    "\n",
    "En cambio, esta función en NumPy se implementa en el hardware del ordenador con vectorización. El ordenador puede obtener todos los valores de los vectores $w$ y $x$, y en un solo paso, multiplica cada par de $w$ y $x$ entre sí todos al mismo tiempo en paralelo. A continuación, el ordenador toma estos 16 números y utiliza un hardware especializado para sumarlos de forma muy eficiente, en lugar de tener que realizar distintas sumas una tras otra para sumar estos 16 números. Esto significa que los códigos con vectorización pueden realizar cálculos en mucho menos tiempo que los códigos sin vectorización. Esto es más importante cuando se ejecutan algoritmos en grandes conjuntos de datos o se intenta entrenar grandes modelos, lo que suele ocurrir con el aprendizaje automático. Por eso, ser capaz de vectorizar las implementaciones de los algoritmos de aprendizaje, ha sido un paso clave para conseguir que los algoritmos de aprendizaje se ejecuten de forma eficiente, y por lo tanto escalen bien a grandes conjuntos de datos en los que muchos algoritmos modernos de aprendizaje automático tienen que operar ahora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_11.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Ahora, veamos un ejemplo concreto de cómo esto ayuda a implementar la regresión lineal múltiple y esta regresión lineal con múltiples características de entrada. Digamos que tienes un problema con 16 características y 16 parámetros, $w_1$ a $w_16$, además del parámetro $b$. Lo calculas 16 términos derivados para estos 16 pesos y códigos, tal vez almacenes los valores de $w$ y $d$ en dos np.arrays, con $d$ almacenando los valores de las derivadas. Para este ejemplo, sólo voy a ignorar el parámetro $b$. Ahora, usted quiere calcular una actualización para cada uno de estos 16 parámetros. $w_j$ se actualiza a $w_j$ menos la tasa de aprendizaje, digamos 0,1, por $d_j$, para $j$ de 1 a 16. \n",
    "\n",
    "En código, sin vectorización, estarías haciendo algo como esto. Actualizar $w_1$ para que sea $w_1$ menos la tasa de aprendizaje 0,1 veces $d_1$, a continuación, actualizar $w_2$ de forma similar, y así sucesivamente hasta $w_16$, actualizado como $w_16$ menos 0,1 veces $d_16$. En código sin vectorización, puede utilizar un bucle for como este para $j$ en el rango 0 a 16, que de nuevo va de 0 a 15, dijo $w_j$ es igual a $w_j$ menos 0,1 veces $d_j$.  \n",
    "\n",
    "En cambio, con la vectorización, puedes imaginar el hardware de procesamiento paralelo del ordenador así. Toma los 16 valores del vector $w$ y resta en paralelo, 0,1 veces los 16 valores del vector $d$, y asigna los 16 cálculos de vuelta a $w$ todo al mismo tiempo y todo en un solo paso. En el código, se puede implementar esto de la siguiente manera, $w$ se asigna a $w$ menos 0,1 veces $d$. Detrás de las escenas, el ordenador toma estas matrices NumPy, $w$ y $d$, y utiliza el hardware de procesamiento en paralelo para llevar a cabo todos los 16 cálculos de manera eficiente. Usando una implementación vectorizada, deberías obtener una implementación mucho más eficiente de la regresión lineal. Tal vez la diferencia de velocidad no sea enorme si tienes 16 características, pero si tienes miles de características y quizás conjuntos de entrenamiento muy grandes, este tipo de implementación vectorizada hará una gran diferencia en el tiempo de ejecución de tu algoritmo de aprendizaje. Podría ser la diferencia entre que los códigos terminen en uno o dos minutos, o que tarden muchas horas en hacer lo mismo. \n",
    "\n",
    "En el laboratorio opcional que sigue a este vídeo, ves una introducción a una de las librerías de Python más usadas y de Machine Learning, que ya hemos tocado en este vídeo llamada NumPy. Verás cómo se crean vectores codificados y estos vectores o listas de números se llaman arrays NumPy, y también verás cómo tomar el producto punto de dos vectores usando una función NumPy llamada dot. También puedes ver cómo el código vectorizado, como el uso de la función dot, puede correr mucho más rápido que un bucle for. De hecho, podrás cronometrar este código tú mismo, y con suerte verás que se ejecuta mucho más rápido. Este laboratorio opcional introduce una buena cantidad de nueva sintaxis de NumPy, así que cuando leas el laboratorio opcional, por favor, sigue sintiendo que tienes que entender todo el código de inmediato, pero puedes guardar este cuaderno y usarlo como referencia para mirar cuando estés trabajando con datos almacenados en arrays de NumPy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA </b></font>\n",
    "\n",
    "¿Cuál de las siguientes es una implementación vectorizada para calcular la predicción de un modelo de regresión lineal?\n",
    " \n",
    "* f = np.dot(w,x) + b\n",
    "* f = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b\n",
    "* f = 0\n",
    "  for j in range(n):\n",
    "  f = f + w[j] * x[j]\n",
    "  f = f + b\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta: f = np.dot(w,x) + b.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Felicidades por haber terminado este video sobre vectorización. Has aprendido una de las técnicas más importantes y útiles en la implementación de algoritmos de aprendizaje automático. En el próximo vídeo, pondremos las matemáticas de la regresión lineal múltiple junto con la vectorización, de manera que influirás en el descenso de gradiente para la regresión lineal múltiple con vectorización. Pasemos al siguiente vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent for multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "\n",
    "https://drive.google.com/file/d/1YjWU4UT9illr9ByivyFoyUirBmE2Hyzd/view?usp=sharing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usted ha aprendido acerca de los descensos de gradiente sobre la regresión lineal múltiple y también la vectorización. Vamos a poner todo junto para implementar el descenso de gradiente para la regresión lineal múltiple con vectorización. Esto sería genial. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_13.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Repasemos rápidamente cómo es la regresión lineal múltiple. Usando nuestra notación anterior, veamos cómo se puede escribir más sucintamente usando la notación vectorial. Tenemos los parámetros $w_1$ a $w_n$, así como $b$. Pero en lugar de pensar en $w_1$ a $w_n$ como números separados, es decir, parámetros separados, vamos a empezar a recoger todos los $w$ en un vector $w$ de modo que ahora $w$ es un vector de longitud $n$. Sólo vamos a pensar en los parámetros de este modelo como un vector $w$, así como $b$, donde $b$ sigue siendo un número igual que antes. Mientras que antes teníamos que encontrar la regresión lineal múltiple así, ahora usando la notación vectorial, podemos escribir el modelo como $f_{w,b}(x)$ es igual al vector $w$ dot producto con el vector $x$ más $b$. Recuerde que este punto aquí significa producto. Nuestra función de costo puede ser definida como $J$ de $w_1$ a $w_n$, $b$. Pero en lugar de sólo pensar en $J$ como una función de estos y diferentes parámetros $w_j$, así como $b$, vamos a escribir $J$ como una función del vector de parámetros $w$ y el número $b$. Este $w_1$ a wn se sustituye por este vector $w$ (row) (señala en violeta) y $J$ ahora toma esta entrada del vector $w$ y un número $b$ y devuelve un número. Esto es lo que parece el descenso de gradiente.  \n",
    "\n",
    "Vamos a actualizar repetidamente cada parámetro $w_j$ para ser $w_j$ menos $\\alpha$ veces la derivada del coste $J$, donde $J$ tiene los parámetros $w_1$ a $w_n$ y $b$. Una vez más, acabamos de escribir esto como $J$ del vector $w$ y el número $b$. Vamos a ver lo que parece cuando se implementa el descenso por gradiente $y$, en particular, vamos a echar un vistazo a la derivada término. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_14.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Veremos que el descenso de gradiente se vuelve un poco diferente con múltiples características en comparación con una sola característica. Esto es lo que teníamos cuando teníamos el descenso de gradiente con una característica. Teníamos una regla de actualización para $w$ y una regla de actualización separada para $b$. Esperemos que esto le resulte familiar. Este término aquí es la derivada de la función de costo $J$ con respecto al parámetro $w$. Del mismo modo, tenemos una regla de actualización para el parámetro $b$, con la regresión univariante, tuvimos una sola característica. Llamamos a esa característica $x^{(i)}$ sin ningún subíndice.  \n",
    "\n",
    "Ahora, aquí hay una nueva notación para cuando tenemos $n$ características, donde $n$ es dos o más. Tenemos esta regla de actualización para el descenso de gradiente. Actualizar $w_1$ para ser $w_1-\\alpha$ veces esta expresión aquí (señala la ecuación de la derecha en amarillo) y esta fórmula es en realidad la derivada del costo $J$ con respecto a $w_1$. La fórmula para la derivada de $J$ con respecto a $w_1$ a la derecha se ve muy similar al caso de una característica a la izquierda. El término de error todavía toma una predicción $f(x)$ menos el objetivo $y$. Una diferencia es que $w$ y $x$ son ahora vectores y así como $w$ a la izquierda se ha convertido ahora en $w_1$ aquí a la derecha, $x^{(i)}$ aquí a la izquierda es ahora en cambio $x_1^{(i)}$ aquí a la derecha y esto es sólo para $J$ igual a 1.\n",
    "\n",
    "Para la regresión lineal múltiple, tenemos $J$ que va de 1 a $n$ y así vamos a actualizar los parámetros $w_1$, $w_2$, todo el camino hasta $w_n$, y luego como antes, vamos a actualizar $b$. Si usted implementa esto, se obtiene el descenso de gradiente para la regresión múltiple. Eso es todo para el descenso de gradiente para la regresión múltiple.  \n",
    "\n",
    "Antes de pasar de este video, quiero hacer un rápido aparte o una nota lateral rápida en una forma alternativa para encontrar $w$ y $b$ para la regresión lineal. Este método se llama la **ecuación normal**. Mientras que resulta que el descenso de gradiente es un gran método para minimizar la función de costo $J$ para encontrar $w$ y $b$, hay otro algoritmo que funciona sólo para la regresión lineal y casi ninguno de los otros algoritmos que se ven en esta especialización para resolver para $w$ y $b$ y este otro método no necesita un algoritmo de descenso de gradiente iterativo. Llamado el método de la ecuación normal, resulta que es posible utilizar una biblioteca de álgebra lineal avanzada para resolver simplemente para $w$ y $b$ todo en un objetivo sin iteraciones.  \n",
    "\n",
    "Algunas desventajas del método de la ecuación normal son:\n",
    "\n",
    "* primero a diferencia del descenso de gradiente, este no es generalizado a otros algoritmos de aprendizaje, como el algoritmo de regresión logística que aprenderás la próxima semana o las redes neuronales u otros algoritmos que verás más adelante en esta especialización. \n",
    "\n",
    "* El método de la ecuación normal también es bastante lento si el número de características y este grande.\n",
    "\n",
    "Casi ningún practicante de aprendizaje automático debería implementar el método de la ecuación normal por sí mismo, pero si estás usando una biblioteca de aprendizaje automático madura y llamas a la regresión lineal, hay una posibilidad de que en el backend, esté usando esto para resolver para $w$ y $b$. Si alguna vez estás en la entrevista de trabajo y escuchas el término ecuación normal, eso es a lo que se refiere. No te preocupes por los detalles de cómo funciona la ecuación normal. Sólo ten en cuenta que algunas bibliotecas de aprendizaje automático pueden utilizar este complicado método en el back-end para resolver para $w$ y $b$. Sólo ten en cuenta que algunas bibliotecas de aprendizaje automático pueden utilizar este complicado método en el back-end para resolver para $w$ y $b$. Pero para la mayoría de los algoritmos de aprendizaje, incluyendo la forma de implementar la regresión lineal por ti mismo, los descensos de gradiente ofrecen una mejor manera de hacer el trabajo. \n",
    "\n",
    "En el laboratorio opcional que sigue a este vídeo, verás cómo definir un modelo de regresión múltiple codificado y también cómo calcular la predicción $f(x)$. También verás cómo calcular el coste e implementar el descenso de gradiente para un modelo de regresión lineal múltiple. Esto será utilizando la biblioteca NumPy de Python. Si algo del código parece muy nuevo, está bien, pero no dudes en echar un vistazo también al laboratorio opcional anterior que introduce NumPy y la vectorización para refrescar las funciones de NumPy y cómo implementar esos códigos. Eso es todo. Ahora conoces la regresión lineal múltiple. Este es probablemente el algoritmo de aprendizaje más utilizado en la actualidad. Pero hay más. Con sólo unos pocos trucos, como elegir y escalar las características adecuadamente y también elegir la forma de aprendizaje $\\alpha$ adecuadamente, realmente harías que esto funcionara mucho mejor. Sólo faltan unos pocos vídeos para esta semana. Pasemos al siguiente vídeo para ver esos pequeños trucos que te ayudarán a que la regresión lineal múltiple funcione mucho mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz: Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 1 </b></font>\n",
    "\n",
    "En el conjunto de entrenamiento siguiente, ¿cuál es $x_4^{(3)}$? Escriba el número que aparece a continuación (se trata de un número entero como 123, sin puntos decimales).\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./imagenes/C1_W2_Página_04.jpg\"   style=\"width:250px;height:140px;\" >\n",
    "</figure>\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 2 </b></font>\n",
    "\n",
    "¿Cuáles de las siguientes son las posibles ventajas de la vectorización? Elija la mejor opción.\n",
    "\n",
    "* Hace que tu código se ejecute más rápido\n",
    "* Puede hacer que su código sea más corto \n",
    "* Permite que su código se ejecute más fácilmente en el hardware de cálculo paralelo\n",
    "* Todas las opciones anteriores \n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 3 </b></font>\n",
    "\n",
    "¿Verdadero/Falso? Para hacer que el descenso de gradiente converja el doble de rápido, una técnica que casi siempre funciona es duplicar la tasa de aprendizaje $\\alpha$. \n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta 1: 30.\n",
    "\n",
    "Respuesta 2: Todas las opciones anteriores.\n",
    "    \n",
    "Respuesta 3: Falso.\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT DESCENT IN PRACTICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "\n",
    "https://drive.google.com/file/d/1Opqg3VOFLM8EKgxYFEQBj-gNMWX59fSw/view?usp=sharing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que bienvenido de nuevo. Vamos a echar un vistazo a algunas técnicas que hacen que el gran inter sentido funcione mucho mejor. En este video verás una técnica llamada \"feature scaling\" que permitirá que el descenso de gradiente se ejecute mucho más rápido. Vamos a empezar por echar un vistazo a la relación entre el tamaño de una característica, es decir, lo grande que son los números de esa característica y el tamaño de su parámetro asociado. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_17.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Como ejemplo concreto, vamos a predecir el precio de una casa utilizando dos características $x_1$, el tamaño de la casa, y $x_2$, el número de habitaciones. Digamos que $x_1$ suele oscilar entre 300 y 2000 pies cuadrados. Y $x_2$ en el conjunto de datos oscila entre 0 y 5 dormitorios. Por tanto, en este ejemplo, $x_1$ toma un rango de valores relativamente grande y $x_2$ toma un rango de valores relativamente pequeño. Ahora tomemos un ejemplo de una casa que tiene un tamaño de 2000 pies cuadrados tiene cinco dormitorios y un precio de 500k o $500,000. \n",
    "\n",
    "Para este ejemplo de entrenamiento, cuáles cree que son los valores razonables para el tamaño de los parámetros $w_1$ y $w_2$? Bien, veamos un posible conjunto de parámetros. Digamos que $w_1$ es 50 y $w_2$ es 0,1 y $b$ es 50 para los fines de la discusión. \n",
    "\n",
    "Así que en este caso el precio estimado en miles de dólares es 100.000k aquí más 0,5 k más 50 k. Lo que supone algo más de 100 millones de dólares. Así que eso está claramente muy lejos del precio real de 500.000 dólares. Y por lo que este no es un muy buen conjunto de opciones de parámetros para $w_1$ y $w_2$.  \n",
    "\n",
    "Ahora echemos un vistazo a otra posibilidad. Digamos que $w_1$ y $w_2$ fueran al revés. $w_1$ es 0,1 y $w_2$ es 50 y b sigue siendo también 50. En esta elección de $w_1$ y $w_2$, $w_1$ es relativamente pequeño y $w_2$ es relativamente grande, 50 es mucho mayor que 0,1. Así que aquí el precio previsto es 0,1 por 2000 más 50 por cinco más 50. El primer término se convierte en 200k, el segundo en 250k, y el más 50. Así que esta versión del modelo predice un precio de 500.000 dólares, que es una estimación mucho más razonable y resulta ser el mismo precio que el verdadero de la casa. \n",
    "\n",
    "Así que espero que te des cuenta de que cuando un posible rango de valores de una característica es grande, como el tamaño y los pies cuadrados que va todo el camino hasta 2000. Es más probable que un buen modelo aprenda a elegir un valor de parámetro relativamente pequeño, como 0,1. Del mismo modo, cuando los posibles valores de la característica son pequeños, como el número de dormitorios, entonces un valor razonable para sus parámetros será relativamente grande, como 50. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_18.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Entonces, ¿cómo se relaciona esto con el descenso del gradiente? Bien, echemos un vistazo al diagrama de dispersión de las características, donde el tamaño de los pies cuadrados es el eje horizontal $x_1$ y el número de dormitorios exudados está en el eje vertical. Si graficas los datos de entrenamiento, te darás cuenta de que el eje horizontal está en una escala mucho más grande o en un rango de valores mucho más grande en comparación con el eje vertical.  \n",
    "\n",
    "A continuación, veamos cómo podría verse la función de coste en un gráfico de contorno. Podríamos ver un gráfico de contorno en el que el eje horizontal tiene un rango mucho más estrecho, digamos entre cero y uno, mientras que el eje vertical toma valores mucho más grandes, digamos entre 10 y 100. Así que los contornos forman óvalos o elipses y son cortos en un lado y más largos en el otro. Y esto se debe a que un cambio muy pequeño en $w_1$ puede tener un impacto muy grande en el precio estimado y eso es un impacto muy grande en el coste $J$. Porque $w_1$ tiende a multiplicarse por un número muy grande, el tamaño y los pies cuadrados. Por el contrario, se necesita un cambio mucho mayor en w2 para cambiar mucho las predicciones. Y por lo tanto, pequeños cambios en $w_2$, no cambian la función de costes casi tanto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_19.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Entonces, ¿dónde nos deja esto? Esto es lo que podría terminar sucediendo si se ejecuta el descenso del gradiente, si usted fuera a utilizar sus datos de entrenamiento como es. Debido a que los contornos son tan altos y delgados el descenso de gradiente puede terminar rebotando de un lado a otro durante mucho tiempo antes de que finalmente pueda encontrar su camino hacia el mínimo global. En situaciones como ésta, una cosa útil es escalar las características. Esto significa realizar alguna transformación de los datos de entrenamiento, de modo que $x_1$, por ejemplo, pueda oscilar entre 0 y 1, y $x_2$ también pueda oscilar entre 0 y 1. De este modo, los puntos de datos se parecen más a esto, y se puede observar que la escala del gráfico de la parte inferior es ahora bastante diferente de la de la parte superior. \n",
    "\n",
    "El punto clave es que la re-escala $x_1$ y $x_2$ están ahora tomando rangos de valores comparables entre sí. Y si se ejecuta el descenso de gradiente en una función de coste para encontrar en este, re-escala $x_1$ y $x_2$ utilizando estos datos transformados, a continuación, los contornos se verá más como esto más como círculos y menos alto y delgado. Y el descenso por gradiente puede encontrar un camino mucho más directo hacia el mínimo global.  \n",
    "\n",
    "Así que para recapitular, cuando tienes diferentes características que toman rangos de valores muy diferentes, puede hacer que el descenso de gradiente se ejecute lentamente, pero re-escalando las diferentes características para que todas tomen rangos de valores comparables. porque la velocidad, la actualización y el disenso significativamente. ¿Cómo se hace esto realmente? Vamos a echar un vistazo a eso en el siguiente vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "\n",
    "https://drive.google.com/file/d/1CHn0yJClCQObfz0fJnI5ymT1_HD-3pS-/view?usp=sharing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo se puede implementar el escalado de características, para tomar características que toman rangos de valores muy diferentes y capacitarlas para que tengan rangos de valores comparables entre sí. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_21.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "¿Cómo se **escalan las características**? Bien, si $x_1$ oscila entre 3-2.000, una forma de obtener una versión a escala de $x_1$ es tomar cada valor original de $x_1$ y dividirlo por 2.000, el máximo del rango. La escala $x_1$ oscilará entre 0,15 y uno. Del mismo modo, dado que $x_2$ oscila entre 0 y 5, se puede calcular una versión de escala de $x_2$ tomando cada valor original de $x_2$ y dividiéndolo por cinco, que es de nuevo el máximo. Así que la escala es $x_2$ ahora irá de 0 a 1. Si se traza la escala de $x_1$ y $x_2$ en un gráfico, podría verse así. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_22.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Además de dividir por el máximo, también puedes hacer lo que se llama **normalización de la media**. Lo que parece es que se comienza con las características originales y luego se re-escala para que ambos se centran en torno a cero. Mientras que antes sólo tenían valores mayores que cero, ahora tienen valores negativos y positivos que pueden estar generalmente entre uno negativo y más uno. Para calcular la normalización de la media de $x_1$, primero encuentre el promedio, también llamado la media de $x_1$ en su conjunto de entrenamiento, y llamemos a esta media $\\mu_1$, siendo éste el alfabeto griego Mu. Por ejemplo, puede encontrar que la media de la característica 1, $\\mu_1$ es de 600 pies cuadrados. Tomemos cada $x_1$, restemos la media $\\mu_1$, y luego dividamos por la diferencia 2.000 menos 300, donde 2.000 es el máximo y 300 el mínimo, y si haces esto, obtienes que la $x_1$ normalizada oscile entre 0,18-0,82 negativo. Del mismo modo, para la media normalizada de $x_2$, se puede calcular la media de la característica 2. Por ejemplo, $\\mu_2$ puede ser 2,3. A continuación, puede tomar cada $x_2$, restar $\\mu_2$ y dividir por 5 menos 0. De nuevo, el máximo 5 menos la media, que es 0. La media normalizada de $x_2$ ahora oscila entre 0,46-0 54 negativo. Si graficas los datos de entrenamiento usando la media normalizada de $x_1$ y $x_2$, podría verse así. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_23.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Hay un último método común de re-escalado llamado **normalización Z-score**. Para implementar la normalización de la puntuación Z, necesita calcular algo llamado la desviación estándar de cada característica. Si no sabes lo que es la desviación estándar, no te preocupes, no necesitarás saberlo para este curso. O si has oído hablar de la distribución normal o de la curva en forma de campana, a veces también llamada distribución de Gauss, este es el aspecto de la desviación estándar de la distribución normal. Pero si no has oído hablar de esto, tampoco tienes que preocuparte. Pero si usted sabe lo que es la desviación estándar, entonces para implementar una normalización de la puntuación Z, primero se calcula la media $\\mu$, así como la desviación estándar, que a menudo se denota por el alfabeto griego minúsculo Sigma de cada característica. Por ejemplo, tal vez la característica 1 tiene una desviación estándar de 450 y una media de 600, entonces para normalizar la puntuación Z de $x_1$, tome cada $x_1$, reste $\\mu_1$, y luego divida por la desviación estándar, que voy a denotar como $\\sigma_2$. Lo que puede encontrar es que el Z-score normalizado $x_1$ ahora oscila entre 0,67-3,1 negativo. Del mismo modo, si calcula que la desviación estándar de la segunda característica es de 1,4 y la media es de 2,3, entonces puede calcular $x_2$ menos $\\mu_2$ dividido por $\\sigma_2$, y en este caso, la puntuación Z normalizada por $x_2$ podría oscilar ahora entre 1,6-1,9 negativo. Si se trazan los datos de entrenamiento sobre los normalizados $x_1$ y $x_2$ en un gráfico, podría tener este aspecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_24.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Como regla general, cuando se realiza el escalado de características, es posible que desee obtener las características para ir desde tal vez en cualquier lugar alrededor de uno negativo a algún lugar alrededor de más uno para cada característica $x$. Pero estos valores, uno negativo y más uno puede ser un poco flojo. Si las características van de tres negativo a más tres o de 0,3 negativo a más 0,3, todo esto está bien. Si usted tiene una característica $x_1$ que termina siendo entre cero y tres, eso no es un problema. Puedes cambiar la escala si quieres, pero si no la cambias, también debería funcionar bien. O si usted tiene una característica diferente, $x_2$, cuyos valores están entre negativo 2 y más 0,5, de nuevo, eso está bien, no hay daño re-escala, pero podría ser bien si lo dejas solo también. Pero si otra característica, como $x_3$ aquí, oscila entre 100 negativo y más 100, entonces esto toma un rango muy diferente de valores, digamos algo de alrededor de uno negativo a más uno. Probablemente sea mejor reescalar esta característica $x_3$ para que vaya de algo más cercano a uno negativo a más uno. Del mismo modo, si tiene una característica $x_4$ que toma valores realmente pequeños, digamos entre 0,001 negativo y más 0,001, entonces estos valores son muy pequeños. Eso significa que también es conveniente reescalar. Por último, ¿qué pasa si su característica $x_5$, como las mediciones de los pacientes de un hospital por la temperatura oscila entre 98,6-105 grados Fahrenheit? En este caso, estos valores están en torno a 100, lo que es bastante grande en comparación con otras características de escala, y esto realmente hará que el descenso de gradiente se ejecute más lentamente. En este caso, es probable que el cambio de escala de las características ayude. Casi nunca hay ningún daño en llevar a cabo la re-escalada de características. En caso de duda, le animo a que lo haga. Eso es todo para el escalado de características. Con esta pequeña técnica, a menudo serás capaz de conseguir que el descenso de gradiente se ejecute mucho más rápido. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA </b></font>\n",
    "\n",
    "¿Cuál de los siguientes es un paso válido utilizado durante el escalado de características? \n",
    "- Multiplicar cada valor por el valor máximo de la característica\n",
    "- Dividir cada valor por el valor máximo de la característica \n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta: Dividir cada valor por el valor máximo de la característica.\n",
    "\n",
    "Al dividir todos los valores por el máximo, el nuevo rango máximo de las características reescaladas es ahora 1 (y todos los demás valores reescalados son menores que 1).\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso es el escalado de características. Con o sin escalado de características, cuando ejecutas el descenso de gradiente, ¿cómo puedes saber, cómo puedes comprobar si el descenso de gradiente está realmente funcionando? Si está encontrando el mínimo global o algo cercano a él. En el siguiente video, vamos a echar un vistazo a cómo reconocer si el descenso de gradiente está convergiendo, y luego en el video después de eso, esto conducirá a la discusión de cómo elegir una buena tasa de aprendizaje para el descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking gradient descent for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "\n",
    "https://drive.google.com/file/d/1LPbS3m8zVhGFRNWOFXrLLmocO2qjQV6v/view?usp=sharing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se ejecuta el descenso de gradiente, ¿cómo se puede saber si está convergiendo? Es decir, si te está ayudando a encontrar parámetros cercanos al mínimo global de la función de coste. Si aprendemos a reconocer el aspecto de una implementación del descenso de gradiente que funciona bien, también podremos, en un vídeo posterior, elegir mejor una buena tasa de aprendizaje $\\alpha$. Echemos un vistazo.\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_26.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Como recordatorio, aquí está la regla del descenso de gradiente. Una de las opciones clave es la elección de la tasa de aprendizaje $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_27.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Aquí hay algo que suelo hacer para asegurarme de que el descenso por gradiente está funcionando bien. Recordemos que el trabajo del descenso de gradiente es encontrar los parámetros $w$ y $b$ que, con suerte, minimizan la función de coste $J$. Lo que a menudo hago es trazar la función de coste $J$, que se calcula en el conjunto de entrenamiento, y trazo el valor de $J$ en cada iteración del descenso de gradiente. Recuerde que cada iteración significa después de cada actualización simultánea de los parámetros $w$ y $b$. En este gráfico, el eje horizontal es el número de iteraciones de descenso de gradiente que ha ejecutado hasta ahora. Usted puede obtener una curva que se parece a esto. \n",
    "\n",
    "Observa que el eje horizontal es el número de iteraciones de descenso de gradiente y no un parámetro como $w$ o $b$. Esto difiere de los gráficos anteriores que has visto donde el eje vertical era el coste $J$ y el eje horizontal era un único parámetro como $w$ o $b$. Esta curva también se llama curva de aprendizaje. Ten en cuenta que hay algunos tipos diferentes de curvas de aprendizaje utilizadas en el aprendizaje automático, y verás algunos de los tipos más adelante en este curso también. Concretamente, si usted mira aquí en este punto de la curva, esto significa que después de haber ejecutado el descenso de gradiente para 100 iteraciones, lo que significa 100 actualizaciones simultáneas de los parámetros, usted tiene algunos valores aprendidos para $w$ y $b$. Si usted calcula el costo $J(w,b)$ para los valores de $w$ y $b$, los que obtuvo después de 100 iteraciones, se obtiene este valor para el costo $J$. Ese es este punto en el eje vertical. Este punto aquí corresponde al valor de $J$ para los parámetros que obtuviste después de 200 iteraciones de descenso de gradiente. \n",
    "\n",
    "Mirar este gráfico te ayuda a ver cómo cambia tu coste $J$ después de cada iteración del descenso por gradiente. \n",
    "\n",
    "- Si el descenso de gradiente está funcionando correctamente, entonces el coste $J$ debería disminuir después de cada iteración. \n",
    "- Si $J$ aumenta después de una iteración, eso significa que o bien $\\alpha$ se ha elegido mal, y normalmente significa que $\\alpha$ es demasiado grande, o podría haber un error en el código. \n",
    "\n",
    "Otra cosa útil que esta parte puede decirle es que si usted mira esta curva, en el momento en que llega a tal vez 300 iteraciones también, el costo $J$ se está nivelando y ya no está disminuyendo mucho. Para 400 iteraciones, parece que la curva se ha aplanado. Esto significa que el descenso por gradiente ha convergido más o menos porque la curva ya no disminuye. \n",
    "\n",
    "Observando esta curva de aprendizaje, puede intentar detectar si el descenso por gradiente está convergiendo o no. Por cierto, el número de iteraciones que el descenso de gradiente lleva a una conversión puede variar mucho entre diferentes aplicaciones. En una aplicación, puede converger después de sólo 30 iteraciones. En una aplicación diferente, puede tardar 1.000 o 100.000 iteraciones. Resulta muy difícil saber de antemano cuántas iteraciones necesita el descenso de gradiente para converger, por lo que se puede crear un gráfico como éste, una curva de aprendizaje. Intenta averiguar cuándo puedes empezar a entrenar tu modelo en particular. \n",
    "\n",
    "Otra forma de decidir cuándo ha terminado de entrenar tu modelo es con una prueba de convergencia automática. Aquí está el alfabeto griego $\\epsilon$. Dejemos que $\\epsilon$ sea una variable que represente un número pequeño, como 0,001 o 10-3. Si el costo $J$ disminuye en menos de este número $\\epsilon$ en una iteración, entonces es probable que estés en esta parte plana de la curva que ves a la izquierda y puedes declarar la convergencia. Recuerde, la convergencia, es de esperar en el caso de que usted encontró los parámetros $w$ y $b$ que están cerca del valor mínimo posible de $J$. Por lo general, me parece que la elección de la derecha $\\epsilon$ umbral es bastante difícil. De hecho, tiendo a mirar gráficos como éste de la izquierda, en lugar de confiar en las pruebas de convergencia automáticas. Mirando la figura sólida se puede decir, le daré en alguna advertencia avanzada si tal vez el descenso de gradiente no está funcionando correctamente también. \n",
    "\n",
    "Ahora has visto cómo debería ser la curva de aprendizaje cuando el descenso por gradiente está funcionando bien. Tomemos estas ideas y en el próximo vídeo, echemos un vistazo a cómo elegir una tasa de aprendizaje adecuada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "\n",
    "https://drive.google.com/file/d/1QtZfxtoKKh-BiQi_4aIRKnfHbGoZ6Bed/view?usp=sharing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su algoritmo de aprendizaje funcionará mucho mejor con una elección adecuada de la tasa de aprendizaje. Si es demasiado pequeña, funcionará muy lentamente y si es demasiado grande, puede que ni siquiera converja. Veamos cómo puede elegir una buena tasa de aprendizaje para su modelo. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_29.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Concretamente, si graficas el coste para un número de iteraciones y notas que los costes a veces suben y a veces bajan, deberías tomarlo como una clara señal de que el descenso de gradiente no está funcionando correctamente. Esto podría significar que:\n",
    "\n",
    "- hay un error en el código. \n",
    "- O a veces podría significar que su tasa de aprendizaje es demasiado grande. \n",
    "\n",
    "Así que aquí hay una ilustración de lo que podría estar sucediendo. Aquí el eje vertical es una función de costo $J$, y el eje horizontal representa un parámetro como tal $w_1$ y si la tasa de aprendizaje es demasiado grande, entonces si usted comienza aquí, su paso de actualización puede sobrepasar el mínimo y terminar aquí, y en el siguiente paso de actualización aquí, su ganancia sobrepasa por lo que termina aquí y así sucesivamente. Por eso a veces el coste puede subir en lugar de disminuir. Para arreglar esto, puedes usar una tasa de aprendizaje más pequeña. Entonces tus actualizaciones pueden empezar aquí y bajar un poco y bajar un poco, y esperamos que disminuya consistentemente hasta que alcance el mínimo global.  \n",
    "\n",
    "A veces puede ver que el costo aumenta constantemente después de cada iteración, como esta curva aquí. Esto también es probable debido a una tasa de aprendizaje que es demasiado grande, y podría ser abordado por la elección de una tasa de aprendizaje más pequeña. Pero tasas de aprendizaje como esta también podrían ser un signo de un posible código roto. Por ejemplo, si escribí mi código de manera que w1 se actualiza como $w_1$ más $\\alpha$ veces este término derivado, esto podría dar lugar a que el costo aumenta constantemente en cada iteración. Esto se debe a que tener el término derivado mueve su costo $J$ más lejos del mínimo global en lugar de acercarse. Así que recuerde, usted quiere utilizar en signo menos, por lo que el código debe ser actualizado w1 actualizado por $w_1$ menos $\\alpha$ veces el término derivado.  \n",
    "\n",
    "Un consejo de depuración para una correcta implementación del descenso de gradiente es que con una tasa de aprendizaje lo suficientemente pequeña, la función de coste debería disminuir en cada iteración. Así que si el descenso de gradiente no está funcionando, una cosa que hago a menudo y espero que encuentres este consejo útil también, una cosa que voy a hacer a menudo es sólo establecer Alpha para ser un número muy pequeño y ver si eso hace que el costo disminuya en cada iteración. Si incluso con $\\alpha$ ajustado a un número muy pequeño, $J$ no disminuye en cada iteración, sino que a veces aumenta, entonces suele significar que hay un error en alguna parte del código. Tenga en cuenta que la configuración de $\\alpha$ para ser muy pequeño se entiende aquí como un paso de depuración y un valor muy pequeño de $\\alpha$ no va a ser la opción más eficiente para el entrenamiento de su algoritmo de aprendizaje. Una compensación importante es que si su tasa de aprendizaje es demasiado pequeña, entonces el descenso de gradiente puede tomar muchas iteraciones para converger.  \n",
    "\n",
    "Así que cuando ejecuto el descenso de gradiente, suelo probar un rango de valores para la tasa de aprendizaje $\\alpha$. Puedo empezar probando una tasa de aprendizaje de 0,001 y también puedo probar una tasa de aprendizaje 10 veces mayor, por ejemplo 0,01 y 0,1, y así sucesivamente. Para cada elección de $\\alpha$, podría ejecutar el descenso de gradiente sólo para un puñado de iteraciones y graficar la función de coste $J$ como una función del número de iteraciones y después de probar algunos valores diferentes, podría entonces elegir el valor de $\\alpha$ que parece disminuir la tasa de aprendizaje rápidamente, pero también consistentemente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_30.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "De hecho, lo que yo hago es probar un rango de valores como este. Después de probar con 0,001, triplico la tasa de aprendizaje hasta 0,003. Después de eso, voy a tratar de 0,01, que es de nuevo cerca de tres veces mayor que 0,003. Así que estos son más o menos probando los descensos de gradiente con cada valor de $\\alpha$ siendo aproximadamente tres veces mayor que el valor anterior. Lo que voy a hacer es probar un rango de valores hasta que encuentre el valor de que es demasiado pequeño y luego también asegurarse de que he encontrado un valor que es demasiado grande. Poco a poco intentaré elegir la mayor tasa de aprendizaje posible, o algo ligeramente menor que el mayor valor razonable que he encontrado. Cuando hago eso, normalmente me da una buena tasa de aprendizaje para mi modelo. Espero que esta técnica también te sea útil para elegir una buena tasa de aprendizaje para tu implementación del descenso de gradiente. \n",
    "\n",
    "En el próximo laboratorio opcional también puedes echar un vistazo a cómo se realiza el escalado de características en el código y también ver cómo las diferentes opciones de la tasa de aprendizaje Alpha pueden conducir a un mejor o peor entrenamiento de tu modelo. Espero que te diviertas jugando con el valor de Alpha y viendo los resultados de las diferentes opciones de $\\alpha$. Por favor, eche un vistazo y ejecute el código en el laboratorio opcional para obtener una intuición más profunda sobre el escalado de características, así como la tasa de aprendizaje $\\alpha$.\n",
    "\n",
    "La elección de las tasas de aprendizaje es una parte importante del entrenamiento de muchos algoritmos de aprendizaje y espero que este vídeo te dé la intuición sobre las diferentes opciones y cómo elegir un buen valor para $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA </b></font>\n",
    "\n",
    "Se ejecuta el descenso de gradiente durante 15 iteraciones con $\\alpha$=0.3 y se calcula $J(w)$ después de cada iteración. Encuentras que el valor de $J(w)$ aumenta con el tiempo.  ¿Cómo crees que deberías ajustar la tasa de aprendizaje $\\alpha$?\n",
    "\n",
    "- Siga ejecutándolo durante más iteraciones\n",
    "- Intenta ejecutarlo durante sólo 10 iteraciones para que $J(w)$ no aumente tanto.\n",
    "- Pruebe un valor más pequeño de $\\alpha$ (digamos $\\alpha$=0,1).\n",
    "- Pruebe un valor más grande de $\\alpha$ (decir $\\alpha$=1,0).\n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta: Pruebe un valor más grande de $\\alpha$ (decir $\\alpha$=1,0).\n",
    "\n",
    "Como la función de coste es creciente, sabemos que el descenso del gradiente es divergente, por lo que necesitamos una tasa de aprendizaje más baja.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, hay un par de ideas más que puede utilizar para hacer la regresión lineal múltiple mucho más potente. Se trata de elegir características personalizadas, que también le permitirán ajustar curvas, no sólo una línea recta a sus datos. Vamos a echar un vistazo a eso en el siguiente video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "\n",
    "https://drive.google.com/file/d/1QtZfxtoKKh-BiQi_4aIRKnfHbGoZ6Bed/view?usp=sharing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La elección de las características puede tener un gran impacto en el rendimiento de su algoritmo de aprendizaje. De hecho, para muchas aplicaciones prácticas, elegir o introducir las características adecuadas es un paso crítico para que el algoritmo funcione bien. En este vídeo, vamos a ver cómo puede elegir o diseñar las características más adecuadas para su algoritmo de aprendizaje.  \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_32.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Veamos la ingeniería de características retomando el ejemplo de la predicción del precio de una casa. Digamos que tenemos dos características para cada casa. $x_1$ es la anchura de la parcela en la que está construida la casa. La segunda característica, $x_2$, es la profundidad de la parcela, supongamos que rectangular, sobre la que se ha construido la casa. Dadas estas dos características, $x_1$ y $x_2$, se podría construir un modelo como éste en el que $f(x)$ es $w_1x_1$ más $w_2x_2$ más $b$, donde $x_1$ es la fachada o anchura, y $x_2$ es la profundidad. Este modelo podría funcionar bien. \n",
    "Pero aquí hay otra opción para que usted pueda elegir una forma diferente de utilizar estas características en el modelo que podría ser aún más eficaz. Usted podría notar que el área del terreno puede ser calculada como la fachada o el ancho por la profundidad. Es posible que tenga la intuición de que la superficie del terreno predice mejor el precio que la fachada y la profundidad como características separadas. Puede definir una nueva característica, $x_3$, como $x_1$ por $x_2$. Esta nueva característica $x_3$ es igual a la superficie del terreno. Con esta característica, puede tener un modelo $f_{w,b}(x)$ igual a $w_1x_1$ más $w_2x_2$ más $w_3x_3$ más $b$, de modo que el modelo puede ahora elegir los parámetros $w_1$, $w_2$ y $w_3$, dependiendo de si los datos muestran que la fachada o la profundidad o el área $x_3$ del solar resulta ser lo más importante para predecir el precio de la vivienda.  \n",
    "\n",
    "Lo que acabamos de hacer, crear una nueva característica, es un ejemplo de lo que se llama ingeniería de características, en la que puedes utilizar tus conocimientos o intuiciones sobre el problema para diseñar nuevas características, normalmente transformando o combinando las características originales del problema para facilitar que el algoritmo de aprendizaje haga predicciones precisas. Dependiendo de los conocimientos que tengas sobre la aplicación, en lugar de limitarte a tomar las características con las que has empezado, a veces, definiendo nuevas características, puedes conseguir un modelo mucho mejor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA </b></font>\n",
    "\n",
    "Si se tienen las medidas de las dimensiones de una piscina (longitud, anchura, altura), ¿cuál de las dos siguientes sería una característica de ingeniería más útil?\n",
    "- longitud×anchura×altura\n",
    "- longitud+anchura+altura\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta: longitud×anchura×altura.\n",
    "\n",
    "El volumen de la piscina podría ser una característica útil a utilizar.  Esta es la característica de ingeniería más útil de las dos.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso es ingeniería de características. Resulta que este tipo de ingeniería de características, le permite ajustar no sólo líneas rectas, sino curvas, funciones no lineales a sus datos. Vamos a echar un vistazo en el siguiente video a cómo se puede hacer eso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><font size=\"2\" color=\"green\"><b> Link a video </b></font></summary>\n",
    "\n",
    "https://drive.google.com/file/d/1QtZfxtoKKh-BiQi_4aIRKnfHbGoZ6Bed/view?usp=sharing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora sólo hemos ajustado líneas rectas a nuestros datos. Tomemos las ideas de la regresión lineal múltiple y la ingeniería de características para crear un nuevo algoritmo llamado regresión polinómica, que nos permitirá ajustar curvas, funciones no lineales, a nuestros datos. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_34.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Supongamos que tenemos un conjunto de datos sobre viviendas con este aspecto, en el que la característica $x$ es el tamaño en pies cuadrados. No parece que una línea recta se ajuste muy bien a este conjunto de datos. Quizás quieras ajustar una curva, quizás una función cuadrática a los datos como esta que incluye un tamaño $x$ y también $x^2$, que es el tamaño elevado a la potencia de dos. Tal vez eso le dará un mejor ajuste a los datos. Pero entonces puedes decidir que tu modelo cuadrático no tiene realmente sentido porque una función cuadrática finalmente vuelve a bajar. Bueno, en realidad no esperaríamos que los precios de la vivienda bajaran cuando el tamaño aumenta. Parece que las casas grandes deberían costar más. Entonces puedes elegir una función cúbica donde ahora tenemos no sólo $x^2$, sino $x^3$. Tal vez este modelo produzca esta curva de aquí, que se ajusta un poco mejor a los datos porque el tamaño sí vuelve a subir a medida que aumenta el tamaño. Ambos son ejemplos de regresión polinómica, porque usted tomó su característica opcional $x$, y la elevó a la potencia de dos o tres o cualquier otra potencia. En el caso de la función cúbica, la primera característica es el tamaño, la segunda característica es el tamaño al cuadrado, y la tercera característica es el tamaño al cubo. \n",
    "\n",
    "Sólo quiero señalar una cosa más, que es que si usted crea características que son estas potencias como el cuadrado de las características originales como este, entonces la escala de características se vuelve cada vez más importante. Si el tamaño de la casa oscila entre, digamos, 1-1.000 pies cuadrados, entonces la segunda característica, que es un tamaño al cuadrado, oscilará entre uno y un millón, y la tercera característica, que es el tamaño al cubo, oscila entre uno y mil millones. Estas dos características, $x^2$ y $x^3$, adquieren rangos de valores muy diferentes en comparación con la característica original $x$. Si estás utilizando el descenso de gradiente, es importante aplicar el escalado de características para conseguir que tus características tengan rangos de valores comparables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W2_Página_35.jpg\"   style=\"width:500px;height:280px;\" >\n",
    "</figure>\n",
    "\n",
    "Finalmente, un último ejemplo de cómo usted realmente tiene una amplia gama de opciones de características a utilizar. Otra alternativa razonable a tomar el tamaño al cuadrado y el tamaño al cubo es decir el uso de la raíz cuadrada de $x$. Su modelo puede parecer $w_1$ veces $x$ más $w_2$ veces la raíz cuadrada de $x+b$. La función de raíz cuadrada se ve así, y se vuelve un poco menos empinada como $x$ aumenta, pero nunca se aplana completamente, y ciertamente nunca vuelve a bajar. Esta sería otra opción de características que podría funcionar bien para este conjunto de datos también.  \n",
    "\n",
    "Se preguntará, ¿cómo puedo decidir qué características utilizar? Más adelante, en el segundo curso de esta especialización, verá cómo puede elegir diferentes características y diferentes modelos que incluyan o no estas características, y tendrá un proceso para medir el rendimiento de estos diferentes modelos para ayudarle a decidir qué características incluir o no incluir. Por ahora, sólo quiero que sepas que puedes elegir las características que utilizas. Utilizando la ingeniería de características y las funciones polinómicas, puedes obtener potencialmente un modelo mucho mejor para tus datos. \n",
    "\n",
    "En el laboratorio opcional que sigue a este vídeo, verás un código que implementa la regresión polinómica utilizando características como $x$, $x^2$ y $x^3$. Por favor, echa un vistazo y ejecuta el código para ver cómo funciona. También hay otro laboratorio opcional después de ese que muestra cómo usar un popular conjunto de herramientas de código abierto que implementa la regresión lineal. Scikit-learn es una biblioteca de aprendizaje automático de código abierto muy utilizada por muchos profesionales en muchas de las principales empresas de IA, Internet y aprendizaje automático del mundo.Si ahora o en el futuro utilizas el aprendizaje automático en tu trabajo, es muy probable que uses herramientas como Scikit-learn para entrenar tus modelos. Trabajar en este laboratorio opcional te dará la oportunidad no sólo de entender mejor la regresión lineal, sino también de ver cómo se puede hacer en unas pocas líneas de código utilizando una biblioteca como Scikit-learn. \n",
    "\n",
    "Para que tengas una comprensión sólida de estos algoritmos, y seas capaz de aplicarlos, creo que es importante que sepas cómo implementar la regresión lineal por ti mismo y no sólo llamar a alguna función de scikit-learn que es una caja negra. Pero scikit-learn también tiene un papel importante en la forma en que el aprendizaje automático se hace en la práctica hoy en día. \n",
    "\n",
    "Estamos casi al final de esta semana. Enhorabuena por haber terminado todos los vídeos de esta semana. Por favor, echa un vistazo a los cuestionarios de práctica y también al laboratorio de práctica, que espero que te permita probar y practicar las ideas que hemos discutido. En el laboratorio de prácticas de esta semana, pondrás en práctica la regresión lineal. Espero que te diviertas mucho haciendo funcionar este algoritmo de aprendizaje. Mucha suerte con ello. También espero verte en los vídeos de la próxima semana, donde iremos más allá de la regresión, es decir, de la predicción de números, para hablar de nuestro primer algoritmo de clasificación, que puede predecir categorías. Nos vemos la semana que viene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice quiz: Gradient descent in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 1 </b></font>\n",
    "\n",
    "¿Cuál de los siguientes es un paso válido utilizado durante el escalado de características? \n",
    "- Sumar la media (promedio) de cada valor y luego dividir por el (máx. - mín.)\n",
    "- Reste la media (promedio) de cada valor y luego divídala por el (máx. - mín.)\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./imagenes/C1_W2_Página_04.jpg\"   style=\"width:250px;height:140px;\" >\n",
    "</figure>\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 2 </b></font>\n",
    "\n",
    "Supongamos que un amigo ejecuta el descenso de gradiente tres veces distintas con tres elecciones de la tasa de aprendizaje $\\alpha$ y traza las curvas de aprendizaje para cada una (coste $J$ para cada iteración).\n",
    "¿Para qué caso, A o B, es probable que la tasa de aprendizaje $\\alpha$ sea demasiado grande?\n",
    "- Ambos casos A y B\n",
    "- Sólo el caso B\n",
    "- Sólo el caso A \n",
    "- Ni el caso A ni el B\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 3 </b></font>\n",
    "\n",
    "De las siguientes circunstancias, ¿para cuál es especialmente útil el escalado de características?\n",
    "- El escalado de características es útil cuando una característica es mucho mayor (o menor) que otra.\n",
    "- El escalado de rasgos es útil cuando todos los rasgos de los datos originales (antes de aplicar el escalado) van de 0 a 1.\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 4 </b></font>\n",
    "\n",
    "Usted está ayudando a una tienda de comestibles a predecir sus ingresos y tiene datos sobre los artículos vendidos por semana y el precio por artículo. ¿Cuál podría ser una característica de ingeniería útil?\n",
    "- Para cada producto, calcule el número de artículos vendidos por el precio por artículo.\n",
    "- Para cada producto, calcule el número de artículos vendidos dividido por el precio por artículo. \n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 5 </b></font>\n",
    "\n",
    "¿Verdadero/Falso? Con la regresión polinómica, los valores predichos $f_{w,b}(x)$ no tienen que ser necesariamente una función lineal de la característica de entrada $x$.\n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta 1: Reste la media (promedio) de cada valor y luego divídala por el (máx. - mín.).\n",
    "\n",
    "Respuesta 2: Sólo el caso B.\n",
    "    \n",
    "Respuesta 3: El escalado de características es útil cuando una característica es mucho mayor (o menor) que otra.\n",
    "   \n",
    "Respuesta 4: Para cada producto, calcule el número de artículos vendidos dividido por el precio por artículo.\n",
    "   \n",
    "Respuesta 5: Verdadero.\n",
    "    \n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
