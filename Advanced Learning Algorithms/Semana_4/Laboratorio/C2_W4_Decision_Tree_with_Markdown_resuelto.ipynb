{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de prácticas: Árboles de decisión\n",
    "\n",
    "En este ejercicio, implementarás un árbol de decisión desde cero y lo aplicarás a la tarea de clasificar si una seta es comestible o venenosa.\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Packages ](#1)\n",
    "- [ 2 -  Problem Statement](#2)\n",
    "- [ 3 - Dataset](#3)\n",
    "  - [ 3.1 One hot encoded dataset](#3.1)\n",
    "- [ 4 - Decision Tree Refresher](#4)\n",
    "  - [ 4.1  Calculate entropy](#4.1)\n",
    "    - [ Exercise 1](#ex01)\n",
    "  - [ 4.2  Split dataset](#4.2)\n",
    "    - [ Exercise 2](#ex02)\n",
    "  - [ 4.3  Calculate information gain](#4.3)\n",
    "    - [ Exercise 3](#ex03)\n",
    "  - [ 4.4  Get best split](#4.4)\n",
    "    - [ Exercise 4](#ex04)\n",
    "- [ 5 - Building the tree](#5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 Paquetes \n",
    "\n",
    "En primer lugar, vamos a ejecutar la celda de abajo para importar todos los paquetes que necesitarás durante esta tarea.\n",
    "- [numpy](https://www.numpy.org) es el paquete fundamental para trabajar con matrices en Python.\n",
    "- [matplotlib](https://matplotlib.org) es una famosa biblioteca para trazar gráficos en Python.\n",
    "- ``utils.py`` contiene funciones de ayuda para esta tarea. No es necesario modificar el código de este archivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 -  Planteamiento del problema\n",
    "\n",
    "Supongamos que está creando una empresa que cultiva y vende setas silvestres. \n",
    "- Dado que no todas las setas son comestibles, le gustaría poder decir si una seta determinada es comestible o venenosa basándose en sus atributos físicos\n",
    "- Tiene algunos datos existentes que puede utilizar para esta tarea. \n",
    "\n",
    "¿Puedes utilizar los datos para ayudarte a identificar qué setas se pueden vender de forma segura? \n",
    "\n",
    "Nota: El conjunto de datos utilizado es sólo para fines ilustrativos. No pretende ser una guía para identificar las setas comestibles.\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"3\"></a>\n",
    "## 3 - Conjunto de datos\n",
    "\n",
    "Comenzará cargando el conjunto de datos para esta tarea. El conjunto de datos que ha recogido es el siguiente:\n",
    "\n",
    "\n",
    "| Cap Color | Stalk Shape | Solitary | Edible |\n",
    "|:---------:|:-----------:|:--------:|:------:|\n",
    "|   Brown   |   Tapering  |    Yes   |    1   |\n",
    "|   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "|   Brown   |  Enlarging  |    No    |    0   |\n",
    "|   Brown   |  Enlarging  |    No    |    0   |\n",
    "|   Brown   |   Tapering  |    Yes   |    1   |\n",
    "|    Red    |   Tapering  |    Yes   |    0   |\n",
    "|    Red    |  Enlarging  |    No    |    0   |\n",
    "|   Brown   |  Enlarging  |    Yes   |    1   |\n",
    "|    Red    |   Tapering  |    No    |    1   |\n",
    "|   Brown   |  Enlarging  |    No    |    0   |\n",
    "\n",
    "\n",
    "- Tienes 10 ejemplos de setas. Para cada ejemplo, tienes\n",
    "    - Tres características\n",
    "        - Color del sombrero (`Marrón` o `Rojo`),\n",
    "        - Forma del tallo (`Tapering` o `Enlarging`), y\n",
    "        - Solitario (`Sí` o `No`)\n",
    "    - Etiqueta\n",
    "        - Comestible (`1` que indica que sí o `0` que indica que es venenoso)\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Conjunto de datos codificados en caliente (One hot encoded)\n",
    "Para facilitar la aplicación, hemos codificado las características en caliente (convirtiéndolas en características de valor 0 o 1)\n",
    "\n",
    "| Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|:---------:|:--------------------:|:--------:|:------:|\n",
    "|     1     |           1          |     1    |    1   |\n",
    "|     1     |           0          |     1    |    1   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "|     1     |           1          |     1    |    1   |\n",
    "|     0     |           1          |     1    |    0   |\n",
    "|     0     |           0          |     0    |    0   |\n",
    "|     1     |           0          |     1    |    1   |\n",
    "|     0     |           1          |     0    |    1   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "\n",
    "Por lo tanto,\n",
    "- `X_train` contiene tres características para cada ejemplo \n",
    "    - Color marrón (Un valor de `1` indica el color del sombrero \"marrón\" y `0` indica el color del sombrero \"rojo\")\n",
    "    - Forma cónica (Un valor de `1` indica \"forma cónica del tallo\" y `0` indica forma de tallo \"creciente\")\n",
    "    - Solitario (Un valor de `1` indica \"Sí\" y `0` indica \"No\")\n",
    "\n",
    "- El valor de `y_train` indica si la seta es comestible \n",
    "    - `y = 1` indica que es comestible\n",
    "    - y = 0` indica que es venenosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ver las variables\n",
    "Vamos a familiarizarnos con el conjunto de datos.  \n",
    "- Un buen punto de partida es imprimir cada variable y ver lo que contiene.\n",
    "\n",
    "El código siguiente imprime los primeros elementos de `X_train` y el tipo de la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few elements of X_train:\n",
      " [[1 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "Type of X_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First few elements of X_train:\\n\", X_train[:5])\n",
    "print(\"Type of X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few elements of y_train: [1 1 0 0 1]\n",
      "Type of y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First few elements of y_train:\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compruebe las dimensiones de sus variables\n",
    "\n",
    "Otra forma útil de familiarizarse con sus datos es ver sus dimensiones.\n",
    "\n",
    "Imprima la forma de `X_train` y `y_train` y vea cuántos ejemplos de entrenamiento tiene en su conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (10, 3)\n",
      "The shape of y_train is:  (10,)\n",
      "Number of training examples (m): 10\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is:', X_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Repaso del árbol de decisión\n",
    "\n",
    "En este laboratorio de práctica, construirá un árbol de decisión basado en el conjunto de datos proporcionado.\n",
    "\n",
    "- Recuerde que los pasos para construir un árbol de decisión son los siguientes:\n",
    "    - Empezar con todos los ejemplos en el nodo raíz\n",
    "    - Calcule la ganancia de información para la división en todas las características posibles y elija la que tenga la mayor ganancia de información\n",
    "    - Dividir el conjunto de datos según la característica seleccionada y crear las ramas izquierda y derecha del árbol\n",
    "    - Repita el proceso de división hasta que se cumplan los criterios de parada\n",
    "  \n",
    "  \n",
    "- En este laboratorio, implementará las siguientes funciones, que le permitirán dividir un nodo en ramas izquierda y derecha utilizando la característica con mayor ganancia de información\n",
    "    - Calcular la entropía en un nodo \n",
    "    - Dividir el conjunto de datos en un nodo en ramas izquierda y derecha basándose en una característica determinada\n",
    "    - Calcular la ganancia de información de la división en una característica determinada\n",
    "    - Elegir la característica que maximiza la ganancia de información\n",
    "    \n",
    "- A continuación, utilizaremos las funciones de ayuda que ha implementado para construir un árbol de decisión repitiendo el proceso de división hasta que se cumplan los criterios de parada \n",
    "    - Para este laboratorio, el criterio de parada que hemos elegido es establecer una profundidad máxima de 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### 4.1  Calcular la entropía\n",
    "\n",
    "Primero, escribirás una función de ayuda llamada `compute_entropy` que calcula la entropía (medida de impureza) en un nodo. \n",
    "- La función toma un array numpy (`y`) que indica si los ejemplos de ese nodo son comestibles (`1`) o venenosos (`0`) \n",
    "\n",
    "Completa la función `compute_entropy()` para:\n",
    "* Calcular $p_1$, que es la fracción de ejemplos que son comestibles (es decir, tienen valor = `1` en `y`)\n",
    "* La entropía se calcula entonces como \n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "\n",
    "* Nota \n",
    "    * El log se calcula con base $2$\n",
    "    * A efectos de implementación, $0\\text{log}_2(0) = 0$. Es decir, si `p_1 = 0` o `p_1 = 1`, establece la entropía a `0`.\n",
    "    * Asegúrese de comprobar que los datos de un nodo no están vacíos (es decir, `len(y) != 0`). Devuelve `0` si lo está\n",
    "    \n",
    "<a name=\"ex01\"></a>\n",
    "### Ejercicio 1\n",
    "\n",
    "Complete la función `compute_entropy()` siguiendo las instrucciones anteriores.\n",
    "    \n",
    "Si te quedas atascado, puedes consultar las pistas que se presentan después de la celda de abajo para ayudarte con la implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: compute_entropy\n",
    "\n",
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Calcula la entropía de \n",
    "    \n",
    "    Args:\n",
    "       y (ndarray): Matriz Numpy que indica si cada ejemplo en un nodo es\n",
    "           comestible (`1`) o venenoso (`0`)\n",
    "       \n",
    "    Devuelve:\n",
    "        entropía (float): La entropía en ese nodo\n",
    "        \n",
    "    \"\"\"\n",
    "    # Debe devolver correctamente las siguientes variables\n",
    "    entropy = 0.\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    if len(y) != 0:\n",
    "        # Su código aquí para calcular la fracción de ejemplos comestibles (es decir, con valor = 1 en y)\n",
    "        p1 = len( y[ y == 1 ]) / len(y)\n",
    "\n",
    "        # Para p1 = 0 y 1, establece la entropía en 0 (para manejar 0log0)\n",
    "        if p1 != 0 and p1 != 1:\n",
    "            # Su código aquí para calcular la entropía utilizando la fórmula proporcionada anteriormente\n",
    "            entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n",
    "        else:\n",
    "            entropy = 0. \n",
    "    \n",
    "           \n",
    "    ### END CODE HERE ###        \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * Para calcular `p1`\n",
    "        * Puedes obtener el subconjunto de ejemplos en `y` que tienen el valor `1` como `y[y == 1]`\n",
    "        * Puede utilizar `len(y)` para obtener el número de ejemplos en `y`.\n",
    "    \n",
    "\n",
    "   * Para calcular la \"entropía\"\n",
    "        * <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.log2.html\">np.log2</a> Permite calcular el logaritmo en base 2 de un array de numpy\n",
    "        * Si el valor de `p1` es 0 o 1, asegúrate de poner la entropía a `0`. \n",
    "\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> Click for more hints</b></font></summary>\n",
    "        \n",
    "   * Aqui tienes la estructura para la funcion\n",
    "    \n",
    "```python \n",
    "    def compute_entropy(y):\n",
    "        \n",
    "        # You need to return the following variables correctly\n",
    "        entropy = 0.\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        if len(y) != 0:\n",
    "            # Your code here to calculate the fraction of edible examples (i.e with value = 1 in y)\n",
    "            p1 =\n",
    "\n",
    "            # For p1 = 0 and 1, set the entropy to 0 (to handle 0log0)\n",
    "            if p1 != 0 and p1 != 1:\n",
    "                # Your code here to calculate the entropy using the formula provided above\n",
    "                entropy = \n",
    "            else:\n",
    "                entropy = 0. \n",
    "        ### END CODE HERE ###        \n",
    "\n",
    "        return entropy\n",
    "```\n",
    "    \n",
    "Si sigues atascado, puedes consultar las pistas que te presentamos a continuación para saber cómo calcular `p1` y `entropía`..\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate p1</b></font></summary>\n",
    "           &emsp; &emsp; Puedes calcula p1 como <code>p1 = len(y[y == 1]) / len(y) </code>\n",
    "    </details>\n",
    "\n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate entropy</b></font></summary>\n",
    "          &emsp; &emsp; Puedes calcular la entropy como <code>entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)</code>\n",
    "    </details>\n",
    "        \n",
    "</details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes comprobar si tu implementación es correcta ejecutando el siguiente código de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy at root node:  1.0\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Calcula la entropía en el nodo raíz (es decir, con todos los ejemplos)\n",
    "# Como tenemos 5 setas comestibles y 5 no comestibles, la entropía debería ser 1\"\n",
    "\n",
    "print(\"Entropy at root node: \", compute_entropy(y_train)) \n",
    "\n",
    "# UNIT TESTS\n",
    "compute_entropy_test(compute_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Entropy at root node:<b> 1.0 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### 4.2  Dividir el conjunto de datos\n",
    "\n",
    "A continuación, escribirás una función de ayuda llamada `split_dataset` que toma los datos en un nodo y una característica para dividir y los divide en ramas izquierda y derecha. Más adelante en el laboratorio, implementarás el código para calcular la calidad de la división.\n",
    "\n",
    "- La función toma los datos de entrenamiento, la lista de índices de puntos de datos en ese nodo, junto con la característica a dividir. \n",
    "- Divide los datos y devuelve el subconjunto de índices en la rama izquierda y derecha.\n",
    "- Por ejemplo, digamos que empezamos en el nodo raíz (así que \"índices_nodo = [0,1,2,3,4,5,6,7,8,9]`), y elegimos dividir en la característica \"0\", que es si el ejemplo tiene o no una gorra marrón.\n",
    "    - La salida de la función es entonces, `índices_izquierda = [0,1,2,3,4,7,9]` y `índices_derecha = [5,6,8]`.\n",
    "    \n",
    "| Index | Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|:-----:|:---------:|:--------------------:|:--------:|:------:|\n",
    "|   0   |     1     |           1          |     1    |    1   |\n",
    "|   1   |     1     |           0          |     1    |    1   |\n",
    "|   2   |     1     |           0          |     0    |    0   |\n",
    "|   3   |     1     |           0          |     0    |    0   |\n",
    "|   4   |     1     |           1          |     1    |    1   |\n",
    "|   5   |     0     |           1          |     1    |    0   |\n",
    "|   6   |     0     |           0          |     0    |    0   |\n",
    "|   7   |     1     |           0          |     1    |    1   |\n",
    "|   8   |     0     |           1          |     0    |    1   |\n",
    "|   9   |     1     |           0          |     0    |    0   |\n",
    "\n",
    "<a name=\"ex02\"></a>\n",
    "### Ejercicio 2\n",
    "\n",
    "Complete la función `split_dataset()` que se muestra a continuación\n",
    "\n",
    "- Para cada índice en `node_indices`\n",
    "    - Si el valor de `X` en ese índice para esa característica es `1`, añada el índice a `left_indices`.\n",
    "    - Si el valor de `X` en ese índice para esa característica es `0`, añade el índice a `indices_derecha`.\n",
    "\n",
    "Si te quedas atascado, puedes consultar las pistas que se presentan después de la celda de abajo para ayudarte con la implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: split_dataset\n",
    "\n",
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Divide los datos en el nodo dado en\n",
    "    ramas izquierda y derecha\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):             Matriz de datos de forma(n_muestras, n_características)\n",
    "        node_indices (lista):  Lista que contiene los índices activos. Es decir, las muestras que se están considerando en este paso.\n",
    "        feature (int):           Índice de la característica a dividir en\n",
    "    \n",
    "    Devuelve:\n",
    "        left_indices (lista): Índices con valor de característica == 1\n",
    "        índices_derechos (lista): Índices con valor de característica == 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    left_indices  = []\n",
    "    right_indices = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "   \n",
    "    # Go through the indices of examples at that node\n",
    "    for i in node_indices:   \n",
    "        if X[i][feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * Aqui tiene la estructura para la funcion\n",
    "```python \n",
    "    def split_dataset(X, node_indices, feature):\n",
    "    \n",
    "        # You need to return the following variables correctly\n",
    "        left_indices = []\n",
    "        right_indices = []\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Go through the indices of examples at that node\n",
    "        for i in node_indices:   \n",
    "            if # Your code here to check if the value of X at that index for the feature is 1\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return left_indices, right_indices\n",
    "```\n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> Click for more hints</b></font></summary>\n",
    "        \n",
    "La condicion es <code> if X[i][feature] == 1: </code>\n",
    "        \n",
    "</details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a comprobar su implementación utilizando los bloques de código que aparecen a continuación. Intentemos dividir el conjunto de datos en el nodo raíz, que contiene todos los ejemplos en la característica 0 (Brown Cap) como habíamos comentado anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
      "Right indices:  [5, 6, 8]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Feel free to play around with these variables\n",
    "# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)\n",
    "feature = 0\n",
    "\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# UNIT TESTS    \n",
    "split_dataset_test(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
    "Right indices:  [5, 6, 8]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### 4.3  Calcular la ganancia de información\n",
    "\n",
    "A continuación, escribirás una función llamada `information_gain` que toma los datos de entrenamiento, los índices en un nodo y una característica para dividir y devuelve la ganancia de información de la división.\n",
    "\n",
    "<a name=\"ex03\"></a>\n",
    "### Exercise 3\n",
    "\n",
    "Complete la función `compute_information_gain()` que se muestra a continuación para calcular\n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$\n",
    "\n",
    "Donde\n",
    "- $H(p_1^\\text{node})$ is entropy at the node \n",
    "- $H(p_1^\\text{left})$ and $H(p_1^\\text{right})$ are the entropies at the left and the right branches resulting from the split\n",
    "- $w^{\\text{left}}$ and $w^{\\text{right}}$ are the proportion of examples at the left and right branch, respectively\n",
    "\n",
    "Nota:\n",
    "- Puede utilizar la función `compute_entropy()` que implementó anteriormente para calcular la entropía\n",
    "- Hemos proporcionado un código de inicio que utiliza la función `split_dataset()` que implementó anteriormente para dividir el conjunto de datos \n",
    "\n",
    "Si te quedas atascado, puedes consultar las pistas que se presentan después de la celda de abajo para ayudarte con la implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: compute_information_gain\n",
    "\n",
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calcula la información de la división del nodo en una característica dada\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Matriz de datos de forma(n_muestras, n_características)\n",
    "        y (array like): lista o ndarray con n_muestras que contiene la variable objetivo\n",
    "        node_indices (ndarray): Lista que contiene los índices activos. Es decir, las muestras que se están considerando en este paso.\n",
    "   \n",
    "    Devuelve:\n",
    "        coste (float):        Coste calculado\n",
    "    \n",
    "    \"\"\"    \n",
    "    # dividir Dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    # Some useful variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    information_gain = 0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Entropy at the node using compute_entropy()\n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    # Entropy at the left branch\n",
    "    left_entropy = compute_entropy(y_left)\n",
    "    # Compute the entropy at the right branch\n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    # Weights \n",
    "    w_left  = len(X_left)  / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    #Weighted entropy\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    #Information gain                                                   \n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    ### END CODE HERE ###  \n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * Here's how you can structure the overall implementation for this function\n",
    "```python \n",
    "    def compute_information_gain(X, y, node_indices, feature):\n",
    "        # Split dataset\n",
    "        left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "\n",
    "        # Some useful variables\n",
    "        X_node, y_node = X[node_indices], y[node_indices]\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[right_indices], y[right_indices]\n",
    "\n",
    "        # You need to return the following variables correctly\n",
    "        information_gain = 0\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Your code here to compute the entropy at the node using compute_entropy()\n",
    "        node_entropy = \n",
    "        # Your code here to compute the entropy at the left branch\n",
    "        left_entropy = \n",
    "        # Your code here to compute the entropy at the right branch\n",
    "        right_entropy = \n",
    "\n",
    "        # Your code here to compute the proportion of examples at the left branch\n",
    "        w_left = \n",
    "        \n",
    "        # Your code here to compute the proportion of examples at the right branch\n",
    "        w_right = \n",
    "\n",
    "        # Your code here to compute weighted entropy from the split using \n",
    "        # w_left, w_right, left_entropy and right_entropy\n",
    "        weighted_entropy = \n",
    "\n",
    "        # Your code here to compute the information gain as the entropy at the node\n",
    "        # minus the weighted entropy\n",
    "        information_gain = \n",
    "        ### END CODE HERE ###  \n",
    "\n",
    "        return information_gain\n",
    " ```\n",
    "If you're still stuck, check out the hints below.\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> Hint to calculate the entropies</b></font></summary>\n",
    "        \n",
    "<code>node_entropy = compute_entropy(y_node)</code><br>\n",
    "<code>left_entropy = compute_entropy(y_left)</code><br>\n",
    "<code>right_entropy = compute_entropy(y_right)</code>\n",
    "        \n",
    "</details>\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate w_left and w_right</b></font></summary>\n",
    "           <code>w_left = len(X_left) / len(X_node)</code><br>\n",
    "           <code>w_right = len(X_right) / len(X_node)</code>\n",
    "    </details>\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate weighted_entropy</b></font></summary>\n",
    "           <code>weighted_entropy = w_left * left_entropy + w_right * right_entropy</code>\n",
    "    </details>\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate information_gain</b></font></summary>\n",
    "           <code> information_gain = node_entropy - weighted_entropy</code>\n",
    "    </details>\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now check your implementation using the cell below and calculate what the information gain would be from splitting on each of the featues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain from splitting the root on brown cap:  0.034851554559677034\n",
      "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n",
      "Information Gain from splitting the root on solitary:  0.2780719051126377\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n",
    "print(\"Information Gain from splitting the root on brown cap: \", info_gain0)\n",
    "    \n",
    "info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n",
    "print(\"Information Gain from splitting the root on tapering stalk shape: \", info_gain1)\n",
    "\n",
    "info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n",
    "print(\"Information Gain from splitting the root on solitary: \", info_gain2)\n",
    "\n",
    "# UNIT TESTS\n",
    "compute_information_gain_test(compute_information_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Information Gain from splitting the root on brown cap:  0.034851554559677034\n",
    "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n",
    "Information Gain from splitting the root on solitary:  0.2780719051126377\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La división en \"Solitario\" (característica = 2) en el nodo raíz proporciona la máxima ganancia de información. Por lo tanto, es la mejor característica para dividir en el nodo raíz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### 4.4  Obtener la mejor división\n",
    "Ahora vamos a escribir una función para obtener la mejor característica para dividir calculando la ganancia de información de cada característica como hicimos anteriormente y devolviendo la característica que da la máxima ganancia de información\n",
    "\n",
    "<a name=\"ex04\"></a>\n",
    "### Ejercicio 4\n",
    "Complete la función `get_best_split()` que se muestra a continuación.\n",
    "- La función toma los datos de entrenamiento, junto con los índices de puntos de datos en ese nodo\n",
    "- La salida de la función es la característica que da la máxima ganancia de información \n",
    "    - Puede utilizar la función `compute_information_gain()` para iterar a través de las características y calcular la información para cada característica\n",
    "Si te quedas atascado, puedes consultar las pistas que se presentan después de la celda de abajo para ayudarte con la implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: get_best_split\n",
    "\n",
    "def get_best_split(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Devuelve la característica y el valor de umbral óptimos\n",
    "    para dividir los datos del nodo \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Matriz de datos de forma(n_muestras, n_características)\n",
    "        y (array like): lista o ndarray con n_muestras que contiene la variable objetivo\n",
    "        node_indices (ndarray): Lista que contiene los índices activos. Es decir, las muestras que se están considerando en este paso.\n",
    "\n",
    "    Devuelve:\n",
    "        mejor_característica (int):     El índice de la mejor característica a dividir\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    best_feature = -1\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    max_info_gain = 0\n",
    "\n",
    "    # Iterate through all features\n",
    "    for feature in range(num_features): \n",
    "\n",
    "       # Your code here to compute the information gain from splitting on this feature\n",
    "       info_gain = compute_information_gain(X, y, node_indices, feature)\n",
    "\n",
    "       # If the information gain is larger than the max seen so far\n",
    "       if info_gain > max_info_gain:  \n",
    "            max_info_gain = info_gain\n",
    "            best_feature  = feature\n",
    "    ### END CODE HERE ##    \n",
    "   \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * Here's how you can structure the overall implementation for this function\n",
    "    \n",
    "```python \n",
    "    def get_best_split(X, y, node_indices):   \n",
    "\n",
    "        # Some useful variables\n",
    "        num_features = X.shape[1]\n",
    "\n",
    "        # You need to return the following variables correctly\n",
    "        best_feature = -1\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        max_info_gain = 0\n",
    "\n",
    "        # Iterate through all features\n",
    "        for feature in range(num_features): \n",
    "            \n",
    "            # Your code here to compute the information gain from splitting on this feature\n",
    "            info_gain = \n",
    "            \n",
    "            # If the information gain is larger than the max seen so far\n",
    "            if info_gain > max_info_gain:  \n",
    "                # Your code here to set the max_info_gain and best_feature\n",
    "                max_info_gain = \n",
    "                best_feature = \n",
    "        ### END CODE HERE ##    \n",
    "   \n",
    "    return best_feature\n",
    "```\n",
    "    \n",
    "If you're still stuck, check out the hints below.\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> Hint to calculate info_gain</b></font></summary>\n",
    "        \n",
    "    <code>info_gain = compute_information_gain(X, y, node_indices, feature)</code>\n",
    "    </details>\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to update the max_info_gain and best_feature</b></font></summary>\n",
    "           <code>max_info_gain = info_gain</code><br>\n",
    "           <code>best_feature = feature</code>\n",
    "    </details>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the implementation of your function using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature to split on: 2\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "best_feature = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)\n",
    "\n",
    "# UNIT TESTS\n",
    "get_best_split_test(get_best_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos anteriormente, la función devuelve que la mejor característica para dividir en el nodo raíz es la característica 2 (\"Solitario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Construir el árbol\n",
    "\n",
    "En esta sección, utilizamos las funciones que implementó anteriormente para generar un árbol de decisión eligiendo sucesivamente la mejor característica para dividir hasta que alcancemos el criterio de parada (la profundidad máxima es 2).\n",
    "\n",
    "No es necesario implementar nada para esta parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not graded\n",
    "tree = []\n",
    "\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
    "    \"\"\"\n",
    "    Construye un árbol utilizando el algoritmo recursivo que divide el conjunto de datos en 2 subgrupos en cada nodo.\n",
    "    Esta función sólo imprime el árbol.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Matriz de datos de forma(n_muestras, n_características)\n",
    "        y (array like): lista o ndarray con n_muestras que contiene la variable objetivo\n",
    "        node_indices (ndarray): Lista que contiene los índices activos. Es decir, las muestras que se están considerando en este paso.\n",
    "        branch_name (string):   Nombre de la rama. ['Raíz', 'Izquierda', 'Derecha']\n",
    "        max_depth (int):        Profundidad máxima del árbol resultante. \n",
    "        current_depth (int):    Profundidad actual. Parámetro utilizado durante la llamada recursiva.\n",
    "   \n",
    "    \"\"\" \n",
    "\n",
    "    # Profundidad máxima alcanzada - parada de la división\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \" * current_depth + \"-\" * current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "    # Otherwise, get best split and split the data\n",
    "    # Get the best feature and threshold at this node\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    tree.append((current_depth, branch_name, best_feature, node_indices))\n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "    # Split the dataset at the best feature\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    \n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: 2\n",
      "- Depth 1, Left: Split on feature: 0\n",
      "  -- Left leaf node with indices [0, 1, 4, 7]\n",
      "  -- Right leaf node with indices [5]\n",
      "- Depth 1, Right: Split on feature: 1\n",
      "  -- Left leaf node with indices [8]\n",
      "  -- Right leaf node with indices [2, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
