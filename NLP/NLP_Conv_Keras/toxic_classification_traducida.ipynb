{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MemFGW7ksHC4"
   },
   "source": [
    "# Clasificación de comentarios tóxicos usando CNN 1D con Keras\n",
    "\n",
    "*Esta notebook plasma los apuntes traducidos al español del Proyecto dicatado por Coursera Project Network, por lo que puede encontrar errores. Todo el contenido principal es el explicado en el proyecto, algunas modificaciones fueron realizadas para personalizar el ejercicio. Todo el mérito es de los instructores. Simplemente espero que los apuntes sirvan como material de estudio complementario.*\n",
    "\n",
    "Bienvenido a esta introducción práctica y guiada a la clasificación de texto utilizando convoluciones 1D con Keras. Al final de este proyecto, serás capaz de aplicar incrustaciones de palabras para la clasificación de texto, utilizar convoluciones 1D como extractores de características en el procesamiento del lenguaje natural (NLP), y realizar la clasificación de texto binario utilizando el aprendizaje profundo. Como caso de estudio, trabajaremos en la clasificación de un gran número de comentarios de Wikipedia como tóxicos o no (es decir, comentarios que son groseros, irrespetuosos o que de alguna manera pueden hacer que alguien abandone una discusión). Esta cuestión es especialmente importante, dadas las conversaciones que la comunidad mundial y las empresas tecnológicas están manteniendo sobre la moderación de contenidos, el acoso en línea y la inclusividad. El conjunto de datos que utilizaremos procede del Toxic Comment Classification Challenge de Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de contenidos\n",
    "\n",
    "- [1 - Introducción e importación de paquetes](#1)\n",
    "- [2 - Cargar y explorar los datos](#2)\n",
    "- [3 - Preparación de los datos - Tokenizar y rellenar los datos de texto](#3)\n",
    "- [4 - Preparar la matriz de incrustación con las incrustaciones GloVe preentrenadas](#4)\n",
    "- [5 - Crear la capa de incrustación](#5)\n",
    "- [6 - Construir el modelo](#6)\n",
    "- [7 - Entrenar el modelo](#7)\n",
    "- [8 - Evaluación del modelo - Clasificar los comentarios tóxicos](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Introducción e importación de paquetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hola a todos. Bienvenidos a este. Proyecto práctico sobre la clasificación de los comentarios tóxicos utilizando 1D convoluciones con Keras Soy Snake Hon, y seré tu instructor para este proyecto guiado. Cuando completen este proyecto hoy, serán capaces de preparar los datos del texto y utilizar las palabras incrustadas (word embedding) para una clasificación y utilizar una CNN de 1D para la para la clasificación de textos. Y por último, serás capaz de clasificar un texto como tóxico o no tóxico. \n",
    "\n",
    "Así que el objetivo principal de aprendizaje para este proyecto es conseguir que te sientas cómodo con la clasificación de texto usando convoluciones. Y como caso de estudio hoy, vamos a clasificar un gran número de comentarios de Wikipedia como tóxicos o no. Es decir, si los comentarios que son groseros o irrespetuosos o que puedan hacer que alguien abandone una discusión. \n",
    "\n",
    "Y puede que te preguntes por qué la convolución de tus redes neuronales. Originalmente desarrolladas por Young McCune hace décadas, las CNN han tenido mucho éxito en varios campos del aprendizaje automático campos como el procesamiento de imágenes. Sin embargo, las redes neuronales recurrentes son la tecnología de punta para las  tecnológicas de aplicaciones de texto y han sido la mejor opción para las tareas lingüísticas debido a su alta precisión y aunque las recurentes han históricamente superado a las CNN en las pruebas lingüísticas, su diseño tiene una limitación inherente, que puede entenderse si se observa cómo procesan la información. \n",
    "\n",
    "La forma en que procesan la información es un ajuste menos natural para el hardware de GPU altamente paralelo que impulsa el aprendizaje automático moderno. Y el cómputo no puede paralizarse completamente porque cada palabra debe esperar hasta que la red haya terminado con una palabra anterior. En comparación, las CNN pueden computar todos los elementos simultáneamente, aprovechando al máximo el paralelismo de la GPU. Por tanto, son computacionalmente más eficientes. Otra ventaja de la convolución de todas las redes cercanas, es que la información se procesa jerárquicamente, lo que hace más fácil capturar relaciones más complejas en los datos. Así que por eso vamos a explorar las CNN para la clasificación de textos hoy. \n",
    "\n",
    "Ahora un poco de contexto sobre los comentarios tóxicos. Discutir cosas que te importan en línea puede ser especialmente difícil. La amenaza de abuso y acoso en línea significa que muchas personas dejan de expresarse y renuncian a buscar opiniones diferentes, y las plataformas en línea luchan con esto día tras día para facilitar eficazmente conversaciones, lo que lleva a muchas comunidades a limitar o cerrar completamente los comentarios de los usuarios. Así que las mejoras de estos modelos actuales que las empresas tecnológicas y de medios sociales están empleando ayudarán a que la discusión en línea sea más productiva y respetuosa. \n",
    "\n",
    "Así que para entrenar nuestra red neuronal para clasificar los comentarios tóxicos, vamos a utilizaremos un conjunto de datos de los comentarios de la página de de Wikipedia. Y este conjunto de datos fue parte de un concurso de Kagle hace dos años. He marcado el enlace para que puedas acceder fácilmente acceder a él en el escritorio de la nube más tarde también han añadido a la sección de recursos en coursera. Así que por favor no te preocupes que actualmente no eres capaz de acceder al enlace. \n",
    "\n",
    "Bueno, eso es en los requisitos previos para este proyecto son que tengas experiencia previa en la teoría del aprendizaje profundo y haber construido modelos de aprendizaje profundo usando TensorFlow o su API de alto nivel Keras. También esperamos que hayas trabajado con convoluciones en el pasado, así que nosotros y no vamos a profundizar en ellos hoy. Pero en su lugar vamos a refrescar brevemente y tocar sobre las convoluciones y cosas como MaxPooling. Y la idea es que apliquen sus conocimientos de las convoluciones en dominios aparentemente no obvios como el procesamiento del lenguaje natural y la clasificación de textos con redes neuronales convolucionales. Y creo que esto es super cool y valioso porque no es obvio para nosotros. Al principio era cómo tomar las convoluciones que hacemos en las imágenes y aplicarlo al texto. Pero de hecho, realmente puedes hacerlo y te mostraré cómo hacerlo muy pronto. Podrás tomar todos los conceptos e intuiciones que has aprendido con el procesamiento de imágenes y aplicarlo a los datos de texto. Operaciones como las stride convolution, MaxPolling, AveragePooling totalmente aplicadas al texto. Y esto no es una idea marginal, ¿verdad? Debido a su practicidad, las convoluciones son utilizadas por un montón de organizaciones en sistemas de producción en el mundo real. Así, empresas como Facebook las utilizan para hacer gran parte de su clasificación de texto. \n",
    "\n",
    "Para hacerlo, vamos a aprender sobre algo llamado \"embeddings\" (incrustaciones) y embeddings son para mí, uno de los temas más fascinantes y de todo el procesamiento del lenguaje natural. Bien, déjame que no me adelante a mí mismo ahora. Voy a minimizar mi pantalla para que, puedas acceder a tu escritorio en la nube. Vamos a estar escribiendo todo nuestro código Keras y Python en cuadernos Júpiter hoy. Y cuando usted abrió este proyecto, usted debe ver una Jupiter Lab ya abierto en tu navegador en tu pantalla. Así que vamos a empezar haciendo doble clic en este archivo naranja aquí llamado \"toxic_classification.ipynb\". Así que este es un cuaderno de Júpiter plantilla que va a utilizar hoy. Si te desplazas por el cuaderno, verás que la mayor parte del cuaderno contiene espacios en blanco y he dejado intencionadamente en blanco para que sigas con mis instrucciones grabadas y escribas y ejecutes el código en tu propia nube o escritorio. Pero también puedes ver que he rellenado previamente algunas de las partes más tediosas del código, como la creación de gráficos y visualizaciones y también la importación de algunas bibliotecas e importantes librerías que usaremos hoy, y que puedes ver en el primer código. Así que aquí esto contiene las importaciones para todas las funciones de ayuda y clases y cosas como capas y funciones de preprocesamiento que vamos a utilizar en breve. \n",
    "\n",
    "Vamos a seguir adelante y la importación de estos paquetes mediante la ejecución de la celda y para ejecutar la celda usted puede simplemente hacer clic en esta ejecución, pero en aquí o haga clic en en cualquier lugar dentro de la celda y presionar shift y enter. Así que después de presionar shift, enter y una vez que haya importado los paquetes únase a mí en la siguiente tarea para cargar el conjunto de datos para este proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si da una advertencia de RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
    "#!pip3 install --upgrade requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Cargar y explorar los datos\n",
    "\n",
    "En esta tarea, vamos a ver los comentarios tóxicos que hemos descargado de Kaggle y que hemos incluido en la carpeta Proyectos del escritorio. Es un archivo bastante grande, de unos 64 megabytes, por lo que preferiría que no lo abras en Excel realmente colgaría el escritorio o la Nube. En su lugar vamos a utilizar la biblioteca **Pandas** para comprobarlo. Así que lo que vamos a hacer es leer en el archivo *train.csv* como un DataFrame de Pandas y vamos a rellenar todos los valores que faltan o NAN's con espacio. Vamos a cargar nuestros datos en este marco de datos utilizando el método `read_csv` y pasamos la ruta del archivo. Y como Júpiter ya está abierto dentro de esta carpeta del Proyecto, podemos pasar el nombre del archivo, y también rellenamos todos los valores que faltan con un espacio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\").fillna(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora echemos un vistazo a 10 ejemplos aleatorios de este conjunto de datos. Podemos utilizar la función `sample` en este marco de datos de pandas `train_df` y mirar 10 muestras aleatorias. Y también estoy pasando este argumento llamado `random_state=1` para que ambos veamos los mismos datos cuando ejecutamos. Siempre y cuando, por supuesto, también escriba el número 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24915</th>\n",
       "      <td>YOU ARE A FAT, GEEKY PRICK WHO HAS NOTHING TO ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75819</th>\n",
       "      <td>Agent X2: Basically thanks - with a 'little' m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53891</th>\n",
       "      <td>Why are my posts being deleted? \\n\\nI have tri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154159</th>\n",
       "      <td>\"\\n\\n Controlled Demolitions and Common Sense ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13040</th>\n",
       "      <td>I do not understand your reply.  //Blaxthos ( ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123190</th>\n",
       "      <td>Is this the bizarro world? Removing content is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33626</th>\n",
       "      <td>Well, WP:RS says that articles should use reli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>Oh hear me go someone removes all my pages i g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48633</th>\n",
       "      <td>can't believe this article was deleted\\nI'm su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42817</th>\n",
       "      <td>\"\\n\\n Comments on GamerGate Workshop page \\n\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic\n",
       "24915   YOU ARE A FAT, GEEKY PRICK WHO HAS NOTHING TO ...      1\n",
       "75819   Agent X2: Basically thanks - with a 'little' m...      0\n",
       "53891   Why are my posts being deleted? \\n\\nI have tri...      0\n",
       "154159  \"\\n\\n Controlled Demolitions and Common Sense ...      0\n",
       "13040   I do not understand your reply.  //Blaxthos ( ...      0\n",
       "123190  Is this the bizarro world? Removing content is...      0\n",
       "33626   Well, WP:RS says that articles should use reli...      0\n",
       "1150    Oh hear me go someone removes all my pages i g...      0\n",
       "48633   can't believe this article was deleted\\nI'm su...      0\n",
       "42817   \"\\n\\n Comments on GamerGate Workshop page \\n\\n...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ver que tenemos el texto de los comentarios en una columna (comment_text) y una columna de valor binario a la derecha (toxic). Y esta columna de valor binario llamado tóxic tiene un valor de 1 si el comentario correspondiente es tóxico y 0 si el comentario correspondiente es no tóxico y puedes ver claramente que en realidad hay un comentario tóxico, pero este conjunto de datos tiene muchos y no pertenece a una discusión en línea. \n",
    "\n",
    "Oh, por cierto, me olvidé de mencionar que he modificado este conjunto de datos un poco de cómo es originalmente en Kaggle. Así que originalmente, este conjunto de datos fue utilizado en un desafío de clasificación donde había columnas para el tipo de tóxico que comentaba era. Aparte de sólo el comentario, siendo tóxico su gravedad de tóxico, si es lenguaje obsceno, tal vez el comentario era una amenaza o era un insulto, o comentarios con contenido racista o xenofobicos se incluyeron en la identidad. Siéntete libre de ir sitio y comprobar la descripción original de los datos, ya sea ahora, pausando mi grabación o más tarde, una vez que hayamos terminado con este proyecto. \n",
    "\n",
    "Muy bien, así que ahora que hemos importado los datos, podemos empezar por sacar todo el texto o los comentarios y almacenarlo en un objeto `x` y, podemos eliminar o no, solo estamos asignando todo el texto de los comentarios a `x` y más tarde haremos un proceso similar para asignar todas las etiquetas a un objeto python `y`. \n",
    "\n",
    "Entonces, asignamos el texto de los comentarios a `x` y obtenemos los valores de este. Así que es un Numpy array con una lista de todas estas cadenas. Y una vez que hemos almacenado todos los comentarios, vamos a imprimámoslo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\"\n",
      " \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\"\n",
      " \"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\"\n",
      " ...\n",
      " 'Spitzer \\n\\nUmm, theres no actual article for prostitution ring.  - Crunch Captain.'\n",
      " 'And it looks like it was actually you who put on the speedy to have the first version deleted now that I look at it.'\n",
      " '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"']\n"
     ]
    }
   ],
   "source": [
    "# Ver algunos pocos comentarios tóxicos\n",
    "x = train_df[\"comment_text\"].values\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que esta visto que realmente no nos permite ver el texto completo de los comentarios, hagamos algo más interesante. Así que si quieres ver algunos comentarios puramente tóxicos, podemos filtrarlos, buscando los comentarios que tienen una etiqueta tóxica igual a 1. Así que estamos localizando todos los comentarios en el marco de datos original donde la columna Tóxico tenía la etiqueta 1. Ahora tomaremos una muestra aleatoria de 10 ejemplos y los devolveremos a nuestra pantalla para que podamos echar un vistazo de los comentarios tóxicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30575</th>\n",
       "      <td>\"\\n Are you asking if we would ever block some...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36038</th>\n",
       "      <td>Amen my sock-pupinski brother!! Hack-a-long: p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19730</th>\n",
       "      <td>JUST READ THE FUCKING CENSUS DATA. Do you thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72261</th>\n",
       "      <td>u no wat ass hole go fuck your self and stop b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77923</th>\n",
       "      <td>Go fuck yourself \\n\\nEat shit and die. 68.0.11...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16382</th>\n",
       "      <td>A big thank you\\n\\nYou have just blocked me in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41247</th>\n",
       "      <td>A message from Jasonceyre \\n\\n   f uc k you ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69311</th>\n",
       "      <td>I'm gonna get the ACLU on you, you racist anti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142032</th>\n",
       "      <td>Hey Freepsbane, does your mother know that you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5613</th>\n",
       "      <td>You guys suk. You really really suck. Fuck you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic\n",
       "30575   \"\\n Are you asking if we would ever block some...      1\n",
       "36038   Amen my sock-pupinski brother!! Hack-a-long: p...      1\n",
       "19730   JUST READ THE FUCKING CENSUS DATA. Do you thin...      1\n",
       "72261   u no wat ass hole go fuck your self and stop b...      1\n",
       "77923   Go fuck yourself \\n\\nEat shit and die. 68.0.11...      1\n",
       "16382   A big thank you\\n\\nYou have just blocked me in...      1\n",
       "41247   A message from Jasonceyre \\n\\n   f uc k you ho...      1\n",
       "69311   I'm gonna get the ACLU on you, you racist anti...      1\n",
       "142032  Hey Freepsbane, does your mother know that you...      1\n",
       "5613    You guys suk. You really really suck. Fuck you...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df[\"toxic\"]==1].sample(10,random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que a continuación, podemos crear una visualización para ver una nube de palabras de las más frecuentes en el conjunto de datos. E incluso podemos ordenar estas palabras por toxicidad. Todo lo que he hecho aquí es filtrar los datos y pedirle que devuelva todos los comentarios que tienen una etiqueta tóxica de 1. Y he utilizado el paquete de la nube de palabras para eliminar algo llamado palabras de parada (STOPWORDS), que son palabras como, *a*, *in* y *and* y palabras similares, y luego generar una imagen que contiene palabras individuales cuyo tamaño de fuente está dado por su frecuencia en el conjunto de datos. Así que las palabras en un fondo más grande significan que aparecen más frecuencia en los datos. \n",
    "\n",
    "Por cierto, es posible que veas las palabras que una disposición diferente disposición en tu pantalla porque no he puesto el estado para este, y también se puede filtrar para las palabras que no son parte de los comentarios tóxicos. Y para hacer eso, sólo tenemos que cambiar este a 1 por 0 y luego presionar shift. Enter una vez más, y cuando lo ejecute, verá inmediatamente un cambio. Y en que esas palabras tóxicas ya no están incluidas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Si no esta instalado wordcloud, en la consola de Anaconda hacer\n",
    "# conda install -c conda-forge wordcloud  \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "STOPWORDS.add(\"n\")\n",
    "import matplotlib.pyplot as plt\n",
    "#Cambiar a ==0 para ver la nube de palabras no toxicas\n",
    "comments = train_df['comment_text'].loc[train_df['toxic']==1].values\n",
    "wordcloud = WordCloud(\n",
    "    width = 640,\n",
    "    height = 640,\n",
    "    background_color = 'white',\n",
    "    stopwords = STOPWORDS).generate(str(comments))\n",
    "fig = plt.figure(\n",
    "    figsize = (12, 8),\n",
    "    facecolor = 'k',\n",
    "    edgecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien, entonces acabamos de extraer y guardar los comentarios por separado en el texto guardandolo en el objeto `x`. Así que vamos a hacer eso para las etiquetas `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = train_df['toxic'].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y esta vez, en lugar de texto común estamos viendo las etiquetas, escribimos el nombre de la columna tóxica y obtenemos sus valores. Y ahora vamos a seguir adelante e imprimir `y`. Es sólo una lista de las etiquetas.\n",
    "\n",
    "Ahora para tener una idea de la distribución de los comentarios tóxicos, vamos a crear un simple hostograma para mostrarnos cuántos comentarios tóxicos ocurren en los datos y cuántos comentarios no tóxicos. Así que desde el `train_df`, estamos mirando la columna tóxica y específicamente, queremos trazar un hostograma. Así que en lugar de pasar por algún proceso laborioso de usar Matplotlib u otra biblioteca, podemos utilizar la función de dentro de los marcos de datos de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['toxic'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4d0280d383d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot frequency of toxic commen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'toxic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hist'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Distribución de comentarios tóxicos'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)\u001b[0m\n\u001b[0;32m   2740\u001b[0m                            \u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2741\u001b[0m                            \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2742\u001b[1;33m                            **kwds)\n\u001b[0m\u001b[0;32m   2743\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36mplot_series\u001b[1;34m(data, kind, ax, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, label, secondary_y, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m                  \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m                  **kwds)\n\u001b[0m\u001b[0;32m   1999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m_plot\u001b[1;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[0;32m   1799\u001b[0m         \u001b[0mplot_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubplots\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1801\u001b[1;33m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1802\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    355\u001b[0m                                                    \u001b[1;34m\"datetime\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m                                                    \u001b[1;34m\"datetimetz\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m                                                    \"timedelta\"])\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mselect_dtypes\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   3324\u001b[0m         \u001b[1;31m# the \"union\" of the logic of case 1 and case 2:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3325\u001b[0m         \u001b[1;31m# we get the included and excluded, and return their logical and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3326\u001b[1;33m         \u001b[0minclude_these\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3327\u001b[0m         \u001b[0mexclude_these\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                 data = sanitize_array(data, index, dtype, copy,\n\u001b[1;32m--> 262\u001b[1;33m                                       raise_cast_failure=True)\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             subarr = construct_1d_arraylike_from_scalar(\n\u001b[1;32m--> 642\u001b[1;33m                 value, len(index), dtype)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[1;34m(value, length, dtype)\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m         \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m         \u001b[0msubarr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type"
     ]
    }
   ],
   "source": [
    "# Plot frequency of toxic commen\n",
    "train_df['toxic'].plot(kind='hist', title='Distribución de comentarios tóxicos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que esta es la distribución donde hay claramente una abundancia de comentarios no tóxicos en comparación con sus contrapartes tóxicas.\n",
    "\n",
    "Y si quieres un recuento exacto, podemos hacer esto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    144277\n",
       "1     15294\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es un claro problema de desequilibrio donde la clase 0 para los comentarios no tóxicos es claramente dominante sobre la clase 1 para los comentarios tóxicos. Sin embargo, en este proyecto no vamos a abordar el problema de desequilibrio. Hay algunos métodos que se pueden explorar como el submuestreo de la clase mayoritaria o el sobremuestreo la clase minoritaria y un algoritmo llamado smote que intenta equilibrar las clases. Lo que vamos a hacer hoy es algo llamado llamado **muestreo estratificado**, donde dividimos nuestros datos en conjuntos de entrenamiento y validación para que la distribución de los comentarios tóxicos y no tóxicos a través de los conjuntos de entrenamiento y validación sean similares. \n",
    "\n",
    "Bien, genial. En esta tarea importamos el conjunto de datos, dividimos el conjunto de datos en comentarios y etiquetas y también visualizamos la distribución de palabras en los comentarios. \n",
    "\n",
    "A continuación vamos a hablar de cómo podemos preparar estos datos que actualmente son una lista de cadenas y convertirla o transformarla en una forma numérica para que Keras pueda aceptarla como entrada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Preparación de los datos - Tokenizar y rellenar los datos de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a convertir nuestros datos, que están actualmente en formato de cadena, en una representación numérica. Así que ya sabes, el gran problema con el uso de redes neuronales en textos es que es un poco difícil obtener cosas del texto, que es, por supuesto, cadenas de longitud arbitraria en algo con lo que una neurona pueda trabajar, que es básicamente una cadena de longitud fija. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_02.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Una de las formas más comunes para hacer frente a esto hace alrededor de 10 años, y de hecho sigue siendo muy popular hoy en día, por cierto, se llama el modelo de **bolsa de palabras (bag of words)**. Recordarán que la bolsa de palabras consiste en tomar cada palabra y contar cuántas veces ocurre en cada documento. Así que básicamente transformas la cadena en un vector donde la longitud es el número de palabras que tienes. Y el problema, por supuesto, con esto es que pierdes completamente el orden de las palabras. El orden de las palabras importa mucho, y es bastante sorprendente que la clasificación pueda funcionar dejando de lado el orden. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_01.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Bien, hay otra transformación que usa caracteres individuales. Así que básicamente, en **one-hot encodding** cada carácter en el texto, y eso tiene un sentido intuitivo como una transformación. Y en realidad podemos rellenar estos caracteres para hacer que todo el texto tenga una longitud fija de caracteres. Pero el problema es que en inglés y otros idiomas idiomas, los espacios importan mucho, cierto, y el concepto de una palabra es un concepto bastante útil que podríamos querer pasar a nuestra red neuronal. Así que al pasar cada carácter como una codificación de caracteres codificación, realmente estamos haciendo que la red neuronal aprenda mucho sobre el lenguaje, por lo que podría ser demasiado crudo, demasiado extremo a menos que tengamos realmente masivas cantidades de datos. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_03.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Así que para obtener los mejores resultados, realmente queremos algo intermedio, y ahí es donde entra en juego **word embedding**. Aquí tenemos la frase *\"I love this movie\"* donde la transformación de cada palabra determina un conjunto de números, y en este caso, la transformamos en cuatro números. Así que la palabra *\"I\"* siempre se transforma en los mismos cuatro números. La palabra *\"love\"* siempre se transforma en los mismos cuatro números. Y podemos hacer esto con vectores más largos. Así que en esta tabla tenemos cada palabra que podríamos haber visto en nuestro **vocabulario**, junto con la transformación en que cada palabra se convierte. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_05.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Así que si no queremos calcular las word embedding en si mismas, podemos usar algunas embeddings precalculadas. Así que **GloVe** uno de los más famosos, generado por un equipo de Stanford. Es en un enorme conjunto de datos, y tiene algunas propiedades increíbles que serían digno de un proyecto entero para explorar más. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_06.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Pero, escucha algunas propiedades interesantes. En realidad puedes hacer aritmética vectorial en el espacio vectorial, donde si tomas los números reales que se incrustaron para la palabra *king* y restas el número codificado para la palabra *man* y se añade el número codificado para la palabra *woman*, obtendrías el número o conjunto de números para la palabra *queen*. Así que esto es increíble. Y muestra que estos zares incrustados realmente codifican alguna información semántica y contenido semántico sobre estas palabras. \n",
    "\n",
    "Así que la siguiente tarea será usar estos números o embeddings generados por Stanford y los usaremos en nuestro modelo para que funcione mejor que si tuviéramos que aprender estos embeddings nosotros mismos con nuestra red neuronal. Se puede pensar en esto como una especie de aprendizaje de transferencia, pero hacia adelante, si eso tiene sentido. Bueno, esto fue un montón de hablar por mi parte. \n",
    "\n",
    "Ahora vamos a seguir adelante y tokenizar el texto. Vamos a establecer el número máximo de palabras para mantener de nuestro documento o conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caracteristicas Máximas\n",
    "max_features    = 20000 \n",
    "# Longitud máxima de texto\n",
    "max_text_lenght = 400 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las características máximas se establecieron como las 20.000 palabras más frecuentes en el conjunto de datos que serán captadas y las demás no. La longitud máxima de texto es la longitud máxima que una secuencia particular o, en nuestro caso, un comentario puede tener. Los comentarios que tienen menor longitud serán rellenados para que sea igual a esta longitud, en este caso 400. \n",
    "\n",
    "Ahora vamos a seguir adelante y tokeneizar nuestro texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.text.Tokenizer object at 0x000002045D4125F8>\n"
     ]
    }
   ],
   "source": [
    "x_tokenizer  =  text.Tokenizer(max_features)\n",
    "# Imprimimos para ver que tipo es x_tokenizer\n",
    "print(x_tokenizer)\n",
    "# vemos que es un objeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta clase, llamada `Tokenizer` nos permite vectorizar un corpus de texto convirtiendo cada texto en una secuencia de enteros donde cada entero es el índice de un token en un diccionario y el valor es la palabra misma. Y le decimos que seleccione las 20.000 palabras más más frecuentes en el conjunto de datos y descarte el resto.\n",
    "\n",
    "Ahora vamos a ajustar este objeto `x_tokenizer`. Para ello podemos llamar al método `fit_on_texts` y pasar una lista de todas nuestras cadenas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "x_tokenizer.fit_on_texts(x) \n",
    "# Imprimimos para ver que tipo es x_tokenizer\n",
    "print(type(x_tokenizer))\n",
    "# vemos que es una clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora lo que podemos hacer es convertir nuestro texto tokenizado en una lista de listas que contiene números. Así que básicamente una lista de secuencias, no un Numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "x_tokenized = x_tokenizer.texts_to_sequences(x)\n",
    "# Imprimimos para ver que tipo es x_tokenized\n",
    "print(type(x_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[412, 437, 73, 134, 14, 249, 2, 71, 314, 78, 50, 9, 13, 626, 8, 2284, 492, 502, 102, 4, 611, 2, 35, 325, 126, 363, 3, 29, 38, 27, 52, 208, 2, 434, 57, 36, 1, 2394, 93, 1, 737, 468]\n",
      "\n",
      "Cantidad elementos en la lista:  42\n"
     ]
    }
   ],
   "source": [
    "# vemos uno de los elementos de esa lista\n",
    "print(x_tokenized[2])\n",
    "print(\"\\nCantidad elementos en la lista: \", len(x_tokenized[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El texto es: Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "\n",
      "Cantidad de palabras en el comentario:  42\n"
     ]
    }
   ],
   "source": [
    "# Vemos el comentario relacionado con esa lista\n",
    "print(\"El texto es:\", x[2])\n",
    "print(\"\\nCantidad de palabras en el comentario: \", len(x[2].split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a seguir adelante y rellenar cada una de estas secuencias para que cada documento o frase o secuencia tenga la misma longitud usando la `max_text_lenght`. Usamos la función `pad sequences` de la clase `sequence` en Keras y le pasamos nuestro texto de `x_tokenizer_list` y le damos la longitud máxima para que pueda hacer el relleno y todo nuestro texto tenga la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x_train_val  = sequence.pad_sequences(x_tokenized , maxlen = max_text_lenght) \n",
    "print(type(x_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matriz rellena de 0 es:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0  412  437   73  134   14  249\n",
      "    2   71  314   78   50    9   13  626    8 2284  492  502  102    4\n",
      "  611    2   35  325  126  363    3   29   38   27   52  208    2  434\n",
      "   57   36    1 2394   93    1  737  468]\n",
      "\n",
      "La cantidad de elementos son:  400\n"
     ]
    }
   ],
   "source": [
    "print(\"La matriz rellena de 0 es:\\n\", x_train_val[2])\n",
    "print(\"\\nLa cantidad de elementos son: \", len(x_train_val[2]))\n",
    "# Se pueden apreciar los mismos valores del embedding que vimos antes para el mismo comentario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gran trabajo En la siguiente tarea, vamos a pasar a preparar nuestra guante pre-entrenado y los beddings de los que hablé en las diapositivas anteriores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Preparar la matriz de incrustación con las incrustaciones GloVe preentrenadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tilizaremos el tokenized que creamos en la última tarea y obtendremos los correspondientes embedding del conjunto de preentrenamiento de GloVe de Stanford. Es súper sencillo. El Stanford en realidad creó un archivo ZIP, que es alrededor de 800 megabytes, con los embeddings de varias dimensiones. El archivo contiene vectores codificados en texto, o embeddings de varios tamaños (de 50, de 100, de 200 dimensional y de 300 dimensiones). Hoy utilizaremos los de 100 dimensiones. \n",
    "\n",
    "Así que aquí es donde se puede descargar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, entonces comencemos haciendo un diccionario o dict en python, mapeando las palabras, que son cadenas, a su representación vectorial. Así que primero vamos a crear un diccionario vacío, `embedding_index`, que analizará este archivo de embeddings y mapeará las palabras a sus embeddings. También vamos a establecer este parámetro `embedding_dim` que usaremos más tarde. Y es sólo para mostrar que vamos a establecer nuestro embedding a 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim    = 100 \n",
    "embedding_index = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a pasar por cada línea en el archivo *\"glove.6B.100d.txt\"*.\n",
    "\n",
    "No te animo a abrir el archivo porque es realmente pesado y puede bloquearse o colgarse incluso en tu propio ordenador, incluso con una gran memoria y una buena CPU. Así que te puedo describir cómo es. Tiene millones de líneas donde el primer valor en cada línea es la representación de la cadena de la palabra, podría ser *hola* o cualquier palabra, y eso es que es el primer valor en una línea particular. Y el resto de la línea contiene los valores numéricos para el embedding. Así que es un vector muy largo. \n",
    "\n",
    "Así que lo que vamos a hacer es primero dividir el archivo por las líneas, así que extraemos líneas individuales en cada iteración del bucle, y podemos hacer esto usando `line.split()`. Y ahora vamos a extraer la palabra y la añadiremos a nuestro diccionario en los próximos pasos. Así que de los valores, que actualmente es una línea, extraemos el primer valor, que es la cadena y la palabra, y el resto de los valores son simplemente coeficientes o los propios embeddings. Vamos a convertirlo en `asarray`, y los valores que estamos convirtiendo en array son todos después de la primera entrada. Vamos a ignorar la primer entrada y sólo mirar los coeficientes y vamos a establecer este tipo de datos para ser `float 32`. \n",
    "\n",
    "Muy bien, y ahora vamos a poblar nuestro índices del diccionario de embeddings. Así que cada palabra en este archivo de embeddings va a ser mapeada a sus coeficientes, y es muy simple y muy útil para mantenerlo en una estructura de diccionario como esta. \n",
    "\n",
    "Antes de que me olvide, todavía no hemos definido lo que es `f`. Así que vamos a hacer eso antes del bucle. Tenemos que abrir nuestro archivo primero. Y ahora, al final del bucle, nos asegurémonos de cerrar los archivos. \n",
    "\n",
    "Y sigamos adelante e imprimamos cuántos vectores de palabras pudimos analizar y vamos a mirar la longitud del diccionario para obtener el número de vectores de palabras. Una vez que ha completado la ejecución con éxito, usted debe ser capaz de ver que son 400.000 vectores de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 400000 vectores de palabras\n"
     ]
    }
   ],
   "source": [
    "# Agregué encoding=\"utf8\" porque me daba un error:\n",
    "# UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 2776: character maps to <undefined>\n",
    "f = open(\"glove.6B.100d.txt\", encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word   = values[0]\n",
    "    coefs  = np.asarray(values[1], dtype=\"float32\")\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "print(f\"Se encontraron {len(embedding_index)} vectores de palabras\")\n",
    "\n",
    "# Otra forma de hacer lo mismo\n",
    "# with open(\"glove.6B.100d.txt\",\"r\") as f: \n",
    "#     for line in f: \n",
    "#         word, *coef = line.split() \n",
    "#         embedding_index[word] = np.asarray(coef, dtype=\"float32\") \n",
    " \n",
    "#     print(f\"se encontraron {len(embedding_index)} vectores de palabras\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar lo que el índice de palabras de nuestro `x_tokenizer` contiene. Así que no tienes que hacer esto, sólo estoy haciendo esto para proporcionar un poco de claridad. Así que este es nuestro vocabulario donde cada palabra en el vocabulario es que se mapea a un valor entero único de acuerdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'of': 3,\n",
       " 'and': 4,\n",
       " 'a': 5,\n",
       " 'you': 6,\n",
       " 'i': 7,\n",
       " 'is': 8,\n",
       " 'that': 9,\n",
       " 'in': 10,\n",
       " 'it': 11,\n",
       " 'for': 12,\n",
       " 'this': 13,\n",
       " 'not': 14,\n",
       " 'on': 15,\n",
       " 'be': 16,\n",
       " 'as': 17,\n",
       " 'have': 18,\n",
       " 'are': 19,\n",
       " 'your': 20,\n",
       " 'with': 21,\n",
       " 'if': 22,\n",
       " 'article': 23,\n",
       " 'was': 24,\n",
       " 'or': 25,\n",
       " 'but': 26,\n",
       " 'page': 27,\n",
       " 'wikipedia': 28,\n",
       " 'my': 29,\n",
       " 'an': 30,\n",
       " 'from': 31,\n",
       " 'by': 32,\n",
       " 'do': 33,\n",
       " 'at': 34,\n",
       " 'me': 35,\n",
       " 'about': 36,\n",
       " 'so': 37,\n",
       " 'talk': 38,\n",
       " 'what': 39,\n",
       " 'can': 40,\n",
       " 'there': 41,\n",
       " 'all': 42,\n",
       " 'has': 43,\n",
       " 'will': 44,\n",
       " 'please': 45,\n",
       " 'no': 46,\n",
       " 'would': 47,\n",
       " 'one': 48,\n",
       " 'like': 49,\n",
       " 'just': 50,\n",
       " 'they': 51,\n",
       " 'he': 52,\n",
       " 'which': 53,\n",
       " 'any': 54,\n",
       " 'been': 55,\n",
       " 'should': 56,\n",
       " 'more': 57,\n",
       " 'we': 58,\n",
       " \"don't\": 59,\n",
       " 'some': 60,\n",
       " 'other': 61,\n",
       " 'who': 62,\n",
       " 'here': 63,\n",
       " 'see': 64,\n",
       " 'also': 65,\n",
       " 'his': 66,\n",
       " 'think': 67,\n",
       " 'because': 68,\n",
       " 'know': 69,\n",
       " 'how': 70,\n",
       " 'edit': 71,\n",
       " 'am': 72,\n",
       " \"i'm\": 73,\n",
       " 'people': 74,\n",
       " 'why': 75,\n",
       " 'up': 76,\n",
       " 'only': 77,\n",
       " \"it's\": 78,\n",
       " 'out': 79,\n",
       " 'articles': 80,\n",
       " 'use': 81,\n",
       " 'when': 82,\n",
       " 'then': 83,\n",
       " 'time': 84,\n",
       " 'may': 85,\n",
       " 'were': 86,\n",
       " 'did': 87,\n",
       " 'them': 88,\n",
       " 'now': 89,\n",
       " 'being': 90,\n",
       " 'user': 91,\n",
       " 'their': 92,\n",
       " 'than': 93,\n",
       " 'thanks': 94,\n",
       " 'even': 95,\n",
       " 'get': 96,\n",
       " 'make': 97,\n",
       " 'good': 98,\n",
       " 'had': 99,\n",
       " 'well': 100,\n",
       " 'very': 101,\n",
       " 'information': 102,\n",
       " 'does': 103,\n",
       " 'could': 104,\n",
       " 'want': 105,\n",
       " 'deletion': 106,\n",
       " 'its': 107,\n",
       " 'such': 108,\n",
       " 'sources': 109,\n",
       " 'way': 110,\n",
       " 'name': 111,\n",
       " 'these': 112,\n",
       " 'first': 113,\n",
       " 'wp': 114,\n",
       " 'help': 115,\n",
       " 'pages': 116,\n",
       " 'new': 117,\n",
       " 'image': 118,\n",
       " 'source': 119,\n",
       " 'editing': 120,\n",
       " 'go': 121,\n",
       " 'need': 122,\n",
       " 'section': 123,\n",
       " 'say': 124,\n",
       " 'again': 125,\n",
       " 'edits': 126,\n",
       " 'thank': 127,\n",
       " 'where': 128,\n",
       " 'fuck': 129,\n",
       " 'made': 130,\n",
       " 'many': 131,\n",
       " 'much': 132,\n",
       " 'used': 133,\n",
       " 'really': 134,\n",
       " 'most': 135,\n",
       " 'deleted': 136,\n",
       " 'discussion': 137,\n",
       " 'same': 138,\n",
       " 'find': 139,\n",
       " 'into': 140,\n",
       " 'work': 141,\n",
       " 'those': 142,\n",
       " 'since': 143,\n",
       " \"i've\": 144,\n",
       " 'right': 145,\n",
       " 'point': 146,\n",
       " 'before': 147,\n",
       " 'after': 148,\n",
       " 'add': 149,\n",
       " 'read': 150,\n",
       " 'look': 151,\n",
       " 'over': 152,\n",
       " 'him': 153,\n",
       " 'take': 154,\n",
       " 'two': 155,\n",
       " 'still': 156,\n",
       " 'back': 157,\n",
       " 'wiki': 158,\n",
       " 'someone': 159,\n",
       " 'fact': 160,\n",
       " 'hi': 161,\n",
       " 'too': 162,\n",
       " 'list': 163,\n",
       " 'link': 164,\n",
       " 'own': 165,\n",
       " 'said': 166,\n",
       " 'something': 167,\n",
       " 'going': 168,\n",
       " 'blocked': 169,\n",
       " '1': 170,\n",
       " '2': 171,\n",
       " 'stop': 172,\n",
       " \"you're\": 173,\n",
       " 'content': 174,\n",
       " 'without': 175,\n",
       " 'block': 176,\n",
       " 'under': 177,\n",
       " 'history': 178,\n",
       " 'http': 179,\n",
       " 'our': 180,\n",
       " 'added': 181,\n",
       " 'utc': 182,\n",
       " 'editors': 183,\n",
       " 'another': 184,\n",
       " 'removed': 185,\n",
       " 'her': 186,\n",
       " 'might': 187,\n",
       " 'welcome': 188,\n",
       " 'note': 189,\n",
       " 'however': 190,\n",
       " 'free': 191,\n",
       " 'place': 192,\n",
       " 'sure': 193,\n",
       " 'case': 194,\n",
       " 'never': 195,\n",
       " \"doesn't\": 196,\n",
       " 'done': 197,\n",
       " 'us': 198,\n",
       " 'vandalism': 199,\n",
       " 'reason': 200,\n",
       " 'put': 201,\n",
       " 'comment': 202,\n",
       " 'personal': 203,\n",
       " 'better': 204,\n",
       " \"that's\": 205,\n",
       " 'yourself': 206,\n",
       " 'using': 207,\n",
       " 'seems': 208,\n",
       " 'ask': 209,\n",
       " 'actually': 210,\n",
       " 'question': 211,\n",
       " 'off': 212,\n",
       " 'while': 213,\n",
       " 'feel': 214,\n",
       " 'anything': 215,\n",
       " 'believe': 216,\n",
       " 'links': 217,\n",
       " 'person': 218,\n",
       " 'things': 219,\n",
       " 'both': 220,\n",
       " 'she': 221,\n",
       " 'best': 222,\n",
       " 'comments': 223,\n",
       " 'policy': 224,\n",
       " 'part': 225,\n",
       " 'hope': 226,\n",
       " 'against': 227,\n",
       " \"can't\": 228,\n",
       " 'already': 229,\n",
       " 'keep': 230,\n",
       " 'thing': 231,\n",
       " '3': 232,\n",
       " 'u': 233,\n",
       " \"didn't\": 234,\n",
       " 'questions': 235,\n",
       " \"i'll\": 236,\n",
       " 'com': 237,\n",
       " 'nothing': 238,\n",
       " 'change': 239,\n",
       " 'wrong': 240,\n",
       " 'though': 241,\n",
       " 'subject': 242,\n",
       " 'problem': 243,\n",
       " 'remove': 244,\n",
       " 'little': 245,\n",
       " 'copyright': 246,\n",
       " 'tag': 247,\n",
       " '•': 248,\n",
       " 'trying': 249,\n",
       " 'long': 250,\n",
       " 'must': 251,\n",
       " 'understand': 252,\n",
       " 'above': 253,\n",
       " 'speedy': 254,\n",
       " 'anyone': 255,\n",
       " 'few': 256,\n",
       " 'world': 257,\n",
       " 'issue': 258,\n",
       " 'last': 259,\n",
       " 'others': 260,\n",
       " 'give': 261,\n",
       " 'editor': 262,\n",
       " 'sorry': 263,\n",
       " 'agree': 264,\n",
       " 'reliable': 265,\n",
       " 'rather': 266,\n",
       " 'let': 267,\n",
       " 'years': 268,\n",
       " 'fair': 269,\n",
       " 'english': 270,\n",
       " 'different': 271,\n",
       " 'making': 272,\n",
       " 'reference': 273,\n",
       " 'come': 274,\n",
       " 'style': 275,\n",
       " 'text': 276,\n",
       " 'references': 277,\n",
       " 'mean': 278,\n",
       " 'try': 279,\n",
       " 'non': 280,\n",
       " 'continue': 281,\n",
       " 'doing': 282,\n",
       " 'great': 283,\n",
       " 'found': 284,\n",
       " 'leave': 285,\n",
       " 'word': 286,\n",
       " 'says': 287,\n",
       " 'got': 288,\n",
       " 'state': 289,\n",
       " 'original': 290,\n",
       " \"isn't\": 291,\n",
       " 'probably': 292,\n",
       " 'site': 293,\n",
       " 'adding': 294,\n",
       " 'every': 295,\n",
       " 'check': 296,\n",
       " 'day': 297,\n",
       " 'simply': 298,\n",
       " 'created': 299,\n",
       " 'life': 300,\n",
       " 'top': 301,\n",
       " 'hello': 302,\n",
       " 'show': 303,\n",
       " 'post': 304,\n",
       " 'either': 305,\n",
       " 'consensus': 306,\n",
       " 'ip': 307,\n",
       " 'least': 308,\n",
       " 'delete': 309,\n",
       " 'else': 310,\n",
       " 'e': 311,\n",
       " 'yes': 312,\n",
       " 'view': 313,\n",
       " 'war': 314,\n",
       " 'far': 315,\n",
       " 'notable': 316,\n",
       " 'enough': 317,\n",
       " 'request': 318,\n",
       " 'etc': 319,\n",
       " 'example': 320,\n",
       " 'opinion': 321,\n",
       " 'contributions': 322,\n",
       " 'called': 323,\n",
       " 'around': 324,\n",
       " 'through': 325,\n",
       " 'www': 326,\n",
       " 'between': 327,\n",
       " 'real': 328,\n",
       " 'yet': 329,\n",
       " 'write': 330,\n",
       " 'reverted': 331,\n",
       " 'book': 332,\n",
       " 'shit': 333,\n",
       " 'down': 334,\n",
       " 'matter': 335,\n",
       " 'admin': 336,\n",
       " 're': 337,\n",
       " 'thought': 338,\n",
       " 'given': 339,\n",
       " 'images': 340,\n",
       " 'account': 341,\n",
       " 'material': 342,\n",
       " 'users': 343,\n",
       " 'bad': 344,\n",
       " 'encyclopedia': 345,\n",
       " 'having': 346,\n",
       " 'clearly': 347,\n",
       " 'title': 348,\n",
       " 'message': 349,\n",
       " 'support': 350,\n",
       " 'needs': 351,\n",
       " 'lot': 352,\n",
       " 'old': 353,\n",
       " 'evidence': 354,\n",
       " '—': 355,\n",
       " 'ever': 356,\n",
       " 'maybe': 357,\n",
       " 's': 358,\n",
       " 'tell': 359,\n",
       " 'revert': 360,\n",
       " 'seem': 361,\n",
       " 'language': 362,\n",
       " 'instead': 363,\n",
       " 'correct': 364,\n",
       " 'template': 365,\n",
       " 'org': 366,\n",
       " 'number': 367,\n",
       " 'clear': 368,\n",
       " 'media': 369,\n",
       " 'important': 370,\n",
       " 'saying': 371,\n",
       " 'pov': 372,\n",
       " '5': 373,\n",
       " '4': 374,\n",
       " 'always': 375,\n",
       " 'written': 376,\n",
       " 'true': 377,\n",
       " 'oh': 378,\n",
       " 'term': 379,\n",
       " 'further': 380,\n",
       " 'states': 381,\n",
       " 'hate': 382,\n",
       " 'quite': 383,\n",
       " 'perhaps': 384,\n",
       " 'review': 385,\n",
       " 'until': 386,\n",
       " 'bit': 387,\n",
       " 'whether': 388,\n",
       " \"i'd\": 389,\n",
       " 'research': 390,\n",
       " 'consider': 391,\n",
       " 'claim': 392,\n",
       " 'guidelines': 393,\n",
       " 'fucking': 394,\n",
       " 'version': 395,\n",
       " 'once': 396,\n",
       " 'based': 397,\n",
       " 'criteria': 398,\n",
       " 'times': 399,\n",
       " 'nigger': 400,\n",
       " 'website': 401,\n",
       " 'getting': 402,\n",
       " 'suck': 403,\n",
       " 'mention': 404,\n",
       " 'three': 405,\n",
       " 'several': 406,\n",
       " 'makes': 407,\n",
       " 'considered': 408,\n",
       " 'words': 409,\n",
       " 'c': 410,\n",
       " 'year': 411,\n",
       " 'hey': 412,\n",
       " 'changes': 413,\n",
       " 'idea': 414,\n",
       " \"there's\": 415,\n",
       " 'cannot': 416,\n",
       " 'ass': 417,\n",
       " 'address': 418,\n",
       " 'notice': 419,\n",
       " 'current': 420,\n",
       " 'group': 421,\n",
       " 'left': 422,\n",
       " 'following': 423,\n",
       " 'listed': 424,\n",
       " 'each': 425,\n",
       " 'date': 426,\n",
       " 'second': 427,\n",
       " 'means': 428,\n",
       " 'facts': 429,\n",
       " 'rules': 430,\n",
       " 'general': 431,\n",
       " 'possible': 432,\n",
       " 'main': 433,\n",
       " 'care': 434,\n",
       " 'regarding': 435,\n",
       " 'american': 436,\n",
       " 'man': 437,\n",
       " 'start': 438,\n",
       " '10': 439,\n",
       " 'topic': 440,\n",
       " 'mentioned': 441,\n",
       " 'course': 442,\n",
       " 'attack': 443,\n",
       " 'kind': 444,\n",
       " 'whole': 445,\n",
       " 'statement': 446,\n",
       " 'known': 447,\n",
       " 'end': 448,\n",
       " 'include': 449,\n",
       " 'issues': 450,\n",
       " 'seen': 451,\n",
       " 'create': 452,\n",
       " 'jpg': 453,\n",
       " 'dont': 454,\n",
       " 'en': 455,\n",
       " 'gay': 456,\n",
       " 'less': 457,\n",
       " 'related': 458,\n",
       " 'call': 459,\n",
       " 'ok': 460,\n",
       " 'sense': 461,\n",
       " 'big': 462,\n",
       " 'suggest': 463,\n",
       " 'happy': 464,\n",
       " 'category': 465,\n",
       " 'including': 466,\n",
       " 'notability': 467,\n",
       " 'info': 468,\n",
       " '2005': 469,\n",
       " 'provide': 470,\n",
       " 'redirect': 471,\n",
       " 'days': 472,\n",
       " 'move': 473,\n",
       " 'myself': 474,\n",
       " 'sentence': 475,\n",
       " \"wikipedia's\": 476,\n",
       " 'love': 477,\n",
       " 'four': 478,\n",
       " 'appropriate': 479,\n",
       " 'school': 480,\n",
       " 'news': 481,\n",
       " 'project': 482,\n",
       " 'changed': 483,\n",
       " 'explain': 484,\n",
       " 'started': 485,\n",
       " 'neutral': 486,\n",
       " 'line': 487,\n",
       " 'mind': 488,\n",
       " 'anyway': 489,\n",
       " 'contribs': 490,\n",
       " 'included': 491,\n",
       " 'removing': 492,\n",
       " 'next': 493,\n",
       " 't': 494,\n",
       " 'looking': 495,\n",
       " 'picture': 496,\n",
       " 'specific': 497,\n",
       " 'community': 498,\n",
       " 'although': 499,\n",
       " 'per': 500,\n",
       " 'order': 501,\n",
       " 'relevant': 502,\n",
       " 'sign': 503,\n",
       " 'die': 504,\n",
       " 'answer': 505,\n",
       " 'away': 506,\n",
       " 'interest': 507,\n",
       " 'full': 508,\n",
       " 'warning': 509,\n",
       " 'lol': 510,\n",
       " 'summary': 511,\n",
       " 'recent': 512,\n",
       " 'later': 513,\n",
       " 'file': 514,\n",
       " 'policies': 515,\n",
       " \"you've\": 516,\n",
       " 'faith': 517,\n",
       " 'claims': 518,\n",
       " 'discuss': 519,\n",
       " 'attacks': 520,\n",
       " 'public': 521,\n",
       " '0': 522,\n",
       " 'currently': 523,\n",
       " 'wrote': 524,\n",
       " 'writing': 525,\n",
       " 'especially': 526,\n",
       " 'interested': 527,\n",
       " 'able': 528,\n",
       " 'wish': 529,\n",
       " 'taken': 530,\n",
       " '6': 531,\n",
       " 'names': 532,\n",
       " 'position': 533,\n",
       " 'single': 534,\n",
       " 'within': 535,\n",
       " 'stuff': 536,\n",
       " 'below': 537,\n",
       " '2006': 538,\n",
       " 'during': 539,\n",
       " 'wanted': 540,\n",
       " 'web': 541,\n",
       " 'appears': 542,\n",
       " 'official': 543,\n",
       " '20': 544,\n",
       " 'live': 545,\n",
       " 'certainly': 546,\n",
       " 'nice': 547,\n",
       " 'color': 548,\n",
       " 'self': 549,\n",
       " 'itself': 550,\n",
       " 'country': 551,\n",
       " 'everyone': 552,\n",
       " 'report': 553,\n",
       " 'anti': 554,\n",
       " 'background': 555,\n",
       " 'lead': 556,\n",
       " 'high': 557,\n",
       " 'common': 558,\n",
       " 'god': 559,\n",
       " 'unless': 560,\n",
       " 'according': 561,\n",
       " 'completely': 562,\n",
       " 'hard': 563,\n",
       " 'books': 564,\n",
       " 'pretty': 565,\n",
       " '7': 566,\n",
       " 'everything': 567,\n",
       " 'p': 568,\n",
       " 'published': 569,\n",
       " 'due': 570,\n",
       " '24': 571,\n",
       " 'process': 572,\n",
       " 'edited': 573,\n",
       " 'looks': 574,\n",
       " 'involved': 575,\n",
       " 'fat': 576,\n",
       " 'therefore': 577,\n",
       " \"won't\": 578,\n",
       " 'remember': 579,\n",
       " 'obviously': 580,\n",
       " 'power': 581,\n",
       " 'd': 582,\n",
       " 'future': 583,\n",
       " 'nor': 584,\n",
       " '100': 585,\n",
       " 'truth': 586,\n",
       " 'came': 587,\n",
       " 'sandbox': 588,\n",
       " '11': 589,\n",
       " 'response': 590,\n",
       " 'party': 591,\n",
       " 'reading': 592,\n",
       " 'stay': 593,\n",
       " 'past': 594,\n",
       " 'game': 595,\n",
       " 'learn': 596,\n",
       " 'admins': 597,\n",
       " 'quote': 598,\n",
       " 'asked': 599,\n",
       " \"wasn't\": 600,\n",
       " 'b': 601,\n",
       " 'city': 602,\n",
       " 'entry': 603,\n",
       " 'stupid': 604,\n",
       " \"he's\": 605,\n",
       " 'posted': 606,\n",
       " 'false': 607,\n",
       " 'faggot': 608,\n",
       " 'whatever': 609,\n",
       " 'google': 610,\n",
       " 'talking': 611,\n",
       " 'ago': 612,\n",
       " '8': 613,\n",
       " 'placed': 614,\n",
       " 'political': 615,\n",
       " 'similar': 616,\n",
       " 'today': 617,\n",
       " 'system': 618,\n",
       " 'administrator': 619,\n",
       " 'united': 620,\n",
       " 'argument': 621,\n",
       " 'paragraph': 622,\n",
       " 'working': 623,\n",
       " 'exactly': 624,\n",
       " '2007': 625,\n",
       " 'guy': 626,\n",
       " '12': 627,\n",
       " 'british': 628,\n",
       " 'took': 629,\n",
       " 'useful': 630,\n",
       " 'government': 631,\n",
       " 'search': 632,\n",
       " 'noticed': 633,\n",
       " 'moron': 634,\n",
       " 'regards': 635,\n",
       " 'small': 636,\n",
       " 'reasons': 637,\n",
       " 'side': 638,\n",
       " '2008': 639,\n",
       " 'form': 640,\n",
       " 'national': 641,\n",
       " 'dispute': 642,\n",
       " 'deleting': 643,\n",
       " 'five': 644,\n",
       " 'guess': 645,\n",
       " 'appreciate': 646,\n",
       " 'particular': 647,\n",
       " 'reverting': 648,\n",
       " 'major': 649,\n",
       " 'problems': 650,\n",
       " 'law': 651,\n",
       " '000': 652,\n",
       " '15': 653,\n",
       " 'npov': 654,\n",
       " 'bitch': 655,\n",
       " 'rule': 656,\n",
       " 'banned': 657,\n",
       " 'often': 658,\n",
       " 'provided': 659,\n",
       " 'music': 660,\n",
       " 'become': 661,\n",
       " 'wikiproject': 662,\n",
       " 'needed': 663,\n",
       " 'status': 664,\n",
       " 'reply': 665,\n",
       " 'knowledge': 666,\n",
       " 'tried': 667,\n",
       " 'along': 668,\n",
       " 'almost': 669,\n",
       " 'cheers': 670,\n",
       " 'stated': 671,\n",
       " 'username': 672,\n",
       " 'film': 673,\n",
       " '9': 674,\n",
       " 'taking': 675,\n",
       " 'fine': 676,\n",
       " '–': 677,\n",
       " 'company': 678,\n",
       " 'vandalize': 679,\n",
       " 'present': 680,\n",
       " 'certain': 681,\n",
       " 'white': 682,\n",
       " 'follow': 683,\n",
       " 'sort': 684,\n",
       " 'otherwise': 685,\n",
       " 'terms': 686,\n",
       " 'points': 687,\n",
       " 'explanation': 688,\n",
       " 'uploaded': 689,\n",
       " \"haven't\": 690,\n",
       " 'description': 691,\n",
       " 'generally': 692,\n",
       " 'recently': 693,\n",
       " 'entire': 694,\n",
       " 'open': 695,\n",
       " 'story': 696,\n",
       " 'tags': 697,\n",
       " 'shows': 698,\n",
       " 'alone': 699,\n",
       " 'ban': 700,\n",
       " 'citation': 701,\n",
       " 'short': 702,\n",
       " 'definition': 703,\n",
       " '14': 704,\n",
       " 'cited': 705,\n",
       " 'likely': 706,\n",
       " 'aware': 707,\n",
       " 'g': 708,\n",
       " 'saw': 709,\n",
       " 'class': 710,\n",
       " 'type': 711,\n",
       " 'soon': 712,\n",
       " 'set': 713,\n",
       " 'week': 714,\n",
       " 'indeed': 715,\n",
       " 'band': 716,\n",
       " 'cite': 717,\n",
       " 'decide': 718,\n",
       " 'mr': 719,\n",
       " 'views': 720,\n",
       " '2004': 721,\n",
       " 'appear': 722,\n",
       " 'family': 723,\n",
       " 'simple': 724,\n",
       " 'area': 725,\n",
       " 'guys': 726,\n",
       " 'theory': 727,\n",
       " 'piece': 728,\n",
       " 'contributing': 729,\n",
       " 'contact': 730,\n",
       " 'external': 731,\n",
       " 'result': 732,\n",
       " 'test': 733,\n",
       " 'internet': 734,\n",
       " 'interesting': 735,\n",
       " 'unblock': 736,\n",
       " 'actual': 737,\n",
       " 'improve': 738,\n",
       " 'copy': 739,\n",
       " '16': 740,\n",
       " 'sourced': 741,\n",
       " 'jew': 742,\n",
       " 'told': 743,\n",
       " 'attention': 744,\n",
       " 'proposed': 745,\n",
       " 'obvious': 746,\n",
       " 'moved': 747,\n",
       " 'email': 748,\n",
       " 'uk': 749,\n",
       " 'members': 750,\n",
       " 'various': 751,\n",
       " 'allowed': 752,\n",
       " 'themselves': 753,\n",
       " 'conflict': 754,\n",
       " 'context': 755,\n",
       " \"article's\": 756,\n",
       " 'black': 757,\n",
       " 'university': 758,\n",
       " 'author': 759,\n",
       " 'thus': 760,\n",
       " 'disagree': 761,\n",
       " 'cunt': 762,\n",
       " 'john': 763,\n",
       " 'went': 764,\n",
       " 'citations': 765,\n",
       " 'sites': 766,\n",
       " 'jews': 767,\n",
       " 'actions': 768,\n",
       " 'hand': 769,\n",
       " 'bias': 770,\n",
       " 'previous': 771,\n",
       " 'third': 772,\n",
       " 'hours': 773,\n",
       " 'human': 774,\n",
       " '18': 775,\n",
       " 'works': 776,\n",
       " 'nonsense': 777,\n",
       " 'science': 778,\n",
       " 'ones': 779,\n",
       " 'death': 780,\n",
       " 'action': 781,\n",
       " '17': 782,\n",
       " 'enjoy': 783,\n",
       " \"aren't\": 784,\n",
       " 'job': 785,\n",
       " 'proper': 786,\n",
       " 'longer': 787,\n",
       " 'large': 788,\n",
       " 'together': 789,\n",
       " 'sucks': 790,\n",
       " '\\xa0': 791,\n",
       " '13': 792,\n",
       " 'addition': 793,\n",
       " \"wouldn't\": 794,\n",
       " 'avoid': 795,\n",
       " 'creating': 796,\n",
       " 'happened': 797,\n",
       " '19': 798,\n",
       " 'valid': 799,\n",
       " 'jewish': 800,\n",
       " 'german': 801,\n",
       " 'deal': 802,\n",
       " '21': 803,\n",
       " 'automatically': 804,\n",
       " 'biased': 805,\n",
       " 'proof': 806,\n",
       " 'worked': 807,\n",
       " 'im': 808,\n",
       " 'series': 809,\n",
       " 'dick': 810,\n",
       " 'goes': 811,\n",
       " 'himself': 812,\n",
       " 'seriously': 813,\n",
       " \"what's\": 814,\n",
       " '23': 815,\n",
       " 'level': 816,\n",
       " 'standard': 817,\n",
       " 'f': 818,\n",
       " '2009': 819,\n",
       " 'accepted': 820,\n",
       " 'respect': 821,\n",
       " 'exist': 822,\n",
       " 'available': 823,\n",
       " 'de': 824,\n",
       " 'helpful': 825,\n",
       " 'video': 826,\n",
       " 'comes': 827,\n",
       " '22': 828,\n",
       " 'meaning': 829,\n",
       " \"shouldn't\": 830,\n",
       " 'manual': 831,\n",
       " 'living': 832,\n",
       " 'opinions': 833,\n",
       " 'sex': 834,\n",
       " 'rights': 835,\n",
       " 'act': 836,\n",
       " 'tildes': 837,\n",
       " 'criticism': 838,\n",
       " 'play': 839,\n",
       " '2010': 840,\n",
       " 'necessary': 841,\n",
       " 'calling': 842,\n",
       " 'accept': 843,\n",
       " 'sections': 844,\n",
       " 'indicate': 845,\n",
       " 'personally': 846,\n",
       " 'yeah': 847,\n",
       " '30': 848,\n",
       " 'july': 849,\n",
       " 'hell': 850,\n",
       " 'accurate': 851,\n",
       " 'violation': 852,\n",
       " 'statements': 853,\n",
       " 'pig': 854,\n",
       " 'attempt': 855,\n",
       " 'months': 856,\n",
       " 'assume': 857,\n",
       " 'afd': 858,\n",
       " 'upon': 859,\n",
       " 'historical': 860,\n",
       " 'usually': 861,\n",
       " 'debate': 862,\n",
       " \"let's\": 863,\n",
       " 'pro': 864,\n",
       " 'details': 865,\n",
       " 'multiple': 866,\n",
       " 'blocking': 867,\n",
       " 'south': 868,\n",
       " 'rest': 869,\n",
       " 'tagged': 870,\n",
       " 'width': 871,\n",
       " 'serious': 872,\n",
       " 'doubt': 873,\n",
       " 'record': 874,\n",
       " 'greek': 875,\n",
       " 'm': 876,\n",
       " 'r': 877,\n",
       " \"they're\": 878,\n",
       " 'separate': 879,\n",
       " 'v': 880,\n",
       " 'space': 881,\n",
       " 'situation': 882,\n",
       " 'cause': 883,\n",
       " \"you'll\": 884,\n",
       " 'speak': 885,\n",
       " 'heard': 886,\n",
       " 'explaining': 887,\n",
       " 'okay': 888,\n",
       " 'refer': 889,\n",
       " 'fix': 890,\n",
       " 'run': 891,\n",
       " 'quality': 892,\n",
       " 'data': 893,\n",
       " 'complete': 894,\n",
       " 'penis': 895,\n",
       " 'sock': 896,\n",
       " 'church': 897,\n",
       " 'w': 898,\n",
       " 'messages': 899,\n",
       " 'none': 900,\n",
       " 'india': 901,\n",
       " 'asking': 902,\n",
       " 'august': 903,\n",
       " 'online': 904,\n",
       " 'lack': 905,\n",
       " 'legal': 906,\n",
       " 'period': 907,\n",
       " 'freedom': 908,\n",
       " 'team': 909,\n",
       " 'military': 910,\n",
       " 'rationale': 911,\n",
       " 'behavior': 912,\n",
       " 'prove': 913,\n",
       " 'apparently': 914,\n",
       " 'access': 915,\n",
       " 'special': 916,\n",
       " 'close': 917,\n",
       " 'changing': 918,\n",
       " 'bullshit': 919,\n",
       " 'directly': 920,\n",
       " 'watch': 921,\n",
       " 'culture': 922,\n",
       " 'difference': 923,\n",
       " 'march': 924,\n",
       " 'early': 925,\n",
       " 'box': 926,\n",
       " 'contribute': 927,\n",
       " 'wikipedian': 928,\n",
       " 'existing': 929,\n",
       " 'huge': 930,\n",
       " 'gets': 931,\n",
       " 'html': 932,\n",
       " 'couple': 933,\n",
       " '25': 934,\n",
       " 'among': 935,\n",
       " 'civil': 936,\n",
       " 'warring': 937,\n",
       " 'supposed': 938,\n",
       " 'primary': 939,\n",
       " 'except': 940,\n",
       " 'head': 941,\n",
       " 'countries': 942,\n",
       " 'born': 943,\n",
       " 'meant': 944,\n",
       " 'modern': 945,\n",
       " '50': 946,\n",
       " 'photo': 947,\n",
       " 'described': 948,\n",
       " 'incorrect': 949,\n",
       " 'fish': 950,\n",
       " 'uses': 951,\n",
       " 'disruptive': 952,\n",
       " 'significant': 953,\n",
       " 'field': 954,\n",
       " 'specifically': 955,\n",
       " 'red': 956,\n",
       " 'purpose': 957,\n",
       " 'pillars': 958,\n",
       " 'friend': 959,\n",
       " 'release': 960,\n",
       " 'archive': 961,\n",
       " 'million': 962,\n",
       " 'produce': 963,\n",
       " 'tv': 964,\n",
       " 'error': 965,\n",
       " 'force': 966,\n",
       " 'table': 967,\n",
       " 'earlier': 968,\n",
       " 'business': 969,\n",
       " 'computer': 970,\n",
       " 'june': 971,\n",
       " 'sometimes': 972,\n",
       " 'half': 973,\n",
       " 'cases': 974,\n",
       " 'outside': 975,\n",
       " 'vote': 976,\n",
       " 'x': 977,\n",
       " 'inclusion': 978,\n",
       " 'particularly': 979,\n",
       " 'character': 980,\n",
       " 'pictures': 981,\n",
       " 'gave': 982,\n",
       " 'linked': 983,\n",
       " 'abuse': 984,\n",
       " '27': 985,\n",
       " 'control': 986,\n",
       " 'possibly': 987,\n",
       " 'numbers': 988,\n",
       " 'home': 989,\n",
       " 'anonymous': 990,\n",
       " 'member': 991,\n",
       " 'january': 992,\n",
       " 'christian': 993,\n",
       " 'scientific': 994,\n",
       " 'arguments': 995,\n",
       " 'tutorial': 996,\n",
       " '2012': 997,\n",
       " 'n': 998,\n",
       " 'reported': 999,\n",
       " 'border': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora va a utilizar este diccionario que hemos creado para seguir adelante y crear una matriz de incrustación correspondiente que podemos utilizar con Keras. Es una matriz numérica simple donde la entrada en el índice *i* es el vector preentrenado para la palabra del índice *i* en nuestro vocabulario de vectores. Así que vamos a hacer eso. \n",
    "\n",
    "Nuestra matriz de incrustación debe tener las dimensiones de nuestro número máximo de características, que es de 400, por las dimensiones de nuestra incrustación, que es de 100. Creemos aquí la matriz de incrustación y vamos a inicializarla numérica con ceros. Pero pronto poblaremos esto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((max_features , embedding_dim)) \n",
    "# Imprimimos para ver la forma\n",
    "print(embedding_matrix.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lo que vamos a hacer es mirar las palabras y los índices en nuestro `x_tokenizer`, que hemos creado antes, que es también un diccionario donde contiene la palabra y también el índice correspondiente a la palabra. Así que vamos a tomar la palabra y el índice de eso y de nuestro embedding, vamos a obtener la palabra. Así que vamos a obtener la palabra y obtener su correspondiente vector de incrustación y llenar este vector de incrustación en nuestra matriz de incrustación.\n",
    "\n",
    "Así que por palabra e índice en nuestro `x_tokenizer`, obtenemos el índice de la palabra, que es un diccionario. Así que estamos obteniendo las palabras y sus índices, y nos estamos asegurando de que si nuestro índice es mayor que nuestro `max_features`, no vamos a usarlo. Sólo vamos a salir del bucle. El -1 es porque el índice es cero. Vamos a seguir adelante y romper el bucle. Así que sólo estamos asegurando que estamos limitados por nuestro `max_features`. De lo contrario, podemos crear nuestro podemos obtener nuestro `embedding_vector` usando el `embedding_index` que creamos antes. Así que esto nos dará nuestra una representación numérica o un vector para esa palabra en particular usando nuestro `embedding_index`. \n",
    "\n",
    "Y ahora lo que vamos a hacer es establecer otra condición. Así que si nuestro vector de incrustación no es ninguno, lo que significa que las palabras que no se encuentran en el `embedding_index` serán todos ceros, vamos a crear nuestra matriz de incrustación. Así que la `embedding_matrix`, es simplemente igual al vector de incrustación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in x_tokenizer.word_index.items(): \n",
    "    if index > max_features -1: \n",
    "        break \n",
    "    else: \n",
    "        embedding_vector = embedding_index.get(word) \n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos hecho esto en la siguiente tarea, vamos a cargaremos la matriz de incrustación de palabras preentrenada en un incrustación de palabras en una capa Keras para que no tengamos que aprender estos word embeddings mientras entrenamos nuestra red neuronal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Crear la capa de incrustación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la última tarea, creamos nuestra matriz de incrustación buscando dentro de las palabras que tenemos en nuestro `x_tokenizer` y ajustando los valores de esas palabras a los valores o vectores de embedding de GloVe. En esta tarea, en realidad los valores dentro de nuestra capa de incrustación son para que sean exactamente los embeddings que obtuvimos de GloVe. Y nuestra capa de incrustación va a ser la primera capa dentro de nuestro modelo. \n",
    "\n",
    "Vamos a crear nuestro modelo usando la clase secuencial de Keras y por lo tanto ya hemos importado sequential desde arriba. Y ahora lo que podemos hacer es empezar a crear una capa de incrustación eficiente que mapea los `embedding_index` en las `embedding_dim`. Y así vamos a cargar la palabra pre entrenada y embeddings en esta capa Keras. La forma en que agregamos a la capa de incrustación a nuestro modelo es simplemente llamando `model.add`, y nosotros podemos definir nuestra capa de incrustación y queremos asegurarnos que necesitamos pasar nuestras `max_features` aquí junto con las `embedding_dim`. Y si no estuviéramos usando embeddings pre entrenadas, esto constituiría toda nuestra capa de incrustación. Podríamos seguir añadiendo otras capas como quizás una capa densa aquí o capas de convolución o RNN o LSTM. \n",
    "\n",
    "Pero ya que estamos utilizando nuestros embeddings GloVe pre-entrenado, vamos a seguir adelante y añadir en esta capa el `embedding_initializer` que inicializará los pesos de los embeddings a nuestros valores en la matriz de incrustación. `embedding_initializer` dijimos que es constante, usando `tf.keras.initializers.Constant` y a esto le pasamos nuestra matriz de incrustación. \n",
    "\n",
    "Y por último, también vamos a seguir adelante y establecer `trainable=Falso` porque no queremos aprender más incrustaciones ya que estamos utilizando el pre-entrenado embeddings, por lo que no queremos actualizar estos pesos durante el entrenamiento. \n",
    "\n",
    "Y ahora vamos a seguir adelante y también añadir un poco de regularización dropout, de 0.2 que debe estaría estar bien. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Embedding(max_features, \n",
    "                    embedding_dim, \n",
    "                    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), \n",
    "                    trainable = False)) \n",
    "model.add(Dropout(0.2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, así que ahora que hemos hecho esto, ¿cómo lo convertimos en un clasificador? ¿Y cómo hacemos una convolución de texto, clasificar? y ¿qué significa eso? Y vamos a explorar estas preguntas en la siguiente tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Construir el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bienvenidos de nuevo ahora que hemos transformado nuestro texto en vectores de longitud fija y también los hemos rellenado y recuperado sus embeddings de GloVe y finalmente añadimos esos embeddings a nuestra capa de incrustación en nuestro modelo. ¿Cómo hacemos una convolución clasificador de texto? y ¿qué significa eso? Así que creo que es útil volver y revisar lo que quería decir por un clasificador bidimensional en imágenes. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_07.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Recuerde que con un clasificador 2D, tomamos una entrada, y luego multiplicaríamos un peso por un bloque de valores, y se pone esos valores en un bloque en una posterior imagen de salida y luego eliminar ese bloque y mover ese bloque por uno o por paso. Y hacemos ese mismo cálculo de los mismos pesos. Y luego rellenamos el siguiente bloque en la siguiente imagen. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_08.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Recuerda que podríamos tener múltiples salidas. Y lo que significaría múltiples salidas es que empezamos con la misma imagen, pero usamos diferentes conjuntos de pesos. Y así, a medida que nos deslizamos sobre el bloque, estábamos en realidad en cada caso, multiplicando por diferentes pesos y luego poniendo las múltiples imágenes o a veces lo llaman canales múltiples. Y entonces usted podría tener este concepto de cómo exactamente se puede tomar múltiples entradas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_09.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Así que si tuviéramos tres imágenes de entrada en este caso, en realidad, si tuviéramos una imagen en color, podríamos convertirla en tres canales un canal rojo, un canal verde y un canal azul. Y podemos hacer lo mismo con la convolución. \n",
    "\n",
    "Y en este caso, en realidad tenemos tres cajas diferentes de pesos. Y luego sumamos el resultado de la convolución de cada bloque de pesos en cada uno de los canales de entrada. Y tenemos un solo canal de salida por lo que podemos tener múltiples entradas y salidas múltiples de esta manera y ahora en el texto, nosotros en realidad no tenemos una cosa bidimensional, tenemos una cosa unidimensional. \n",
    "<figure>\n",
    " <img align=\"left\", src=\"./imagenes/1D-Convolutions-Text_Página_13.jpg\"   style=\"width:320px;height:179px;\" >\n",
    "</figure>\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_17.jpg\"   style=\"width:320px;height:179px;\" >\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_18.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Así que aquí se puede pensar en que una dimensión como ir a través de los píxeles a menudo la imagen y se puede pensar en que tengo como un ancho para mencionar aquí que es en realidad el canal diferente. Así que en lugar de tomar un bloque bidimensional, tomamos un un bloque unidimensional a través de los píxeles. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_20.jpg\"   style=\"width:400px;height:224px;\" >\n",
    "</figure>\n",
    "\n",
    "Y así en este caso, digamos que su longitud tres y tomamos una suma ponderada de cada uno de los píxeles. Así que en este caso, tendríamos tres pesos y los multiplicamos por uno de los canales y tomamos eso, esperamos algo y rellenamos una salida, y movemos ese bloque un paso a la derecha y hacemos lo mismo. Esperamos un poco en los nuevos datos de nuestra incrustación, y sentimos en ese resultado en el siguiente canal o en el siguiente píxel, y ejecutamos esa suma ponderada a través de todos los canales y tomamos esperamos algo y rellenamos un valor. \n",
    "\n",
    "Ahora podríamos tener la salida de múltiples dimensiones o múltiples canales de salida, y en este caso, sólo tendríamos diferentes pesos para cada uno de los canales que se fuera poniendo y este caso eran realmente va a aprender las esperas para todos estos canales diferentes. Y lo que va a hacer es combinar las palabras en menores y en cierto sentido nos dará información. O, con suerte, va a aprender información sobre pares \n",
    "y triples y más palabras. \n",
    "\n",
    "\n",
    "Así que puedes recordar que con las imágenes hacemos esta cosa llamada esta operación llamada **Max Polling**, donde tomaríamos un bloque, típicamente un bloque de dos por dos, y encontramos el máximo de los píxeles en una región de dos por dos. Bueno, en realidad hay una analogía muy obvia con esto en la que miramos un canal en particular y tomamos, en este caso, un en un, dos por dos, o simplemente la longitud o podría ser un bloque de longitud diferente. Y encontramos el máximo o el promedio. Con imágenes esto nos dio la oportunidad de encontrar un tipo de rango más largo de dependencias con nuestras convoluciones. Y con esto es exactamente lo mismo. Así que en realidad podemos construir la misma estructura que teníamos para clasificar dígitos con operaciones 2D operaciones que con operaciones 1D en nuestros textos. \n",
    "\n",
    "<figure>\n",
    " <img align=\"left\", src=\"./imagenes/1D-Convolutions-Text_Página_24.jpg\"   style=\"width:320px;height:179px;\" >\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/1D-Convolutions-Text_Página_28.jpg\"   style=\"width:320px;height:179px;\" >\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que es típico tener una convolución, seguida de algún tipo de pooling, seguido de una convolución, seguido de algún tipo de pooling. Y esto continúa durante un par de secuencias, seguido por una capa densa o alguna otra arquitectura híbrida, como un LSTM o RNN.\n",
    "\n",
    "Volvamos a nuestro código y veamos cómo funciona esto realmente. Así que aquí hay una muy buena intuición que se puede tener sobre el uso de las convoluciones de 1D para la clasificación de textos. La CNN es, en cierto sentido, una arquitectura de extracción de características. No constituye una red independiente y útil por sí misma, sino que está pensada para ser integrada en una red red más grande. Como tal vez, después de pasar a través de su CNN para aprender características, podrías pasar una capa densa al final. Así que no es una red útil por sí misma, sino más bien para extraer características y ser integrada en una red más grande y ser entrenada en conjunto para producir el resultado final. Así que la responsabilidad de las capas de la CNN es extraer sub estructuras significativas que sean útiles para la tarea general de predicción, o clasificación en general. \n",
    "\n",
    "Muy bien, así que antes de que podamos empezar a definir nuestra CNN y el resto de la arquitectura del modelo, vamos a crear algunos parámetros. Así que el número de filtros (`filters`) que vamos a utilizar es 250, así que todos estos van a ser hiperparámetros que se pueden ajustar individualmente. Y el tamaño del filtro (`kernel_size`) también llamado alternativamente el ancho de filtro o la longitud del filtro va a ser 3. Y después de extraer las características usando la CNN de 1D, vamos a pasarlo a través de una capa oculta (`hidden_dims`), y podemos podemos pre definir cuál es la dimensión de esa capa oculta. Así que vamos a decir 250. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters      = 250 \n",
    "kernel_size  = 3 \n",
    "hidden_dims  = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora podemos añadir una convolución 1D, que aprenderá los filtros. Vamos a escribir `model.add` y añadir la capa 1D y usando los embeddings, extraemos características con nuestra convolución. Luego pasamos los filtros, y esto gobierna cuántos canales de salida tiene. Especificamos en el ancho del filtro o el `kernel_size` que ya hemos definido en la celda anterior. Y ahora añadimos un relleno llamado *válido*, que sólo significa que en realidad no estamos rellenando datos porque están ya estan rellenos, por lo que es efectivamente encoge la salida.\n",
    "\n",
    "Ahora vamos a añadir un Max Pooling también 1D en este caso, y ahora podríamos crear una capa de salida con una unidad logística o capa densa con una unidad y pasarla por una función de activación sigmoidea. Pero también se puede añadir más capas de convolución 1D, y esto aprovecharía efectivamente el poder de las CNN para aprender jerarquías de relaciones entre n-gramas en el texto. \n",
    "\n",
    "Vamos a añadirlo con diferente tamaño de kernel, esta vez para que aprenda diferentes filtros con diferente longitud de filtro esta vez. Así que en lugar de 3, vamos a ir con 5. Podemos añadir el mismo relleno que es válido y ahora agregar una no linealidad, agregando una función de activación ReLu. \n",
    "\n",
    "Y ahora vamos a utilizar Global Max Pooling de modo que una vez que se hace, podemos pasarlo a través de nuestra, capa oculta. Así que el Global Max Pooling simplemente mustrea hacia abajo las representación de entrada tomando el valor máximo sobre el tiempo de mencionar como nos fijamos en los slice. \n",
    "\n",
    "Y ahora sólo vamos a añadir, en lugar de una capa 1D, añadiremos una capa oculta densa con las dimensiones ocultas que hemos definido. Así que eso sería 250. Y también vamos a añadir la función de activación ReLu. Y sólo para que evitemos el sobreajuste también vamos a añadir un poco de regularización de dropout. \n",
    "\n",
    "Y ahora podemos proyectar la capa de salida y aplastarla con una sigmoide, con una capa densa de una neurona y con la activación a sigmoide. Así que esto nos daría un valor entre 0 y 1. Y si el valor es, digamos, vamos a establecer un umbral de 0,5, podemos decir que el comentario no es tóxico. Si es mayor de 0,5, es tóxico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(filters, \n",
    "                kernel_size = kernel_size, \n",
    "                padding = \"valid\"))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters,\n",
    "                kernel_size = 5, \n",
    "                padding = \"valid\", \n",
    "                activation = \"relu\")) \n",
    "model.add(GlobalMaxPooling1D()) \n",
    "model.add(Dense(hidden_dims, \n",
    "                activation = \"relu\")) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(1, activation = \"sigmoid\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         2000000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 100)         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 250)         75250     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 250)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 250)         312750    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 250)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,451,001\n",
      "Trainable params: 451,001\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Para ver la arquitectura\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que sólo para resumir después de la construcción nuestra capa de embedding y añadir un poco de dropout para que no se sobreajuste demasiado, añadimos una capa conv 1D. Así que esto es como las capas con 2D que se pueden usar en el reconocimiento de vestimenta o cualquier tipo de reconocimiento de imágenes. Y de nuevo, tenemos este parámetro de filtros, que es cuántos canales de salida tiene esta capa de convolución, y también tenemos un parámetro de tamaño de kernel. Pero en lugar de que el tamaño del kernel sean dos número como en las imágenes, es un solo número porque es sólo una convolución unidimensional, y padding es igual a válido, así que en realidad va a reducir nuestra salida un poco. Luego la activación ReLu significa que ejecutamos una función de activación para una no linealidad al final de esta convolución. Luego tenemos una capa de Max Pooling y luego volvemos a otra capa de convolución y otro Max Pooling más tarde. Y finalmente añadimos una capa densa, y algo de regulación dropout. Y, por último, un sólo un valor binario de salida, añadimos esa última capa densa. \n",
    "\n",
    "Y ahora lo que podemos hacer es compilar nuestro modelo y establecer la pérdida binaria de entropía cruzada, y vamos a utilizar el optimizador de Adam. También queremos la precisión para saber qué tan bien lo está haciendo nuestro modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.compile(loss      = \"binary_crossentropy\", \n",
    "               optimizer = \"adam\", \n",
    "               metrics   = [\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigamos adelante y dividamos nuestro conjunto de datos en un conjunto de entrenamiento y el conjunto de validación y luego ajustar nuestros datos de entrenamiento a nuestro modelo y evaluar en los datos de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Entrenar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ha construido y compilado su modelo, vamos a entrenarlo. Pero antes de que podamos hacer eso, tenemos que dividir nuestros datos en conjuntos de entrenamiento y validación. \n",
    "\n",
    "Estás usando la función de ayuda `train_test_split` de SciKitLearn. Ahora podemos especificar nuestro tamaño de la prueba, vamos a establecerlo al 15% del total de los datos y así obtenemos para ambos la misma división. Establezcamos el estado aleatorio a uno. Esto es opcional, de acuerdo, así que eso se encarga de crear nuestros conjuntos de entrenamiento y validación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " x_train , x_val , y_train, y_val = train_test_split( \n",
    "                                        x_train_val,  \n",
    "                                        y ,  \n",
    "                                        test_size = 0.15, \n",
    "                                        random_state = 1) #opcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y lo que podemos hacer ahora, antes de ajustar nuestros datos al modelo, es definir algunos parámetros para el proceso de entrenamiento, como el tamaño del batch que es el número de ejemplos del batch de entrenamiento que el modelo mira a la vez. Vamos a establecer que a 32. Entonces, ¿cuántas veces pasamos sobre los datos de entrenamiento? podemos setearlo en 3.\n",
    "\n",
    "Y ahora podemos seguir adelante y ajustar nuestro modelo. Así que vamos a llamar a `model.fit` y ajustar a nuestro `x_train` e `y_train` y especificar el tamaño de batch que acabamos de definir nosotros. También vamos a establecer el número de épocas (epoch). Y por último, vamos a especificar los datos de validación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4239/4239 [==============================] - 1065s 251ms/step - loss: 0.3121 - accuracy: 0.9044 - val_loss: 0.3130 - val_accuracy: 0.9042\n",
      "Epoch 2/3\n",
      "4239/4239 [==============================] - 1059s 250ms/step - loss: 0.3041 - accuracy: 0.9047 - val_loss: 0.3014 - val_accuracy: 0.9041\n",
      "Epoch 3/3\n",
      "4239/4239 [==============================] - 1056s 249ms/step - loss: 0.3027 - accuracy: 0.9049 - val_loss: 0.3001 - val_accuracy: 0.9041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20473b22f28>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs     = 3\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "          batch_size = batch_size,  \n",
    "          epochs = epochs, \n",
    "          validation_data= (x_val, y_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cuando termines el entrenamiento, deberías tener una precisión de validación de entre 0,94 y 0,97. Así que en cualquier lugar entre ese rango."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8 - Evaluación del modelo - Clasificar los comentarios tóxicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que mi precisión de entrenamiento es de 0.90 y de validación final, es un 90% y eso es que es bastante bueno. Así que abajo que hemos completado el entrenamiento del modelo podemos realmente clasificar los comentarios tóxicos de los datos de prueba que no hemos mirado. Así que vamos a seguir adelante. \n",
    "\n",
    "Así que eso es que tenemos que ir a través del mismo proceso que antes. Pero ahora, como ya hemos escrito las funciones para hacer el trabajo por nosotros, podemos pasar sin problemas por todo el pre procesamiento. Así que primero tenemos que importar los datos. Así que usamos pandas, con método `read_csv` para seguir adelante y cargar el archivo *'test.csv'*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I think its crap that the link to roggenbier i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>, 25 February 2010 (UTC) \\n\\n :::Looking it ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>==Current Position== \\n Anyone have confirmati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this other one from 1897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>:: Wallamoose was changing the cited material ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>==Indefinitely blocked== \\n I have indefinitel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>== Energy  == \\n\\n I have edited the introduct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>:yeah, thanks for reviving the tradition of pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLM Software,NBFC software,Non Banking Financi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@RedSlash, cut it short. If you have sources s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>==================== \\n Deception is the way o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>. \\n\\n           Jews are not a race because y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>:::If Ollie or others think that one list of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153134</th>\n",
       "      <td>\" \\n\\n == Same coffee shop? == \\n\\n My memory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153135</th>\n",
       "      <td>SO many things wrong with that viewpoint - fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153136</th>\n",
       "      <td>\" \\n\\n Unless we have an article for some othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153137</th>\n",
       "      <td>Hannah and Maddie are soooooo awesome and are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153138</th>\n",
       "      <td>:::no problem, I tagged it and cleaned it out....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153139</th>\n",
       "      <td>:PS I've just looked at the history of this ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153140</th>\n",
       "      <td>\"== Your edit to Maungaturoto == \\n Please don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153141</th>\n",
       "      <td>:If you wish to contest the prod, please remov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153142</th>\n",
       "      <td>Balancing the two approaches to psychiatry ( b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153143</th>\n",
       "      <td>Ah, suck my balls.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153144</th>\n",
       "      <td>== Your name mentioned == \\n Hi, I just though...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153145</th>\n",
       "      <td>I've just discovered yet another list: List of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153146</th>\n",
       "      <td>==Wikiproject Video Games assessment== \\n I do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153147</th>\n",
       "      <td>::Consensus for ruining Wikipedia? I think tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153148</th>\n",
       "      <td>== DAP ?  == \\n\\n What's point with DAP ?! Naz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153149</th>\n",
       "      <td>shut down the mexican border withought looking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153150</th>\n",
       "      <td>:Jerome, I see you never got around to this…! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153151</th>\n",
       "      <td>==Lucky bastard== \\n http://wikimediafoundatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153152</th>\n",
       "      <td>==WTF== \\n It's no longer a redlink.  Now what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153153</th>\n",
       "      <td>\" \\n\\n ==\"\"Illness\"\" no shit== \\n Just for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153154</th>\n",
       "      <td>==shame on you all!!!== \\n\\n You want to speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153155</th>\n",
       "      <td>MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153156</th>\n",
       "      <td>\" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153157</th>\n",
       "      <td>:Disagree. Soviet railways need their own arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153158</th>\n",
       "      <td>This idiot can't even use proper grammar when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>. \\n i totally agree, this stuff is nothing bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>== Throw from out field to home plate. == \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>\" \\n\\n == \"\"One of the founding nations of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>\" \\n :::Stop already. Your bullshit is not wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text\n",
       "0       Yo bitch Ja Rule is more succesful then you'll...\n",
       "1       == From RfC == \\n\\n The title is fine as it is...\n",
       "2       \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3       :If you have a look back at the source, the in...\n",
       "4               I don't anonymously edit articles at all.\n",
       "5       Thank you for understanding. I think very high...\n",
       "6       Please do not add nonsense to Wikipedia. Such ...\n",
       "7                        :Dear god this site is horrible.\n",
       "8       \" \\n Only a fool can believe in such numbers. ...\n",
       "9       == Double Redirects == \\n\\n When fixing double...\n",
       "10      I think its crap that the link to roggenbier i...\n",
       "11      \"::: Somebody will invariably try to add Relig...\n",
       "12      , 25 February 2010 (UTC) \\n\\n :::Looking it ov...\n",
       "13      \" \\n\\n It says it right there that it IS a typ...\n",
       "14      \" \\n\\n == Before adding a new product to the l...\n",
       "15      ==Current Position== \\n Anyone have confirmati...\n",
       "16                               this other one from 1897\n",
       "17      == Reason for banning throwing == \\n\\n This ar...\n",
       "18      :: Wallamoose was changing the cited material ...\n",
       "19                 |blocked]] from editing Wikipedia.   |\n",
       "20      ==Indefinitely blocked== \\n I have indefinitel...\n",
       "21      == Arabs are committing genocide in Iraq, but ...\n",
       "22      Please stop. If you continue to vandalize Wiki...\n",
       "23      == Energy  == \\n\\n I have edited the introduct...\n",
       "24      :yeah, thanks for reviving the tradition of pi...\n",
       "25      MLM Software,NBFC software,Non Banking Financi...\n",
       "26      @RedSlash, cut it short. If you have sources s...\n",
       "27      ==================== \\n Deception is the way o...\n",
       "28      . \\n\\n           Jews are not a race because y...\n",
       "29      :::If Ollie or others think that one list of t...\n",
       "...                                                   ...\n",
       "153134  \" \\n\\n == Same coffee shop? == \\n\\n My memory ...\n",
       "153135  SO many things wrong with that viewpoint - fro...\n",
       "153136  \" \\n\\n Unless we have an article for some othe...\n",
       "153137  Hannah and Maddie are soooooo awesome and are ...\n",
       "153138  :::no problem, I tagged it and cleaned it out....\n",
       "153139  :PS I've just looked at the history of this ar...\n",
       "153140  \"== Your edit to Maungaturoto == \\n Please don...\n",
       "153141  :If you wish to contest the prod, please remov...\n",
       "153142  Balancing the two approaches to psychiatry ( b...\n",
       "153143                                 Ah, suck my balls.\n",
       "153144  == Your name mentioned == \\n Hi, I just though...\n",
       "153145  I've just discovered yet another list: List of...\n",
       "153146  ==Wikiproject Video Games assessment== \\n I do...\n",
       "153147  ::Consensus for ruining Wikipedia? I think tha...\n",
       "153148  == DAP ?  == \\n\\n What's point with DAP ?! Naz...\n",
       "153149  shut down the mexican border withought looking...\n",
       "153150  :Jerome, I see you never got around to this…! ...\n",
       "153151  ==Lucky bastard== \\n http://wikimediafoundatio...\n",
       "153152  ==WTF== \\n It's no longer a redlink.  Now what...\n",
       "153153  \" \\n\\n ==\"\"Illness\"\" no shit== \\n Just for the...\n",
       "153154  ==shame on you all!!!== \\n\\n You want to speak...\n",
       "153155  MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...\n",
       "153156  \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...\n",
       "153157  :Disagree. Soviet railways need their own arti...\n",
       "153158  This idiot can't even use proper grammar when ...\n",
       "153159  . \\n i totally agree, this stuff is nothing bu...\n",
       "153160  == Throw from out field to home plate. == \\n\\n...\n",
       "153161  \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162  \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163  \" \\n :::Stop already. Your bullshit is not wel...\n",
       "\n",
       "[153164 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\") \n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, se ha completado la carga, y ahora vamos a extraer los comentarios de la misma. Así que los comentarios recordar se almacenan en esta columna llamada *comment_text* y obtenemos los valores obtenemos todos los comentarios, los almacenamos en `x_test` y ahora lo que tenemos que hacer es simplemente tekenizar el texto y rellenar para que todos los comentarios sean secuencias tengan la misma longitud de 400. Y para hacer esto estamos utilizando el mismo `x_tokenizer` como antes, ya que ha aprendido las estadísticas de nuestros datos. Así que utilizamos la función `texts_to_sequences` y pasar en nuestros comentarios `x_test`. \n",
    "\n",
    "Y ahora vamos a obtener el resultado después de padding estas secuencias tokenizadase. Y aquí podemos pasar la longitud máxima que utilizamos antes que se elevó a 400. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_df.comment_text.values \n",
    "x_test_token = x_tokenizer.texts_to_sequences(x_test)\n",
    "x_testing = sequence.pad_sequences(x_test_token, maxlen = max_text_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora que está hecho. Podemos utilizar nuestro modelo de entrenamiento para predecir las etiquetas de clase, como en los comentarios, ser tóxico o no, utilizando la función `model.predict` y pasando los datos procesados de los comentarios y vamos a establecer el tamaño de batch a 32 o 128. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4787/4787 [==============================] - 312s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "y_testing = model.predict(x_testing, verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "PvVbtuzBsHDO",
    "outputId": "6cfea216-0353-4032-a69d-cb5a1f915fbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que tenemos ah tienen alrededor de 153.000, los comentarios aquí que son tóxicos y no tóxicos. Y es sólo una columna.\n",
    "\n",
    "Podemos seguir adelante y mirar el primer valor aquí. Así que esto nos dará una indicación de si la primera entrada o comentario en la prueba es tóxico o no. Así que vamos a establecer nuestro umbral de clasificación en 0,5, lo que significa que si tenemos un valor superior a 0,5 significa que el comentario es tóxico. Si tiene un valor inferior a 0,5 no es tóxico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "UkOBp_TDsHDj",
    "outputId": "3780a302-1f23-4115-bf02-f06f10da8067"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05167863], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_testing[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que, nuestro modelo es bastante seguro que el primer comentario aquí es tóxico, así que vamos a ver en el primer comentario. Y debido a que esto es fuera de poner las puntuaciones de probabilidad entre 0 y 1, podemos convertir estas dos etiquetas utilizando una condición False. Así que vamos a hacer eso y vamos a añadir las etiquetas del conjunto test. Así que vamos a crear una columna en nuestra `test_df` llamado *'Tóxico'*, y lo que vamos a hacer es mirar en cada valor en nuestras predicciones. Así que, como este. Así que vamos a escribir que para x en y_testing, vamos a ver si el valor de x es menor que 0,5. Y si lo es, diremos que no es tóxico, de lo contrario, nuestra etiqueta va a ser tóxico. \n",
    "\n",
    "Y en la siguiente línea, vamos a echar un vistazo a las primeras 10 o 20 entradas de nuestra prueba. Si ahora recordamos que nuestro marco de datos de prueba vino con un Id, así que sólo queremos mirar el texto del comentario en sí y la etiqueta tóxica, y así vamos a filtrar para esas columnas específicas. Así que el comentario, el texto y las etiquetas las etiquetas previstas en este caso, ya sea tóxico o no tóxico. Y vamos a ver la cabeza y los primeros 20 valores en este y nuestro modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>Toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I think its crap that the link to roggenbier i...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>, 25 February 2010 (UTC) \\n\\n :::Looking it ov...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>==Current Position== \\n Anyone have confirmati...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this other one from 1897</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>:: Wallamoose was changing the cited material ...</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
       "      <td>No toxico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment_text      Toxic\n",
       "0   Yo bitch Ja Rule is more succesful then you'll...  No toxico\n",
       "1   == From RfC == \\n\\n The title is fine as it is...  No toxico\n",
       "2   \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...  No toxico\n",
       "3   :If you have a look back at the source, the in...  No toxico\n",
       "4           I don't anonymously edit articles at all.  No toxico\n",
       "5   Thank you for understanding. I think very high...  No toxico\n",
       "6   Please do not add nonsense to Wikipedia. Such ...  No toxico\n",
       "7                    :Dear god this site is horrible.  No toxico\n",
       "8   \" \\n Only a fool can believe in such numbers. ...  No toxico\n",
       "9   == Double Redirects == \\n\\n When fixing double...  No toxico\n",
       "10  I think its crap that the link to roggenbier i...  No toxico\n",
       "11  \"::: Somebody will invariably try to add Relig...  No toxico\n",
       "12  , 25 February 2010 (UTC) \\n\\n :::Looking it ov...  No toxico\n",
       "13  \" \\n\\n It says it right there that it IS a typ...  No toxico\n",
       "14  \" \\n\\n == Before adding a new product to the l...  No toxico\n",
       "15  ==Current Position== \\n Anyone have confirmati...  No toxico\n",
       "16                           this other one from 1897  No toxico\n",
       "17  == Reason for banning throwing == \\n\\n This ar...  No toxico\n",
       "18  :: Wallamoose was changing the cited material ...  No toxico\n",
       "19             |blocked]] from editing Wikipedia.   |  No toxico"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"Toxic\"] = [\"No toxico\" if x < 0.5 else \"Toxico\" for x in y_testing]\n",
    "test_df[['comment_text', 'Toxic']].head(20)\n",
    "#Para ver 20 comentario aleatorios agregar\n",
    "#.sample(20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df[\"Toxico\"] = [\"toxico\" if y > 0.2 else \"No Toxico\" for y in y_testing]\n",
    "#test_df[['comment_text', 'Toxic']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que tenemos tres comentarios tóxicos hasta ahora en las primeras 20 entradas, por lo que puede ver claramente que la regla de trabajo yo bleep es más exitosa de lo que tú nunca serás. \n",
    "\n",
    "Y con eso, llegamos al final de este proyecto. Hemos aprendido un par de cosas realmente importantes. En primer lugar, aprendimos cómo podemos preprocesar datos de texto, específicamente para la clasificación de texto de aprendizaje profundo. Y también aprendimos cómo se puede utilizar word embeddings que es realmente práctico en todo el mundo, no sólo en esta aplicación. Y acabamos de aprender sobre un ejemplo de cómo emplear word embeddings para clasificaciones de texto. Y por último, aprendimos cómo tomar convoluciones y pooling y todas estas otras operaciones y conceptos que aplicamos y operamos en imágenes y los aplicamos al texto de una manera realmentepráctica para obtener una alta precisión en la clasificación de los comentarios en línea como tóxicos o no tóxicos. Y espero que hayan aprendido a ver las convoluciones bajo una nueva luz hoy, no sólo con una aplicación en imágenes, donde son sino también en aplicaciones de procesamiento de lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "3-pxZLFQw3VR",
    "outputId": "ea865536-9643-455f-899d-a4ae290af3a1"
   },
   "outputs": [],
   "source": [
    "# Plot frequency of toxic comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZs1wuFjhxss"
   },
   "source": [
    "### Task 3: Data Prep — Tokenize and Pad Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df.comment_text.values\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descaga de embedings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\") \n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.Toxico.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08VYEZ8DqwRH"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Su5kx6yjsHEQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Toxic Comments Classification using 1D CNN with Keras.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a32cce1f9b3e4570ca846caf6903fa722a4c567520840cb717f6823875f1f99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
