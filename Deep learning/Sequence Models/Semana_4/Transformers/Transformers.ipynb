{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMERS\n",
    "\n",
    "*Esta notebook esta creada con las explicaciones que se encuentran en el [blog codificandobits](https://www.codificandobits.com/blog/redes-transformer/), esta misma información se explica en el [canal de Youtube codificandobits](https://www.youtube.com/watch?v=Wp8NocXW_C4&t=1s). Los contenidos y figuras se han obtenido/adaptado directamente de esta fuente. Todo el mérito es de Miguel Sotaquirá. Simplemente espero que los apuntes sirvan como material de estudio complementario.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "Desde hace aproximadamente tres años se viene gestando otra gran revolución en la Inteligencia Artificial.\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./imagenes/01.jpg\"   style=\"width:636px;height:205px;\" >\n",
    "</figure>\n",
    "\n",
    "- En 2017 aparece una publicación que transforma literalmente la concepción de lo que la Inteligencia Artificial es capaz de hacer. Aparecen los **Transformers**. Desarrollado por investigadores de Google, y nacieron inicialmente como una alternativa al problema de la traducción de texto de un idioma a otro.\n",
    "- En Febrero de 2019 OpenAI creó el modelo **GPT-2** capaz de sintetizar texto muy similar al que generaría un ser humano. \n",
    "- En Abril de 2020 Facebook liberó el código fuente de **BlenderBot**, un chatbot que se percibe más humano en una conversación que una persona real. \n",
    "- En Mayo de 2020 lanzó **GPT-3**, un modelo de natural language processing 10 veces más grande que su antecesor y mucho más poderoso: alimentado con tan solo una corta frase de inicio, este modelo de Machine Learning es capaz de escribir una historia completa, de manera bastante convincente, siendo incluso capaz de engañar a un ser humano.\n",
    "- A comienzos de Junio de 2020 también presentó **TransCoder**, un modelo capaz de tomar código en Python y convertirlo en código C++, ¡y esto a pesar de que nunca fue entrenado para esta tarea! El Transcoder de Facebook permite traducir código de un lenguaje de programación a otro.\n",
    "\n",
    "Detrás de esta nueva revolución están las Redes Transformer, que nacieron en el contexto del Procesamiento del Lenguaje Natural (o NLP por sus siglas en Inglés: Natural Language Processing), un área del Machine Learning que busca que los computadores logren entender, interpretar y manipular el lenguaje humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Recurrentes vs Transformers\n",
    "\n",
    "### Inconveniente principal de las Redes Recurrentes:\n",
    "\n",
    "- Tienen una **memoria de corto plazo**:\n",
    "\n",
    "    *Por ejemplo, para la generación de texto la red podrá producir frases relativamente cortas y bastante coherentes. Pero cuando la secuencia generada es extensa, la red no está en capacidad de tener como referencia el texto que apareció al inicio, y así el texto generado no será del todo coherente*\n",
    "    Las Redes Recurrentes tienen una memoria de corto plazo, lo que hace que no sean capaces de generar/analizar largas secuencias. En rojo el texto generado incorrectamente por una Red Recurrente.\n",
    "\n",
    "<figure>\n",
    " <img align=\"center\", src=\"./imagenes/02.jpg\"   style=\"width:568px;height:117px;\" >\n",
    "</figure>\n",
    "\n",
    "- Además el texto es **procesado de manera secuencial**, es decir palabra por palabra.\n",
    "\n",
    "### Ventajas de las Redes Transformer:\n",
    "\n",
    "- Tienen una **memoria de mucho más largo plazo**:\n",
    "    Pues logran analizar secuencias muy extensas usando un mecanismo que se llama *atención*.\n",
    "    \n",
    "- Adicionalmente logran procesar toda la **secuencia en paralelo**, y no en serie como ocurre con las Redes Recurrentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Red Transformer\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/03.jpg\"   style=\"width:50%\" >\n",
    "</figure>\n",
    "\n",
    "Como se observa en el diagrama de bloques general de una Red Transformer, la secuencia de entrada es procesada primero por el **codificador** (izquierda) y luego es enviada al **decodificador** (derecha)\n",
    "\n",
    "Esta secuencia es inicialmente convertida en una representación numérica usando un **embedding**.\n",
    "\n",
    "Después se añade una **codificación de posición** y los vectores resultantes ingresan a la **etapa de codificación**, que se encarga de extraer la información mas relevante de la secuencia en su idioma original.\n",
    "\n",
    "La salida de esta etapa se conecta al decodificador, que toma esta información para generar secuencialmente el texto traducido al segundo idioma.\n",
    "\n",
    "Aunque tiene muchos elementos realmente es sencillo entender cómo funciona. Veamos entonces en detalle cada módulo de esta red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODIFICADOR\n",
    "\n",
    "Esta formado por:\n",
    "\n",
    "- **Embedin de entrada**\n",
    "- **Codificación de posición**\n",
    "- **Bloque de Codificación**\n",
    "- **Resultado Final**\n",
    "\n",
    "### EMBEDDING DE ENTRADA\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/04.jpg\"   style=\"width:30\" >\n",
    "</figure>\n",
    "\n",
    "Primero está el **bloque embedding**, que es simplemente un algoritmo que convierte el texto en una serie de vectores, o tokens, es decir en una *representación numérica* que puede ser “comprendida” por la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODIFICACIÓN DE POSICIÓN\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/05.jpg\"   style=\"width:30%\" >\n",
    "</figure>\n",
    "\n",
    "Como la secuencia se procesa en paralelo es necesario indicarle a la red el orden en el que se encuentran las palabras dentro del texto. Esto se logra con el codificador de posición.\n",
    "\n",
    "Este codificador genera una serie de vectores que se sumarán a los tokens, y que indican la posición relativa de cada token dentro de la secuencia. Para esto se usan funciones senoidales para las posiciones pares, y cosenoidales para las impares, con lo que cada vector generado tendrá un patrón numérico único con la información de la posición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE DE CODIFICACIÓN\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/06.jpg\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "Ahora viene el bloque de codificación, que contiene seis codificadores, todos con una estructura idéntica. Analicemos en detalle uno de estos codificadores.\n",
    "\n",
    "Cada codificador tiene cuatro elementos: \n",
    "- un **bloque atencional**, \n",
    "- un **bloque de conexión residual**, \n",
    "- una **red neuronal** y \n",
    "- otro **bloque de conexión residual**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLOQUE ATENCIONAL\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/07.jpg\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "Veamos en detalle el bloque atencional, que es tal vez el más importante de toda la red, pues se encarga de analizar la totalidad de la secuencia de entrada (recordemos que la red la procesa de manera simultánea) y de encontrar relaciones entre varias palabras de esta secuencia.\n",
    "\n",
    "Por ejemplo, si el texto de entrada es “*I love Italian food*”, podemos ver que hay al menos dos posibles asociaciones entre palabras: el verbo “*love*” y el sujeto (“*I*”) y el sustantivo “*food*” asociado al adjetivo “*Italian*”. Pero además entre estas dos frases (\"*I Love*\" e \"*Italian Food*\") también hay una asociación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/08.jpg\"   style=\"width:45%;\" >\n",
    "</figure>\n",
    "\n",
    "Así, lo que hace el bloque atencional es expresar numéricamente las relaciones que existen a diferentes niveles dentro de la secuencia, y luego codifica cada una de ellas con esta información del contexto, indicando así cuáles son los elementos del texto a los que se deben prestar más atención al momento de hacer la traducción.\n",
    "\n",
    "Esta es precisamente la manera como las redes transformer “*comprenden*” este contexto para codificar adecuadamente cada palabra.\n",
    "\n",
    "Para lograr esto en primer lugar los **tokens** se llevan simultáneamente a tres pequeñas redes neuronales, entrenadas para calcular los vectores “**query**”, “**key**” y “**value**”. Estos vectores son simplemente tres representaciones alternativas de los tokens originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/09.jpg\"   style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "Después de esto se toma el **query** de cada **token** y se compara con cada uno de los **keys** existentes. Esta comparación es simplemente una multiplicación de vectores, y con esto se obtendrá un puntaje que mide el grado de asociación entre pares de palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/10.jpg\"   style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "Así, para el caso de la frase que queremos traducir, si analizamos la palabra “*Italian*” los puntajes obtenidos indican que al codificar este **token** se le debería prestar más atención a la propia palabra “*Italian*” seguida por la palabra “*food*”, y se debería enfocar menos en las palabras “*love*” y “*I*”, que tienen los menores puntajes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/11.jpg\"   style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "La idea es ahora usar estos puntajes para ponderar cada uno de los vectores **values**, indicando así la importancia de cada palabra al momento de la codificación de los **tokens**.\n",
    "\n",
    "Para poder hacer esto se deben escalar los PUNTAJES, dividiéndolos primero entre el tamaño de cada vector, y luego llevándolos a una función **softmax**. Esta función permite simplemente representar cada puntaje como una probabilidad entre cero y uno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/12.jpg\"   style=\"width:45%;\" >\n",
    "</figure>\n",
    "\n",
    "- Un valor cercano a 1 indica que la red debe prestarle más atención a ese **token** en particular, \n",
    "- Un valor cercano a 0 que la palabra no es muy relevante.\n",
    "\n",
    "Finalmente, se debe condensar toda esta información resultante de la comparación en un solo vector por cada **token**. Así que tomamos la MATRIZ DE PUNTAJES que acabamos de obtener y la multiplicamos por la MATRIZ DE VALORES: el resultado serán cuatro nuevos **tokens**, que contendrán la codificación de la información de contexto más relevante para cada palabra de la secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/13.jpg\"   style=\"width:35%;\" >\n",
    "</figure>\n",
    "\n",
    "Así que, en resumen, el **bloque atencional** toma los tokens iniciales y codifica en los tokens resultantes los elementos de la secuencia a los que se debe dar más relevancia.\n",
    "\n",
    "Sin embargo recordemos que en nuestra frase original encontramos, además de asociaciones entre palabras, asociaciones entre frases: para traducir la porción “*Italian food*” se necesita prestar atención a “*I love*”.\n",
    "\n",
    "Así que un solo bloque atencional no es suficiente. Al usar **múltiples bloques atencionales** es posible detectar y codificar asociaciones entre palabras y grupos de palabras a diferentes niveles.\n",
    "\n",
    "Las salidas de estos bloques se combinan en una última **red neuronal** que condensa toda la información resultante en un único vector para cada token de entrada\n",
    "\n",
    "Múltiples bloques atencionales permiten detectar asociaciones entre palabras y grupos de palabras a diferentes niveles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLOQUE RESIDUAL\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/14.jpg\"   style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "A este bloque se llevan tanto la entrada como la salida del bloque atencional, y esto se hace pues la red es muy profunda y si tan solo se enviara a la salida la información progresivamente se degradaría y esto dificultaría el entrenamiento y desempeño de la red.\n",
    "\n",
    "Esta etapa toma los dos datos, los suma y luego los normaliza para que tengan la escala adecuada requerida por el siguiente bloque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RED NEURONAL seguida de BLOQUE RESIDUAL\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/15.jpg\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "La **red neuronal** procesa en paralelo todos los vectores de la secuencia, tomando la información atencional de las capas anteriores y consolidándola en una única representación. La entrada y la salida de esta red neuronal son luego llevadas a un **bloque residual** que tiene exactamente las mismas características del bloque anterior: una suma seguida por una normalización de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTADO FINAL\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/16.jpg\"   style=\"width:30%;\" >\n",
    "</figure>\n",
    "\n",
    "Y con esto está listo el primer codificador.\n",
    "\n",
    "Así que en resumen este bloque toma los tokens de entrada, los procesa en paralelo y entrega a la salida una representación que contiene información atencional sobre las diferentes relaciones entre palabras o grupos de palabras de la secuencia, importantes al momento de la traducción.\n",
    "\n",
    "Y este proceso se repite para los codificadores restantes, que son idénticos en estructura al codificador que acabamos de analizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODIFICADOR\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/17.jpg\"   style=\"width:30%;\" >\n",
    "</figure>\n",
    "\n",
    "Ahora nos enfocamos el segundo bloque importante de la red transformer, que se encarga de hacer la traducción.\n",
    "Esta formado por:\n",
    "\n",
    "- Los **bloques de embedding de salida** y \n",
    "- Un **codificador posicional**, que cumplen exactamente la misma función de los bloques que vimos en la etapa de codificación.\n",
    "- El **decodificador**, que es muy similar al bloque de codificación: en total cuenta con 6 decodificadores, cada uno de ellos conectado al codificador, lo que permite conocer la información atencional de la entrada, en el idioma original, para poder realizar la traducción.\n",
    "    Cada decodificador es similar a los bloques de codificación que vimos anteriormente: cuenta con ***bloques atencionales***, ***residuales*** y ***redes neuronales*** que tienen la misma estructura de los codificadores. Sin embargo tienen un **bloque atencional de enmascaramiento** y un **bloque residual adicionales*.\n",
    "- Una **capa lineal** que, junto con la **capa softmax**, permite generar una a una las palabras de la secuencia de salida.\n",
    "\n",
    "Veamos entonces cómo funciona paso a paso la decodificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE ATENCIONAL CON ENMASCARAMIENTO\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/18.jpg\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "La traducción comienza con la palabra clave “*inicio*”, la cual es codificada con el **embedding** y **codificación posicional**.\n",
    "\n",
    "Al ingresar al primer **decodificador** es procesada por el ***bloque atencional de enmascaramiento***. Este bloque es prácticamente idéntico al *bloque atencional* visto anteriormente: codifica la relación entre diferentes elementos de la secuencia de salida, usando los *queries*, *keys* y *values* vistos anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/19.jpg\"   style=\"width:30%;\" >\n",
    "</figure>\n",
    "\n",
    "Pero con una diferencia importante: como se está generando cada palabra de manera secuencial, una a una, el decodificador debe prestar atención únicamente a la palabra generada actualmente y a las anteriores, no a las futuras.\n",
    "\n",
    "Por ejemplo, si en la secuencia traducida nos ubicamos en la palabra “*la*”, el decodificador debería tener acceso a esta palabra y a “*amor*”, pero no a palabras que aparecerán posteriormente en la secuencia (“*comida*” e “*italiana*”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/20.jpg\"   style=\"width:30%;\" >\n",
    "</figure>\n",
    "\n",
    "Así que para evitar esto se agrega un bloque que enmascara, es decir que simplemente hace cero, las palabras a las que durante la decodificación no se debe prestar atención.\n",
    "\n",
    "Al igual que con el codificador, en este caso también se emplean múltiples bloques atencionales para detectar relaciones a diferentes niveles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE ATENCIONAL\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/21.jpg\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "Todos los bloques residuales, así como la red neuronal de este decodificador funcionan de forma idéntica a como ocurría en los codificadores.\n",
    "\n",
    "Así que nos enfocaremos ahora en el bloque atencional que en este caso tiene la misma estructura pero un funcionamiento ligeramente diferente al del codificador.\n",
    "\n",
    "Este bloque enfoca su atención tanto en la secuencia original como en la de salida y para ello toma la salida del codificador y las lleva a las redes “**queries**” y “**keys**”, mientras que el nodo “**values**” usa como entrada el dato proveniente del **bloque residual** anterior.\n",
    "\n",
    "El bloque atencional del decodificador toma como entrada la información generada por el codificador de la etapa anterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/22.jpg\"   style=\"width:40%;\" >\n",
    "</figure>\n",
    "\n",
    "Es de esta manera como el codificador le indica al decodificador a qué elementos debe prestar más atención al momento de generar la secuencia de salida.\n",
    "\n",
    "De nuevo, se usan múltiples bloques atencionales de manera simultánea para codificar asociaciones a diferentes niveles.\n",
    "\n",
    "Este bloque se replica un total de seis veces, y al final genera un vector con cantidades numéricas, y lo único que falta es convertirlo en una palabra:\n",
    "\n",
    "En la decodificación también se requieren múltiples decodificadores para encontrar relaciones entre palabras y grupos de palabras a diferentes niveles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPA LINEAL Y CAPA SORFTMAX\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/23.jpg\"   style=\"width:30%;\" >\n",
    "</figure>\n",
    "\n",
    "Para eso se usa en primer lugar la **capa lineal**, que es simplemente una red neuronal que toma el vector producido por el decodificador y lo transforma en un vector mucho más grande.\n",
    "\n",
    "Por ejemplo, si el traductor aprende 10000 palabras (es decir el tamaño del vocabulario), entonces el vector de salida de la capa lineal tendrá precisamente 10000 elementos.\n",
    "\n",
    "La **capa softmax** toma cada elemento de este vector y lo convierte en una probabilidad, todas con valores positivos entre 0 y 1. La posición con la probabilidad más alta será seleccionada y la palabra asociada con dicha posición será precisamente la salida del modelo en ese instante de tiempo.\n",
    "\n",
    "Y el proceso se repite hasta generar la totalidad de la secuencia de salida."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
