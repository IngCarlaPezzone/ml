{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 1  \n",
    "A Transformer Network processes sentences from left to right, one word at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True\n",
    "- **False**\n",
    "\n",
    "A Transformer Network can ingest entire sentences all at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 2  \n",
    "Transformer Network methodology is taken from: (Check all that apply)\n",
    "\n",
    "- None of these\n",
    "- Convolutional neural network style of architecture\n",
    "- **Attention mechanism**\n",
    "- **Convolutional Neural Network style of processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 3  \n",
    "What are the key inputs to computing the attention value for each word?\n",
    "\n",
    "<img src=\"pregunta_3.png\">\n",
    "\n",
    "- The key inputs to computing the attention value for each word are called the quotation, knowledge, and value.\n",
    "\n",
    "- **The key inputs to computing the attention value for each word are called the query, key, and value.**\n",
    "\n",
    "- The key inputs to computing the attention value for each word are called the quotation, key, and vector.\n",
    "\n",
    "- The key inputs to computing the attention value for each word are called the query, knowledge, and vector.\n",
    "\n",
    "The key inputs to computing the attention value for each word are called the query, key, and value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 4  \n",
    "Which of the following correctly represents Attention ?\n",
    "\n",
    "\n",
    "- $ Attention(Q, K, V) = softmax(\\frac{QV^T}{\\sqrt{d_k}})K $\n",
    "- $ Attention(Q, K, V) = min(\\frac{QK^T}{\\sqrt{d_k}})V $\n",
    "- $ Attention(Q, K, V) = min(\\frac{QV^T}{\\sqrt{d_k}})K $\n",
    "- $ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $ Correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 5  \n",
    "Are the following statements true regarding Query (Q), Key (K) and Value (V)?  \n",
    "\n",
    "- Q = interesting questions about the words in a sentence  \n",
    "\n",
    "- K = specific representations of words given a Q  \n",
    "\n",
    "- V = qualities of words given a Q\n",
    "\n",
    "False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pregunta 6  \n",
    "<img src=\"pregunta_6.png\">\n",
    "\n",
    "\n",
    "i here represents the computed attention weight matrix associated with the ith “word” in a sentence.\n",
    "\n",
    "False\n",
    "\n",
    "Correct! $i$ here represents the computed attention weight matrix associated with the $$ith$$ “head” (sequence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 7  \n",
    "Following is the architecture within a Transformer Network (without displaying positional encoding and output layers(s)).\n",
    "\n",
    "<img src=\"pregunta_7.png\">\n",
    "\n",
    "- V\n",
    "- **Q**\n",
    "- K\n",
    "\n",
    "\n",
    "This first block’s output is used to generate the Q matrix for the next Multi-Head Attention block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 8  \n",
    "Following is the architecture within a Transformer Network (without displaying positional encoding and output layers(s)).\n",
    "\n",
    "<img src=\"pregunta_7.png\">\n",
    "\n",
    "The output of the decoder block contains a softmax layer followed by a linear layer to predict the next word one word at a time. \n",
    "\n",
    "False\n",
    "\n",
    "The output of the decoder block contains a linear layer followed by a softmax layer to predict the next word one word at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 9  \n",
    "Which of the following statements is true?\n",
    "\n",
    "- The transformer network is similar to the attention model in that neither contain positional encoding.\n",
    "- The transformer network differs from the attention model in that only the attention model contains positional encoding.\n",
    "- The transformer network is similar to the attention model in that both contain positional encoding.\n",
    "- **The transformer network differs from the attention model in that only the transformer network contains positional encoding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregunta 10  \n",
    "Which of these is a good criterion for a good positionial encoding algorithm?\n",
    "\n",
    "- It must be nondeterministc\n",
    "- **The algorithm should be abble to generalize to longer sentences**\n",
    "- Distance between any two time steps should be inconsistent for all sentence lengths\n",
    "- It should output a common encoding for each time step (words, position in a sentence)\n",
    "\n",
    "\n",
    "This is a good criterion for a good positional encoding algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a32cce1f9b3e4570ca846caf6903fa722a4c567520840cb717f6823875f1f99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
