{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adTDe2CTh3MU"
   },
   "source": [
    "# Traducción neuronal automática\n",
    "\n",
    "Bienvenido a tu primera tarea de programación de esta semana. \n",
    "\n",
    "* Construirás un modelo de Traducción Neuronal Automática (NMT) para traducir fechas legibles por humanos (\"25 de junio de 2009\") a fechas legibles por máquinas (\"2009-06-25\"). \n",
    "* Lo harás utilizando un modelo de atención, uno de los más sofisticados modelos de secuencia a secuencia. \n",
    "\n",
    "Este cuaderno ha sido realizado en colaboración con el Deep Learning Institute de NVIDIA. \n",
    "\n",
    "## Nota importante sobre el envío al AutoGrader\n",
    "\n",
    "Antes de enviar su tarea al AutoGrader, por favor asegúrese de que no está haciendo lo siguiente:\n",
    "\n",
    "1. No ha añadido ninguna declaración _extra_ `print` en la tarea.\n",
    "2. No ha añadido ninguna celda de código _extra_ en la tarea.\n",
    "3. No ha cambiado ningún parámetro de la función.\n",
    "4. No ha utilizado ninguna variable global dentro de sus ejercicios calificados. A menos que se le indique específicamente que lo haga, por favor absténgase de hacerlo y utilice las variables locales en su lugar.\n",
    "5. No está cambiando el código de asignación donde no es necesario, como la creación de variables _extra_.\n",
    "\n",
    "Si hace algo de lo siguiente, obtendrá un error como `Grader no encontrado` (o similarmente inesperado) al enviar su tarea. Antes de pedir ayuda/depurar los errores de su tarea, compruebe esto primero. Si este es el caso, y no recuerda los cambios que ha realizado, puede obtener una nueva copia de la tarea siguiendo estas [instrucciones](https://www.coursera.org/learn/nlp-sequence-models/supplement/qHIve/h-ow-to-refresh-your-workspace).\n",
    "\n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "# Neural Machine Translation\n",
    "\n",
    "Welcome to your first programming assignment for this week! \n",
    "\n",
    "* You will build a Neural Machine Translation (NMT) model to translate human-readable dates (\"25th of June, 2009\") into machine-readable dates (\"2009-06-25\"). \n",
    "* You will do this using an attention model, one of the most sophisticated sequence-to-sequence models. \n",
    "\n",
    "This notebook was produced together with NVIDIA's Deep Learning Institute. \n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/nlp-sequence-models/supplement/qHIve/h-ow-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LCkjDBFh3Md"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Packages](#0)\n",
    "- [1 - Translating Human Readable Dates Into Machine Readable Dates](#1)\n",
    "    - [1.1 - Dataset](#1-1)\n",
    "- [2 - Neural Machine Translation with Attention](#2)\n",
    "    - [2.1 - Attention Mechanism](#2-1)\n",
    "        - [Exercise 1 - one_step_attention](#ex-1)\n",
    "        - [Exercise 2 - modelf](#ex-2)\n",
    "        - [Exercise 3 - Compile the Model](#ex-3)\n",
    "- [3 - Visualizing Attention (Optional / Ungraded)](#3)\n",
    "    - [3.1 - Getting the Attention Weights From the Network](#3-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14508,
     "status": "ok",
     "timestamp": 1612468511651,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "RcBRMzPiiMmp",
    "outputId": "17e9a429-5bb6-4401-a23a-f8f756d6113d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0pkH-k0h3Mf"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Traducir fechas legibles por humanos a fechas legibles por máquinas\n",
    "\n",
    "* El modelo que construirás aquí podría usarse para traducir de un idioma a otro, como por ejemplo traducir del inglés al hindi. \n",
    "* Sin embargo, la traducción de idiomas requiere conjuntos de datos masivos y suele requerir días de entrenamiento en GPUs. \n",
    "* Para que puedas experimentar con estos modelos sin utilizar conjuntos de datos masivos, realizaremos una tarea más sencilla de \"traducción de fechas\". \n",
    "* La red introducirá una fecha escrita en una variedad de formatos posibles (*por ejemplo, \"el 29 de agosto de 1958\", \"30/03/1968\", \"24 JUNIO 1987\"*) \n",
    "* La red las traducirá a fechas estandarizadas y legibles por la máquina (*por ejemplo, \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). \n",
    "* Haremos que la red aprenda a dar salida a las fechas en el formato común legible por la máquina AAAA-MM-DD. \n",
    "\n",
    "<!-- \n",
    "Echa un vistazo a [nmt_utils.py](./nmt_utils.py) para ver todos los formatos. Cuenta y averigua cómo funcionan los formatos, necesitarás este conocimiento más adelante. !--> \n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "<a name='1'></a>\n",
    "## 1 - Translating Human Readable Dates Into Machine Readable Dates\n",
    "\n",
    "* The model you will build here could be used to translate from one language to another, such as translating from English to Hindi. \n",
    "* However, language translation requires massive datasets and usually takes days of training on GPUs. \n",
    "* To give you a place to experiment with these models without using massive datasets, we will perform a simpler \"date translation\" task. \n",
    "* The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) \n",
    "* The network will translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). \n",
    "* We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. \n",
    "\n",
    "<!-- \n",
    "Take a look at [nmt_utils.py](./nmt_utils.py) to see all the formatting. Count and figure out how the formats work, you will need this knowledge later. !--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BhEaJvph3Mf"
   },
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Conjunto de datos\n",
    "\n",
    "Vamos a entrenar el modelo con un conjunto de datos de 10.000 fechas legibles por humanos y sus fechas equivalentes, estandarizadas y legibles por máquinas. Vamos a ejecutar las siguientes celdas para cargar el conjunto de datos e imprimir algunos ejemplos. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Dataset\n",
    "\n",
    "We will train the model on a dataset of 10,000 human readable dates and their equivalent, standardized, machine readable dates. Let's run the following cells to load the dataset and print some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16981,
     "status": "ok",
     "timestamp": 1612468514155,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "gwIf5l17h3Mg",
    "outputId": "1fca5fb8-3a9b-4a78-f726-7aef8e14ee41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 24423.61it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16972,
     "status": "ok",
     "timestamp": 1612468514156,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "zCTqMyPch3Mg",
    "outputId": "42c9d8aa-d07b-4618-ab8a-4db4e1b971e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10'),\n",
       " ('saturday april 28 1990', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('08 jul 2008', '2008-07-08'),\n",
       " ('8 sep 1999', '1999-09-08'),\n",
       " ('thursday january 1 1981', '1981-01-01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ao4Ffrkxh3Mg"
   },
   "source": [
    "Has cargado:\n",
    "- `dataset`: una lista de tuplas de (fecha legible por humanos, fecha legible por máquinas).\n",
    "- `human_vocab`: un diccionario python que asigna todos los caracteres utilizados en las fechas legibles por humanos a un índice de valor entero.\n",
    "- `machine_vocab`: un diccionario python que asigna todos los caracteres utilizados en las fechas legibles por máquina a un índice de valor entero. \n",
    "    - Nota: Estos índices no son necesariamente consistentes con `human_vocab`. \n",
    "- `inv_machine_vocab`: el diccionario inverso de `machine_vocab`, mapeando de nuevo los índices a caracteres. \n",
    "\n",
    "Vamos a preprocesar los datos y mapear los datos de texto en bruto en los valores de los índices. \n",
    "- Estableceremos Tx=30 \n",
    "    - Suponemos que Tx es la longitud máxima de la fecha legible para los humanos.\n",
    "    - Si obtenemos una entrada más larga, tendríamos que truncarla.\n",
    "- Estableceremos Ty=10\n",
    "    - \"AAAA-MM-DD\" tiene 10 caracteres.\n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "You've loaded:\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date).\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. \n",
    "    - **Note**: These indices are not necessarily consistent with `human_vocab`. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
    "\n",
    "Let's preprocess the data and map the raw text data into the index values. \n",
    "- We will set Tx=30 \n",
    "    - We assume Tx is the maximum length of the human readable date.\n",
    "    - If we get a longer input, we would have to truncate it.\n",
    "- We will set Ty=10\n",
    "    - \"YYYY-MM-DD\" is 10 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16962,
     "status": "ok",
     "timestamp": 1612468514157,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "Qdso90EBh3Mg",
    "outputId": "0a364ad8-8b25-4de3-f036-d5d8e40bdf8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9C0UY25h3Mh"
   },
   "source": [
    "Ahora tiene:\n",
    "- `X`: una versión procesada de las fechas legibles por humanos en el conjunto de entrenamiento.\n",
    "    - Cada carácter en X es reemplazado por un índice (entero) asignado al carácter usando `human_vocab`. \n",
    "    - Cada fecha se rellena para asegurar una longitud de $T_x$ utilizando un carácter especial (< pad >). \n",
    "    - `X.shape = (m, Tx)` donde m es el número de ejemplos de entrenamiento en un lote.\n",
    "- `Y`: una versión procesada de las fechas legibles por la máquina en el conjunto de entrenamiento.\n",
    "    - Cada carácter se sustituye por el índice (entero) al que se asigna en `machine_vocab`. \n",
    "    - Y.shape = (m, Ty)`. \n",
    "- Xoh: versión one-hot de X\n",
    "    - Cada índice de `X` se convierte a la representación one-hot (si el índice es 2, la versión de un solo paso tiene la posición del índice 2 a 1, y el resto de posiciones a 0).\n",
    "    - Xoh.shape = (m, Tx, len(human_vocab))\n",
    "- Yoh: versión one-hot de Y\n",
    "    - Cada índice de `Y` se convierte a la representación one-hot. \n",
    "    - Yoh.shape = (m, Ty, len(machine_vocab))`. \n",
    "    - El valor de `len(machine_vocab) = 11` es porque hay 10 dígitos numéricos (0 a 9) y el símbolo `-`.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "You now have:\n",
    "- `X`: a processed version of the human readable dates in the training set.\n",
    "    - Each character in X is replaced by an index (integer) mapped to the character using `human_vocab`. \n",
    "    - Each date is padded to ensure a length of $T_x$ using a special character (< pad >). \n",
    "    - `X.shape = (m, Tx)` where m is the number of training examples in a batch.\n",
    "- `Y`: a processed version of the machine readable dates in the training set.\n",
    "    - Each character is replaced by the index (integer) it is mapped to in `machine_vocab`. \n",
    "    - `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`\n",
    "    - Each index in `X` is converted to the one-hot representation (if the index is 2, the one-hot version has the index position 2 set to 1, and the remaining positions are 0.\n",
    "    - `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`\n",
    "    - Each index in `Y` is converted to the one-hot representation. \n",
    "    - `Yoh.shape = (m, Ty, len(machine_vocab))`. \n",
    "    - `len(machine_vocab) = 11` since there are 10 numeric digits (0 to 9) and the `-` symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7qKvWrTh3Mh"
   },
   "source": [
    "* Veamos también algunos ejemplos de entrenamiento preprocesados. \n",
    "* Siéntase libre de jugar con `index` en la celda de abajo para navegar por el conjunto de datos y ver cómo se preprocesan las fechas de origen/destino. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "* Let's also look at some examples of preprocessed training examples. \n",
    "* Feel free to play with `index` in the cell below to navigate the dataset and see how source/target dates are preprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16952,
     "status": "ok",
     "timestamp": 1612468514158,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "kUOayR4gh3Mh",
    "outputId": "d20994de-bbea-4cc7-ffaf-38a05974c9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 9 may 1998\n",
      "Target date: 1998-05-09\n",
      "\n",
      "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94o4RYbOh3Mi"
   },
   "source": [
    "<a name='2'></a>\n",
    "\n",
    "## 2 - Traducción automática neuronal con atención\n",
    "\n",
    "* Si tuviera que traducir el párrafo de un libro del francés al inglés, no leería todo el párrafo, luego cerraría el libro y traduciría.\n",
    "* Incluso durante el proceso de traducción, leería/releería y se centraría en las partes del párrafo en francés correspondientes a las partes del inglés que está escribiendo.\n",
    "* El mecanismo de atención le dice a un modelo de traducción automática neuronal dónde debe prestar atención en cualquier paso.\n",
    "\n",
    "<un name='2-1'></a>\n",
    "### 2.1 - Mecanismo de Atención\n",
    "\n",
    "En esta parte, implementará el mecanismo de atención presentado en los videos de conferencias.\n",
    "* Aquí hay una figura para recordar cómo funciona el modelo.\n",
    "    * El diagrama de la izquierda muestra el modelo de atención.\n",
    "    * El diagrama de la derecha muestra lo que hace un paso de \"atención\" para calcular las variables de atención $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "    * Las variables de atención $\\alpha^{\\langle t, t' \\rangle}$ se utilizan para calcular la variable de contexto $context^{\\langle t \\rangle}$ para cada paso de tiempo en la salida ($t=1, \\ ldots, T_y$).\n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> Figura 1: Traducción automática neuronal con atención </center></caption>\n",
    "\n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "<a name='2'></a>\n",
    "## 2 - Neural Machine Translation with Attention\n",
    "\n",
    "* If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. \n",
    "* Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. \n",
    "* The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Attention Mechanism\n",
    "\n",
    "In this part, you will implement the attention mechanism presented in the lecture videos. \n",
    "* Here is a figure to remind you how the model works. \n",
    "    * The diagram on the left shows the attention model. \n",
    "    * The diagram on the right shows what one \"attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "    * The attention variables $\\alpha^{\\langle t, t' \\rangle}$ are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2TkQnykh3Mi"
   },
   "source": [
    "Estas son algunas de las propiedades del modelo que puedes observar: \n",
    "\n",
    "#### LSTMs de pre-atención y post-atención en ambos lados del mecanismo de atención\n",
    "- Hay dos LSTMs separadas en este modelo (ver el diagrama de la izquierda): LSTMs de preatención y postatención.\n",
    "- La Bi-LSTM de preatención es la que está en la parte inferior de la imagen, es una LSTM bidireccional y viene *antes* del mecanismo de atención.\n",
    "    - El mecanismo de atención se muestra en el centro del diagrama de la izquierda.\n",
    "    - La Bi-LSTM de preatención pasa por $T_x$ pasos de tiempo\n",
    "- LSTM de post-atención: en la parte superior del diagrama viene *después* del mecanismo de atención. \n",
    "    - El LSTM de post-atención pasa por $T_y$ pasos de tiempo. \n",
    "\n",
    "- El LSTM de post-atención pasa el estado oculto $s^{\\langle t \\rangle}$ y el estado de la célula $c^{\\langle t \\rangle}$ de un paso de tiempo al siguiente. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "Here are some properties of the model that you may notice: \n",
    "\n",
    "#### Pre-attention and Post-attention LSTMs on both sides of the attention mechanism\n",
    "- There are two separate LSTMs in this model (see diagram on the left): pre-attention and post-attention LSTMs.\n",
    "- *Pre-attention* Bi-LSTM is the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism.\n",
    "    - The attention mechanism is shown in the middle of the left-hand diagram.\n",
    "    - The pre-attention Bi-LSTM goes through $T_x$ time steps\n",
    "- *Post-attention* LSTM: at the top of the diagram comes *after* the attention mechanism. \n",
    "    - The post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$ from one time step to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpznWuWqh3Mi"
   },
   "source": [
    "#### Un LSTM tiene tanto un estado oculto como un estado de celda\n",
    "* En los videos de la conferencia, estábamos usando sólo una RNN básica para el modelo de secuencia de post-atención\n",
    "    * Esto significa que el estado capturado por la RNN estaba dando salida sólo al estado oculto $s^{\\langle t\\rangle}$. \n",
    "* En esta tarea, estamos utilizando un LSTM en lugar de un RNN básico.\n",
    "    * Así que el LSTM tiene tanto el estado oculto $s^{\\langle t\\rangle}$ como el estado de la célula $c^{\\langle t\\rangle}$. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "#### An LSTM has both a hidden state and cell state\n",
    "* In the lecture videos, we were using only a basic RNN for the post-attention sequence model\n",
    "    * This means that the state captured by the RNN was outputting only the hidden state $s^{\\langle t\\rangle}$. \n",
    "* In this assignment, we are using an LSTM instead of a basic RNN.\n",
    "    * So the LSTM has both the hidden state $s^{\\langle t\\rangle}$ and the cell state $c^{\\langle t\\rangle}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85btUzl4h3Mj"
   },
   "source": [
    "#### Cada paso de tiempo no utiliza las predicciones del paso de tiempo anterior\n",
    "* A diferencia de los ejemplos anteriores de generación de texto, en este modelo, el LSTM de post-atención en el tiempo $t$ no toma la predicción del paso de tiempo anterior $y^{\\langle t-1 \\rangle}$ como entrada.\n",
    "* El LSTM de post-atención en el tiempo 't' sólo toma el estado oculto $s^{\\langle t\\rangle}$ y el estado de la célula $c^{\\langle t\\rangle}$ como entrada. \n",
    "* Hemos diseñado el modelo de esta manera porque a diferencia de la generación del lenguaje (donde los caracteres adyacentes están altamente correlacionados) no hay una dependencia tan fuerte entre el carácter anterior y el siguiente en una fecha AAAA-MM-DD.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "#### Each time step does not use predictions from the previous time step\n",
    "* Unlike previous text generation examples earlier in the course, in this model, the post-attention LSTM at time $t$ does not take the previous time step's prediction $y^{\\langle t-1 \\rangle}$ as input.\n",
    "* The post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t\\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input. \n",
    "* We have designed the model this way because unlike language generation (where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYT3v7rUh3Mk"
   },
   "source": [
    "#### Concatenación de estados ocultos de las LSTMs de preatención hacia adelante y hacia atrás\n",
    "- $\\overrightarrow{a}^{\\langle t \\rangle}$: estado oculto de la LSTM de preatención hacia delante.\n",
    "- $\\overleftarrow{a}^{\\langle t \\rangle}$: estado oculto de la dirección hacia atrás, preatención LSTM.\n",
    "- $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}, \\overleftarrow{a}^{\\langle t \\rangle}]$: la concatenación de las activaciones de las direcciones hacia delante $\\overrightarrow{a}^{\\langle t \\rangle}$ y hacia atrás $\\overleftarrow{a}^{\\langle t \\rangle}$ de la Bi-LSTM de preatención. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "#### Concatenation of hidden states from the forward and backward pre-attention LSTMs\n",
    "- $\\overrightarrow{a}^{\\langle t \\rangle}$: hidden state of the forward-direction, pre-attention LSTM.\n",
    "- $\\overleftarrow{a}^{\\langle t \\rangle}$: hidden state of the backward-direction, pre-attention LSTM.\n",
    "- $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}, \\overleftarrow{a}^{\\langle t \\rangle}]$: the concatenation of the activations of both the forward-direction $\\overrightarrow{a}^{\\langle t \\rangle}$ and backward-directions $\\overleftarrow{a}^{\\langle t \\rangle}$ of the pre-attention Bi-LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97GUKCqwh3Mk"
   },
   "source": [
    "#### Calculando las \"energías\" $e^{\\langle t, t' \\rangle}$ como función de $s^{\\langle t-1 \\rangle}$ y $a^{\\langle t' \\rangle}$\n",
    "- Recordemos en los vídeos de la lección \"Modelo de atención\", en el tiempo 6:45 a 8:16, la definición de \"e\" como función de $s^{\\langle t-1 \\rangle}$ y $a^{\\langle t \\rangle}$.\n",
    "    - \"e\" se llama la variable \"energías\".\n",
    "    - $s^{\\langle t-1 \\rangle}$ es el estado oculto de la LSTM post-atención\n",
    "    - $a^{\\langle t' \\rangle}$ es el estado oculto de la LSTM de preatención.\n",
    "    - $s^{\\langle t-1 \\rangle}$ y $a^{\\langle t \\rangle}$ se introducen en una red neuronal simple, que aprende la función de salida $e^{\\langle t, t' \\rangle}$.\n",
    "    - $e^{\\langle t, t' \\rangle}$ se utiliza entonces cuando se calcula la atención $\\alpha^{\\langle t, t' \\rangle}$ que $y^{\\langle t \\rangle}$ debe prestar a $a^{\\langle t' \\rangle}$.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "#### Computing \"energies\" $e^{\\langle t, t' \\rangle}$ as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t' \\rangle}$\n",
    "- Recall in the lesson videos \"Attention Model\", at time 6:45 to 8:16, the definition of \"e\" as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n",
    "    - \"e\" is called the \"energies\" variable.\n",
    "    - $s^{\\langle t-1 \\rangle}$ is the hidden state of the post-attention LSTM\n",
    "    - $a^{\\langle t' \\rangle}$ is the hidden state of the pre-attention LSTM.\n",
    "    - $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ are fed into a simple neural network, which learns the function to output $e^{\\langle t, t' \\rangle}$.\n",
    "    - $e^{\\langle t, t' \\rangle}$ is then used when computing the attention $\\alpha^{\\langle t, t' \\rangle}$ that $y^{\\langle t \\rangle}$ should pay to $a^{\\langle t' \\rangle}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scu_HnPNh3Mk"
   },
   "source": [
    "- El diagrama de la derecha de la figura 1 utiliza un nodo `RepeatVector` para copiar $s^{\\langle t-1 \\rangle}$ de valor $T_x$ veces.\n",
    "- A continuación, utiliza `Concatenación` para concatenar $s^{\\langle t-1 \\rangle}$ y $a^{\\langle t \\rangle}$.\n",
    "- La concatenación de $s^{\\langle t-1 \\rangle}$ y $a^{\\langle t \\rangle}$ se introduce en una capa \"Densa\", que calcula $e^{\\langle t, t' \\rangle}$. \n",
    "- $e^{\\langle t, t' \\rangle}$ se pasa a través de un softmax para calcular $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "- Obsérvese que el diagrama no muestra explícitamente la variable $e^{\\langle t, t' \\rangle}$, pero $e^{\\langle t, t' \\rangle}$ está por encima de la capa Densa y por debajo de la capa Softmax en el diagrama de la mitad derecha de la figura 1.\n",
    "- Más adelante explicaremos cómo utilizar `RepeatVector` y `Concatenation` en Keras. \n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "- The diagram on the right of figure 1 uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times.\n",
    "- Then it uses `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n",
    "- The concatenation of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ is fed into a \"Dense\" layer, which computes $e^{\\langle t, t' \\rangle}$. \n",
    "- $e^{\\langle t, t' \\rangle}$ is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "- Note that the diagram doesn't explicitly show variable $e^{\\langle t, t' \\rangle}$, but $e^{\\langle t, t' \\rangle}$ is above the Dense layer and below the Softmax layer in the diagram in the right half of figure 1.\n",
    "- We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ukmqe_Yh3Ml"
   },
   "source": [
    "#### Detalles de la implementación\n",
    "   \n",
    "Vamos a implementar este traductor neuronal. Empezarás implementando dos funciones: `one_step_attention()` y `model()`.\n",
    "\n",
    "#### one_step_attention\n",
    "* Las entradas a la one_step_attention en el paso de tiempo $t$ son:\n",
    "    - $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$: todos los estados ocultos de la Bi-LSTM de preatención.\n",
    "    - $s^{<t-1>}$: el estado oculto anterior de la LSTM de post-atención \n",
    "* one_step_attention computa:\n",
    "    - $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$: los pesos de atención\n",
    "    - $context^{ \\langle t \\rangle }$: el vector de contexto:\n",
    "    \n",
    "$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "##### Aclarando el \"contexto\" y la \"c\n",
    "- En los vídeos de la conferencia, el contexto se denominaba $c^{\\langle t \\rangle}$.\n",
    "- En la tarea, estamos llamando el contexto $context^{\\langle t \\rangle}$.\n",
    "    - Esto es para evitar la confusión con la variable de la célula de memoria interna de la LSTM de post-atención, que también se denota $c^{\\langle t \\rangle}$.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "#### Implementation Details\n",
    "   \n",
    "Let's implement this neural translator. You will start by implementing two functions: `one_step_attention()` and `model()`.\n",
    "\n",
    "#### one_step_attention\n",
    "* The inputs to the one_step_attention at time step $t$ are:\n",
    "    - $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$: all hidden states of the pre-attention Bi-LSTM.\n",
    "    - $s^{<t-1>}$: the previous hidden state of the post-attention LSTM \n",
    "* one_step_attention computes:\n",
    "    - $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$: the attention weights\n",
    "    - $context^{ \\langle t \\rangle }$: the context vector:\n",
    "    \n",
    "$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "##### Clarifying 'context' and 'c'\n",
    "- In the lecture videos, the context was denoted $c^{\\langle t \\rangle}$\n",
    "- In the assignment, we are calling the context $context^{\\langle t \\rangle}$.\n",
    "    - This is to avoid confusion with the post-attention LSTM's internal memory cell variable, which is also denoted $c^{\\langle t \\rangle}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIfLKkwoh3Ml"
   },
   "source": [
    "<a name='ex-1'></a>\n",
    "### Ejercicio 1 - one_step_attention \n",
    "\n",
    "Implementa `one_step_attention()`. \n",
    "\n",
    "* La función `model()` llamará a las capas en `one_step_attention()` $T_y$ veces utilizando un bucle for.\n",
    "* Es importante que todas las copias de $T_y$ tengan los mismos pesos. \n",
    "    * No debe reiniciar los pesos cada vez. \n",
    "    * En otras palabras, todos los pasos de $T_y$ deben tener pesos compartidos. \n",
    "* A continuación se explica cómo se pueden implementar capas con pesos compartidos en Keras:\n",
    "    1. Definir los objetos de la capa en un ámbito variable que está fuera de la función `one_step_attention`.  Por ejemplo, definir los objetos como variables globales funcionaría.\n",
    "        - Ten en cuenta que definir estas variables dentro del ámbito de la función `model` técnicamente funcionaría, ya que `model` llamará entonces a la función `one_step_attention`.  Con el fin de facilitar la calificación y la resolución de problemas, estamos definiendo estas variables como globales.  Tenga en cuenta que el calificador automático esperará que sean variables globales también.\n",
    "    2. Llame a estos objetos al propagar la entrada.\n",
    "* Hemos definido las capas que necesita como variables globales. \n",
    "    * Por favor, ejecute las siguientes celdas para crearlas. \n",
    "    * Tenga en cuenta que el calificador automático espera estas variables globales con los nombres de las variables dadas.  Por favor, no cambie el nombre de las variables globales.\n",
    "* Por favor, consulte la documentación de Keras para saber más sobre estas capas.  Las capas son funciones.  A continuación hay ejemplos de cómo llamar a estas funciones.\n",
    "    * [RepeatVector()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector)\n",
    "```Python\n",
    "var_repeated = repeat_layer(var1)\n",
    "```\n",
    "    * [Concatenate()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate)   \n",
    "```Python\n",
    "concatenated_vars = concatenate_layer([var1,var2,var3])\n",
    "```\n",
    "    * [Dense()](https://keras.io/layers/core/#dense)  \n",
    "```Python\n",
    "var_out = dense_layer(var_in)\n",
    "```\n",
    "    * [Activation()](https://keras.io/layers/core/#activation)  \n",
    "```Python\n",
    "activation = activation_layer(var_in)  \n",
    "```\n",
    "    * [Dot()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot)  \n",
    "```Python\n",
    "dot_product = dot_layer([var1,var2])\n",
    "```\n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - one_step_attention \n",
    "\n",
    "Implement `one_step_attention()`. \n",
    "\n",
    "* The function `model()` will call the layers in `one_step_attention()` $T_y$ times using a for-loop.\n",
    "* It is important that all $T_y$ copies have the same weights. \n",
    "    * It should not reinitialize the weights every time. \n",
    "    * In other words, all $T_y$ steps should have shared weights. \n",
    "* Here's how you can implement layers with shareable weights in Keras:\n",
    "    1. Define the layer objects in a variable scope that is outside of the `one_step_attention` function.  For example, defining the objects as global variables would work.\n",
    "        - Note that defining these variables inside the scope of the function `model` would technically work, since `model` will then call the `one_step_attention` function.  For the purposes of making grading and troubleshooting easier, we are defining these as global variables.  Note that the automatic grader will expect these to be global variables as well.\n",
    "    2. Call these objects when propagating the input.\n",
    "* We have defined the layers you need as global variables. \n",
    "    * Please run the following cells to create them. \n",
    "    * Please note that the automatic grader expects these global variables with the given variable names.  For grading purposes, please do not rename the global variables.\n",
    "* Please check the Keras documentation to learn more about these layers.  The layers are functions.  Below are examples of how to call these functions.\n",
    "    * [RepeatVector()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector)\n",
    "```Python\n",
    "var_repeated = repeat_layer(var1)\n",
    "```\n",
    "    * [Concatenate()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate)   \n",
    "```Python\n",
    "concatenated_vars = concatenate_layer([var1,var2,var3])\n",
    "```\n",
    "    * [Dense()](https://keras.io/layers/core/#dense)  \n",
    "```Python\n",
    "var_out = dense_layer(var_in)\n",
    "```\n",
    "    * [Activation()](https://keras.io/layers/core/#activation)  \n",
    "```Python\n",
    "activation = activation_layer(var_in)  \n",
    "```\n",
    "    * [Dot()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot)  \n",
    "```Python\n",
    "dot_product = dot_layer([var1,var2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 16950,
     "status": "ok",
     "timestamp": 1612468514158,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "Cvop5Apyh3Mm"
   },
   "outputs": [],
   "source": [
    "# Define las capas compartidas como variables globales\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # Estamos usando un softmax(eje=1) personalizado cargado en este cuaderno\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 16950,
     "status": "ok",
     "timestamp": 1612468514159,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "mZuMOnTDh3Mn"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: one_step_attention\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attention) LSTM cell\n",
    "    \n",
    "    ## EN ESPAÑOL ##\n",
    "    Realiza un paso de atención: Se obtiene un vector de contexto calculado como un producto punto de los pesos de atención\n",
    "    \"alphas\" y los estados ocultos \"a\" de la Bi-LSTM.\n",
    "    \n",
    "    Argumentos:\n",
    "    a -- salida del estado oculto del Bi-LSTM, matriz numpy de forma (m, Tx, 2*n_a)\n",
    "    s_prev -- estado oculto anterior del LSTM (post-atención), numpy-array de forma (m, n_s)\n",
    "    \n",
    "    Devuelve:\n",
    "    context -- vector de contexto, entrada de la siguiente célula LSTM (post-atención)\n",
    "    \"\"\"\n",
    "#     # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "#     s_prev = None\n",
    "#     # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "#     # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
    "#     concat = None\n",
    "#     # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "#     e = None\n",
    "#     # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "#     energies = None\n",
    "#     # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "#     alphas = None\n",
    "#     # Use dotor together with \"alphas\" and \"a\", in this order, to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "#     context = None\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Utiliza el repetidor para repetir s_prev para que tenga la forma (m, Tx, n_s) y \n",
    "    # así poder concatenarla con todos los estados ocultos \"a\" (≈ 1 línea)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Usa el concatenador para concatenar a y s_prev en el último eje (≈ 1 línea)\n",
    "    # A efectos de calificación, indique primero 'a' y después 's_prev', en este orden.\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Utilice el densor1 para propagar concat a través de una pequeña red neuronal \n",
    "    # totalmente conectada para calcular la variable \"energías intermedias\" e. (≈1 líneas)\n",
    "    e = densor1(concat)\n",
    "    # Usar densor2 para propagar e a través de una pequeña red neuronal totalmente conectada \n",
    "    # para calcular \"energías\" la variable \"energías\" energías. (≈1 líneas)\n",
    "    energies = densor2(e)\n",
    "    # Usar \"activador\" en \"energías\" para calcular los pesos de atención \"alfas\" (≈ 1 línea)\n",
    "    alphas = activator(energies)\n",
    "    # Usar dotor junto con \"alphas\" y \"a\", en este orden, para calcular el vector de contexto \n",
    "    # que se dará a la siguiente célula LSTM (post-atención) (≈ 1 línea)\n",
    "    context = dotor([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def one_step_attention_test(target):\n",
    "\n",
    "    m = 10\n",
    "    Tx = 30\n",
    "    n_a = 32\n",
    "    n_s = 64\n",
    "    #np.random.seed(10)\n",
    "    a = np.random.uniform(1, 0, (m, Tx, 2 * n_a)).astype(np.float32)\n",
    "    s_prev =np.random.uniform(1, 0, (m, n_s)).astype(np.float32) * 1\n",
    "    context = target(a, s_prev)\n",
    "    \n",
    "    assert type(context) == tf.python.framework.ops.EagerTensor, \"Unexpected type. It should be a Tensor\"\n",
    "    assert tuple(context.shape) == (m, 1, n_s), \"Unexpected output shape\"\n",
    "    assert np.all(context.numpy() > 0), \"All output values must be > 0 in this example\"\n",
    "    assert np.all(context.numpy() < 1), \"All output values must be < 1 in this example\"\n",
    "\n",
    "    #assert np.allclose(context[0][0][0:5].numpy(), [0.50877404, 0.57160693, 0.45448175, 0.50074816, 0.53651875]), \"Unexpected values in the result\"\n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "one_step_attention_test(one_step_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcmC3WcQh3Mn"
   },
   "source": [
    "<a name='ex-2'></a>\n",
    "### Ejercicio 2 - modelf\n",
    "\n",
    "Implementa `modelf()` como se explica en la figura 1 y las instrucciones:\n",
    "\n",
    "* `modelf` primero ejecuta la entrada a través de un Bi-LSTM para obtener $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. \n",
    "* Luego, `modelf` llama a `one_step_attention()` $T_y$ veces usando un bucle `for`.  En cada iteración de este bucle:\n",
    "    - Da el vector de contexto calculado $contexto^{<t>}$ a la LSTM de post-atención.\n",
    "    - Pasa la salida de la LSTM de post-atención por una capa densa con activación softmax.\n",
    "    - El softmax genera una predicción $\\hat{y}^{<t>}$.\n",
    "    \n",
    "De nuevo, hemos definido capas globales que compartirán pesos para ser utilizados en `modelf()`.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - modelf\n",
    "\n",
    "Implement `modelf()` as explained in figure 1 and the instructions:\n",
    "\n",
    "* `modelf` first runs the input through a Bi-LSTM to get $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. \n",
    "* Then, `modelf` calls `one_step_attention()` $T_y$ times using a `for` loop.  At each iteration of this loop:\n",
    "    - It gives the computed context vector $context^{<t>}$ to the post-attention LSTM.\n",
    "    - It runs the output of the post-attention LSTM through a dense layer with softmax activation.\n",
    "    - The softmax generates a prediction $\\hat{y}^{<t>}$.\n",
    "    \n",
    "Again, we have defined global layers that will share weights to be used in `modelf()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 16949,
     "status": "ok",
     "timestamp": 1612468514159,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "5RHgmZrVh3Mo"
   },
   "outputs": [],
   "source": [
    "n_a = 32 # número de unidades para el estado oculto \"a\" de la LSTM bidireccional de preatención\n",
    "n_s = 64 # número de unidades para el estado oculto \"s\" de la LSTM de post-atención\n",
    "\n",
    "# Tenga en cuenta que esta es la célula LSTM de post atención.  \n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # Por favor, no modifique esta variable global.\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGkKpb1Nh3Mo"
   },
   "source": [
    "Ahora puedes utilizar estas capas $T_y$ veces en un bucle `for` para generar las salidas, y sus parámetros no se reiniciarán. Usted tendrá que llevar a cabo los siguientes pasos: \n",
    "\n",
    "1. Propagar la entrada `X` en una LSTM bidireccional.\n",
    "    * [Bidireccional](https://keras.io/layers/wrappers/#bidirectional) \n",
    "    * [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
    "    * Recuerda que queremos que el LSTM devuelva una secuencia completa en lugar de sólo el último estado oculto.  \n",
    "    \n",
    "    Código de ejemplo:\n",
    "\n",
    "    ```Python\n",
    "    model = Model(inputs=[...,...,...], outputs=...)\n",
    "    ```\n",
    "    \n",
    "2. Iterar para $t = 0, \\cdots, T_y-1$: \n",
    "    1. Llamar a `one_step_attention()`, pasando la secuencia de estados ocultos $[a^{\\langle 1 \\rangle},a^{\\langle 2 \\rangle}, ..., a^{ \\langle T_x \\rangle}]$ de la LSTM bidireccional de pre-atención, y el estado oculto anterior $s^{<t-1>}$ de la LSTM de post-atención para calcular el vector de contexto $contexto^{<t>}$.\n",
    "    2. Dar $contexto^{<t>}$ a la célula LSTM post-atención. \n",
    "        - Recuerde que debe pasar en el anterior estado oculto $s^{\\langle t-1\\rangle}$ y los estados de la célula $c^{\\langle t-1\\rangle}$ de este LSTM \n",
    "        * Esto produce el nuevo estado oculto $s^{<t>}$ y el nuevo estado de la célula $c^{<t>}$.  \n",
    "\n",
    "        Código de ejemplo:\n",
    "        ```Python\n",
    "        next_hidden_state, _ , next_cell_state = \n",
    "            post_activación_LSTM_célula(inputs=..., initial_state=[prev_hidden_state, prev_cell_state])\n",
    "        ```   \n",
    "        Ten en cuenta que la capa es en realidad la \"célula LSTM post atención\".  A efectos de pasar el calificador automático, por favor, no modifique el nombre de esta variable global.  Esto se arreglará cuando despleguemos las actualizaciones del calificador automático.\n",
    "    3. Aplicar una capa densa, softmax a $s^{<t>}$, obtener la salida.  \n",
    "        Código de ejemplo:\n",
    "        ```Python\n",
    "        output = output_layer(inputs=...)\n",
    "        ```\n",
    "    4. Guarda la salida añadiéndola a la lista de salidas.\n",
    "\n",
    "3. Crea tu instancia de modelo Keras.\n",
    "    * Debe tener tres entradas:\n",
    "        * `X`, las entradas codificadas de un solo golpe para el modelo, de forma ($T_{x}, humanVocabSize)$\n",
    "        * $s^{\\langle 0 \\rangle}$, el estado oculto inicial de la LSTM de post-atención\n",
    "        * $c^{\\langle 0 \\rangle}$, el estado inicial de la célula de la LSTM de post-atención\n",
    "    * La salida es la lista de salidas.  \n",
    "    Código de ejemplo\n",
    "    ```Python\n",
    "    model = Model(inputs=[...,...,...], outputs=...)\n",
    "    ```\n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "Now you can use these layers $T_y$ times in a `for` loop to generate the outputs, and their parameters will not be reinitialized. You will have to carry out the following steps: \n",
    "\n",
    "1. Propagate the input `X` into a bi-directional LSTM.\n",
    "    * [Bidirectional](https://keras.io/layers/wrappers/#bidirectional) \n",
    "    * [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
    "    * Remember that we want the LSTM to return a full sequence instead of just the last hidden state.  \n",
    "    \n",
    "Sample code:\n",
    "\n",
    "```Python\n",
    "sequence_of_hidden_states = Bidirectional(LSTM(units=..., return_sequences=...))(the_input_X)\n",
    "```\n",
    "    \n",
    "2. Iterate for $t = 0, \\cdots, T_y-1$: \n",
    "    1. Call `one_step_attention()`, passing in the sequence of hidden states $[a^{\\langle 1 \\rangle},a^{\\langle 2 \\rangle}, ..., a^{ \\langle T_x \\rangle}]$ from the pre-attention bi-directional LSTM, and the previous hidden state $s^{<t-1>}$ from the post-attention LSTM to calculate the context vector $context^{<t>}$.\n",
    "    2. Give $context^{<t>}$ to the post-attention LSTM cell. \n",
    "        - Remember to pass in the previous hidden-state $s^{\\langle t-1\\rangle}$ and cell-states $c^{\\langle t-1\\rangle}$ of this LSTM \n",
    "        * This outputs the new hidden state $s^{<t>}$ and the new cell state $c^{<t>}$.  \n",
    "\n",
    "        Sample code:\n",
    "        ```Python\n",
    "        next_hidden_state, _ , next_cell_state = \n",
    "            post_activation_LSTM_cell(inputs=..., initial_state=[prev_hidden_state, prev_cell_state])\n",
    "        ```   \n",
    "        Please note that the layer is actually the \"post attention LSTM cell\".  For the purposes of passing the automatic grader, please do not modify the naming of this global variable.  This will be fixed when we deploy updates to the automatic grader.\n",
    "    3. Apply a dense, softmax layer to $s^{<t>}$, get the output.  \n",
    "        Sample code:\n",
    "        ```Python\n",
    "        output = output_layer(inputs=...)\n",
    "        ```\n",
    "    4. Save the output by adding it to the list of outputs.\n",
    "\n",
    "3. Create your Keras model instance.\n",
    "    * It should have three inputs:\n",
    "        * `X`, the one-hot encoded inputs to the model, of shape ($T_{x}, humanVocabSize)$\n",
    "        * $s^{\\langle 0 \\rangle}$, the initial hidden state of the post-attention LSTM\n",
    "        * $c^{\\langle 0 \\rangle}$, the initial cell state of the post-attention LSTM\n",
    "    * The output is the list of outputs.  \n",
    "    Sample code\n",
    "    ```Python\n",
    "    model = Model(inputs=[...,...,...], outputs=...)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 16948,
     "status": "ok",
     "timestamp": 1612468514160,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "qeKbeDOvh3Mo"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: model\n",
    "\n",
    "def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \n",
    "    ## EN ESPAÑOL ##\n",
    "    Argumentos:\n",
    "    Tx -- longitud de la secuencia de entrada\n",
    "    Ty -- longitud de la secuencia de salida\n",
    "    n_a -- tamaño del estado oculto del Bi-LSTM\n",
    "    n_s -- tamaño del estado oculto del LSTM de post-atención\n",
    "    human_vocab_size -- tamaño del diccionario python \"human_vocab\"\n",
    "    machine_vocab_size -- tamaño del diccionario python \"machine_vocab\"\n",
    "\n",
    "    Devuelve:\n",
    "    model -- instancia del modelo Keras\n",
    "    \"\"\"\n",
    "#     # Define the inputs of your model with a shape (Tx,)\n",
    "#     # Define s0 (initial hidden state) and c0 (initial cell state)\n",
    "#     # for the decoder LSTM with shape (n_s,)\n",
    "#     X = Input(shape=(Tx, human_vocab_size))\n",
    "#     s0 = Input(shape=(n_s,), name='s0')\n",
    "#     c0 = Input(shape=(n_s,), name='c0')\n",
    "#     s = s0\n",
    "#     c = c0\n",
    "    \n",
    "#     # Initialize empty list of outputs\n",
    "#     outputs = []\n",
    "    \n",
    "#     ### START CODE HERE ###\n",
    "    \n",
    "#     # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
    "#     a = None\n",
    "    \n",
    "#     # Step 2: Iterate for Ty steps\n",
    "#     for t in range(None):\n",
    "    \n",
    "#         # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "#         context = None\n",
    "        \n",
    "#         # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "#         # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "#         s, _, c = None\n",
    "        \n",
    "#         # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "#         out = None\n",
    "        \n",
    "#         # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "#         None\n",
    "    \n",
    "#     # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "#     model = None\n",
    "    \n",
    "    \n",
    "    # Defina las entradas de su modelo con una forma (Tx,)\n",
    "    # Define s0 (estado oculto inicial) y c0 (estado inicial de la celda)\n",
    "    # para el decodificador LSTM con forma (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Inicializar la lista vacía de salidas\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Paso 1: Definir su Bi-LSTM de preatención. (≈ 1 línea)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Paso 2: Iterar para los pasos Ty\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Paso 2.A: Realizar un paso del mecanismo de atención para recuperar \n",
    "        # el vector de contexto en el paso t (≈ 1 línea)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Paso 2.B: Aplicar la célula LSTM post-atención al vector \"contexto\".\n",
    "        # No olvides pasar: initial_state = [estado oculto, estado de la célula] (≈ 1 línea)\n",
    "        s, _, c = post_activation_LSTM_cell(inputs=context,\n",
    "                                           initial_state = (s, c))\n",
    "        \n",
    "        # Paso 2.C: Aplicar la capa densa a la salida del estado oculto de la LSTM post-atención (≈ 1 línea)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Paso 2.D: Añadir \"out\" a la lista de \"salidas\" (≈ 1 línea)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Paso 3: Crear instancia del modelo tomando tres entradas y devolviendo la lista de salidas. (≈ 1 línea)\n",
    "    model = Model(inputs= [X, s0, c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['InputLayer', [(None, 30, 37)], 0], ['InputLayer', [(None, 64)], 0], ['Bidirectional', (None, 30, 64), 17920], ['RepeatVector', (None, 30, 64), 0, 30], ['Concatenate', (None, 30, 128), 0], ['Dense', (None, 30, 10), 1290, 'tanh'], ['Dense', (None, 30, 1), 11, 'relu'], ['Activation', (None, 30, 1), 0], ['Dot', (None, 1, 64), 0], ['InputLayer', [(None, 64)], 0], ['LSTM', [(None, 64), (None, 64), (None, 64)], 33024, [(None, 1, 64), (None, 64), (None, 64)], 'tanh'], ['Dense', (None, 11), 715, 'softmax']]\n",
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "from test_utils import *\n",
    "\n",
    "def modelf_test(target):\n",
    "    m = 10\n",
    "    Tx = 30\n",
    "    n_a = 32\n",
    "    n_s = 64\n",
    "    len_human_vocab = 37\n",
    "    len_machine_vocab = 11\n",
    "    \n",
    "    \n",
    "    model = target(Tx, Ty, n_a, n_s, len_human_vocab, len_machine_vocab)\n",
    "    \n",
    "    print(summary(model))\n",
    "\n",
    "    \n",
    "    expected_summary = [['InputLayer', [(None, 30, 37)], 0],\n",
    "                         ['InputLayer', [(None, 64)], 0],\n",
    "                         ['Bidirectional', (None, 30, 64), 17920],\n",
    "                         ['RepeatVector', (None, 30, 64), 0, 30],\n",
    "                         ['Concatenate', (None, 30, 128), 0],\n",
    "                         ['Dense', (None, 30, 10), 1290, 'tanh'],\n",
    "                         ['Dense', (None, 30, 1), 11, 'relu'],\n",
    "                         ['Activation', (None, 30, 1), 0],\n",
    "                         ['Dot', (None, 1, 64), 0],\n",
    "                         ['InputLayer', [(None, 64)], 0],\n",
    "                         ['LSTM',[(None, 64), (None, 64), (None, 64)], 33024,[(None, 1, 64), (None, 64), (None, 64)],'tanh'],\n",
    "                         ['Dense', (None, 11), 715, 'softmax']]\n",
    "\n",
    "    assert len(model.outputs) == 10, f\"Wrong output shape. Expected 10 != {len(model.outputs)}\"\n",
    "\n",
    "    comparator(summary(model), expected_summary)\n",
    "    \n",
    "\n",
    "modelf_test(modelf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--RX7hSsh3Mo"
   },
   "source": [
    "Ejecute la siguiente celda para crear su modelo.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "Run the following cell to create your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 20837,
     "status": "ok",
     "timestamp": 1612468518050,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "psdd-Ac6h3Mp"
   },
   "outputs": [],
   "source": [
    "model = modelf(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUJw7Xohh3Mp"
   },
   "source": [
    "#### Nota para la resolución de problemas\n",
    "* Si está obteniendo errores repetidos después de una implementación inicialmente incorrecta de \"model\", pero cree que ha corregido el error, puede seguir viendo mensajes de error al construir su modelo.  \n",
    "* Una solución es guardar y reiniciar su núcleo (o apagar y luego reiniciar su portátil), y volver a ejecutar las celdas.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "#### Troubleshooting Note\n",
    "* If you are getting repeated errors after an initially incorrect implementation of \"model\", but believe that you have corrected the error, you may still see error messages when building your model.  \n",
    "* A solution is to save and restart your kernel (or shutdown then restart your notebook), and re-run the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgeU_I9_h3Mp"
   },
   "source": [
    "Obtengamos un resumen del modelo para comprobar si coincide con el resultado esperado.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "Let's get a summary of the model to check if it matches the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20835,
     "status": "ok",
     "timestamp": 1612468518050,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "tX0vaYmPh3Mq",
    "outputId": "336b9248-70b0-4379-be95-95366874c02a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 30, 37)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[18][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[10][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[11][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[12][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[13][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[14][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[15][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[16][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[17][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[18][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[19][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30, 10)       1290        concatenate[10][0]               \n",
      "                                                                 concatenate[11][0]               \n",
      "                                                                 concatenate[12][0]               \n",
      "                                                                 concatenate[13][0]               \n",
      "                                                                 concatenate[14][0]               \n",
      "                                                                 concatenate[15][0]               \n",
      "                                                                 concatenate[16][0]               \n",
      "                                                                 concatenate[17][0]               \n",
      "                                                                 concatenate[18][0]               \n",
      "                                                                 concatenate[19][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 1)        11          dense[10][0]                     \n",
      "                                                                 dense[11][0]                     \n",
      "                                                                 dense[12][0]                     \n",
      "                                                                 dense[13][0]                     \n",
      "                                                                 dense[14][0]                     \n",
      "                                                                 dense[15][0]                     \n",
      "                                                                 dense[16][0]                     \n",
      "                                                                 dense[17][0]                     \n",
      "                                                                 dense[18][0]                     \n",
      "                                                                 dense[19][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_1[10][0]                   \n",
      "                                                                 dense_1[11][0]                   \n",
      "                                                                 dense_1[12][0]                   \n",
      "                                                                 dense_1[13][0]                   \n",
      "                                                                 dense_1[14][0]                   \n",
      "                                                                 dense_1[15][0]                   \n",
      "                                                                 dense_1[16][0]                   \n",
      "                                                                 dense_1[17][0]                   \n",
      "                                                                 dense_1[18][0]                   \n",
      "                                                                 dense_1[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 64)        0           attention_weights[10][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[11][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[12][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[13][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[14][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[15][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[16][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[17][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[18][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[19][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 64), (None,  33024       dot[10][0]                       \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot[11][0]                       \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[10][2]                      \n",
      "                                                                 dot[12][0]                       \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[11][2]                      \n",
      "                                                                 dot[13][0]                       \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[12][2]                      \n",
      "                                                                 dot[14][0]                       \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[13][2]                      \n",
      "                                                                 dot[15][0]                       \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[14][2]                      \n",
      "                                                                 dot[16][0]                       \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[15][2]                      \n",
      "                                                                 dot[17][0]                       \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[16][2]                      \n",
      "                                                                 dot[18][0]                       \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[17][2]                      \n",
      "                                                                 dot[19][0]                       \n",
      "                                                                 lstm[18][0]                      \n",
      "                                                                 lstm[18][2]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11)           715         lstm[10][0]                      \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[18][0]                      \n",
      "                                                                 lstm[19][0]                      \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiqCePt5h3Mr"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "Here is the summary you should see\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Total params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         52,960\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **Trainable params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         52,960\n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **Non-trainable params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         0\n",
    "        </td>\n",
    "    </tr>\n",
    "                    <tr>\n",
    "        <td>\n",
    "            **bidirectional_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 64)  \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **repeat_vector_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 64) \n",
    "        </td>\n",
    "    </tr>\n",
    "                <tr>\n",
    "        <td>\n",
    "            **concatenate_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 128) \n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **attention_weights's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 1)  \n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **dot_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 1, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "           <tr>\n",
    "        <td>\n",
    "            **dense_3's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 11) \n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u3D9Odhh3Ms"
   },
   "source": [
    "<a name='ex-3'></a>\n",
    "### Ejercicio 3 - Compilar el modelo\n",
    "\n",
    "* Después de crear tu modelo en Keras, necesitas compilarlo y definir la función de pérdida, el optimizador y las métricas que quieres utilizar. \n",
    "    * Función de pérdida: 'categorical_crossentropy'.\n",
    "    * Optimizador: [Adam](https://keras.io/optimizers/#adam) [optimizador](https://keras.io/optimizers/#usage-of-optimizers)\n",
    "        - learning rate = 0.005 \n",
    "        - $\\beta_1$ = 0.9\n",
    "        - $\\beta_2$ = 0.999$\n",
    "        - decay = 0.01  \n",
    "    * métrica: 'accuracy'\n",
    "    \n",
    "    Código de ejemplo\n",
    "    ```Python\n",
    "    optimizer = Adam(lr=..., beta_1=..., beta_2=..., decay=...)\n",
    "    model.compile(optimizer=..., loss=..., metrics=[...])\n",
    "    ```\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - Compile the Model\n",
    "\n",
    "* After creating your model in Keras, you need to compile it and define the loss function, optimizer and metrics you want to use. \n",
    "    * Loss function: 'categorical_crossentropy'.\n",
    "    * Optimizer: [Adam](https://keras.io/optimizers/#adam) [optimizer](https://keras.io/optimizers/#usage-of-optimizers)\n",
    "        - learning rate = 0.005 \n",
    "        - $\\beta_1 = 0.9$\n",
    "        - $\\beta_2 = 0.999$\n",
    "        - decay = 0.01  \n",
    "    * metric: 'accuracy'\n",
    "    \n",
    "Sample code\n",
    "```Python\n",
    "optimizer = Adam(lr=..., beta_1=..., beta_2=..., decay=...)\n",
    "model.compile(optimizer=..., loss=..., metrics=[...])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 20835,
     "status": "ok",
     "timestamp": 1612468518051,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "sBFRJ49rh3Ms"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈2 lines)\n",
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01) # Adam(...) \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TESTS\n",
    "assert opt.lr == 0.005, \"Set the lr parameter to 0.005\"\n",
    "assert opt.beta_1 == 0.9, \"Set the beta_1 parameter to 0.9\"\n",
    "assert opt.beta_2 == 0.999, \"Set the beta_2 parameter to 0.999\"\n",
    "assert opt.decay == 0.01, \"Set the decay parameter to 0.01\"\n",
    "assert model.loss == \"categorical_crossentropy\", \"Wrong loss. Use 'categorical_crossentropy'\"\n",
    "assert model.optimizer == opt, \"Use the optimizer that you have instantiated\"\n",
    "assert model.compiled_metrics._user_metrics[0] == 'accuracy', \"set metrics to ['accuracy']\"\n",
    "\n",
    "print(\"\\033[92mAll tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz71nM3oh3Ms"
   },
   "source": [
    "#### Definir las entradas y salidas, y ajustar el modelo\n",
    "El último paso es definir todas las entradas y salidas para ajustar el modelo:\n",
    "- Tienes la entrada `Xoh` de forma $(m = 10000, T_x = 30, human\\_vocab=37)$ que contiene los ejemplos de entrenamiento.\n",
    "- Necesitas crear `s0` y `c0` para inicializar tu `post_attention_LSTM_cell` con ceros.\n",
    "- Dado el `model()` que has codificado, necesitas que las \"salidas\" sean una lista de 10 elementos de forma (m, T_y). \n",
    "    - La lista `outputs[i][0], ..., outputs[i][Ty]` representa las etiquetas verdaderas (caracteres) correspondientes al ejemplo de entrenamiento $i^{th}$ (`Xoh[i]`). \n",
    "    - `outputs[i][j]` es la etiqueta verdadera del carácter $j^{th}$ en el ejemplo de entrenamiento $i^{th}$.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "#### Define inputs and outputs, and fit the model\n",
    "The last step is to define all your inputs and outputs to fit the model:\n",
    "- You have input `Xoh` of shape $(m = 10000, T_x = 30, human\\_vocab=37)$ containing the training examples.\n",
    "- You need to create `s0` and `c0` to initialize your `post_attention_LSTM_cell` with zeros.\n",
    "- Given the `model()` you coded, you need the \"outputs\" to be a list of 10 elements of shape (m, T_y). \n",
    "    - The list `outputs[i][0], ..., outputs[i][Ty]` represents the true labels (characters) corresponding to the $i^{th}$ training example (`Xoh[i]`). \n",
    "    - `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 20833,
     "status": "ok",
     "timestamp": 1612468518051,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "USFiNKYhh3Mt"
   },
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVkITGi3h3Mt"
   },
   "source": [
    "Ahora vamos a ajustar el modelo y ejecutarlo durante una época.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "Let's now fit the model and run it for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47944,
     "status": "ok",
     "timestamp": 1612468545172,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "tPuwY45bh3Mt",
    "outputId": "ec9dfc4c-1dcb-4577-d872-474f79c60d5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 13s 128ms/step - loss: 16.6658 - dense_2_loss: 1.1100 - dense_2_1_loss: 0.9938 - dense_2_2_loss: 1.7791 - dense_2_3_loss: 2.6455 - dense_2_4_loss: 0.8580 - dense_2_5_loss: 1.2956 - dense_2_6_loss: 2.7538 - dense_2_7_loss: 1.0072 - dense_2_8_loss: 1.7022 - dense_2_9_loss: 2.5204 - dense_2_accuracy: 0.5577 - dense_2_1_accuracy: 0.7317 - dense_2_2_accuracy: 0.3107 - dense_2_3_accuracy: 0.0973 - dense_2_4_accuracy: 0.8953 - dense_2_5_accuracy: 0.3354 - dense_2_6_accuracy: 0.0452 - dense_2_7_accuracy: 0.9012 - dense_2_8_accuracy: 0.2815 - dense_2_9_accuracy: 0.1324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d20356210>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUikskCoh3Mt"
   },
   "source": [
    "Mientras entrena, puede ver la pérdida y la precisión en cada una de las 10 posiciones de la salida. La siguiente tabla le brinda un ejemplo de cuáles podrían ser las precisiones si el lote tuviera 2 ejemplos:\n",
    "\n",
    "<img src=\"images/table.png\" style=\"width:700;height:200px;\"> <br>\n",
    "<caption><center>Por lo tanto, `dense_2_acc_8: 0.89` significa que está prediciendo el séptimo carácter de la salida correctamente el 89 % del tiempo en el lote de datos actual. </center></caption>\n",
    "\n",
    "\n",
    "Hemos utilizado este modelo durante más tiempo y ahorrado los pesos. Ejecute la siguiente celda para cargar nuestros pesos. (Al entrenar un modelo durante varios minutos, debería poder obtener un modelo de precisión similar, pero cargar nuestro modelo le ahorrará tiempo).\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "While training you can see the loss as well as the accuracy on each of the 10 positions of the output. The table below gives you an example of what the accuracies could be if the batch had 2 examples: \n",
    "\n",
    "<img src=\"images/table.png\" style=\"width:700;height:200px;\"> <br>\n",
    "<caption><center>Thus, `dense_2_acc_8: 0.89` means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. </center></caption>\n",
    "\n",
    "\n",
    "We have run this model for longer, and saved the weights. Run the next cell to load our weights. (By training a model for several minutes, you should be able to obtain a model of similar accuracy, but loading our model will save you time.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 47942,
     "status": "ok",
     "timestamp": 1612468545173,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "ooiZCOx0h3Mu"
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUUD9yXxh3Mu"
   },
   "source": [
    "Ahora puedes ver los resultados en nuevos ejemplos.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "You can now see the results on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53835,
     "status": "ok",
     "timestamp": 1612468551077,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "rQ8sd_cuh3Mv",
    "outputId": "c37e92ac-5c60-4caf-b843-6aaeaa37be25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-33 \n",
      "\n",
      "source: 5 April 09\n",
      "output: 2009-04-05 \n",
      "\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-20 \n",
      "\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10 \n",
      "\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09 \n",
      "\n",
      "source: March 3 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', \n",
    "            'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "s00 = np.zeros((1, n_s))\n",
    "c00 = np.zeros((1, n_s))\n",
    "for example in EXAMPLES:\n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    #print(source)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    source = np.swapaxes(source, 0, 1)\n",
    "    source = np.expand_dims(source, axis=0)\n",
    "    prediction = model.predict([source, s00, c00])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjdEQiIDh3Mv"
   },
   "source": [
    "También puedes cambiar estos ejemplos para probarlos con los tuyos propios. La siguiente parte le dará una mejor idea de lo que hace el mecanismo de atención, es decir, a qué parte de la entrada presta atención la red cuando genera un carácter de salida concreto. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "You can also change these examples to test with your own examples. The next part will give you a better sense of what the attention mechanism is doing--i.e., what part of the input the network is paying attention to when generating a particular output character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XIxtN4xh3Mv"
   },
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Visualización de la atención (opcional/sin calificar)\n",
    "\n",
    "Dado que el problema tiene una longitud de salida fija de 10, también es posible realizar esta tarea utilizando 10 unidades softmax diferentes para generar los 10 caracteres de la salida. Pero una ventaja del modelo de atención es que cada parte de la salida (como el mes) sabe que necesita depender solo de una pequeña parte de la entrada (los caracteres de la entrada que dan el mes). Podemos visualizar qué está mirando cada parte de la salida en qué parte de la entrada.\n",
    "\n",
    "Considere la tarea de traducir \"Sábado 9 de mayo de 2018\" a \"2018-05-09\". Si visualizamos el $\\alpha^{\\langle t, t' \\rangle}$ calculado, obtenemos esto:\n",
    "\n",
    "<img src=\"images/date_attention.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> Figura 8: Mapa de atención completa</center></caption>\n",
    "\n",
    "Observe cómo la salida ignora la parte \"Sábado\" de la entrada. Ninguno de los pasos de tiempo de salida presta mucha atención a esa parte de la entrada. También vemos que 9 se tradujo como 09 y May se tradujo correctamente a 05, con la salida prestando atención a las partes de la entrada que necesita para hacer la traducción. El año requiere principalmente que preste atención a la entrada \"18\" para generar \"2018\".\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "<a name='3'></a>\n",
    "## 3 - Visualizing Attention (Optional / Ungraded)\n",
    "\n",
    "Since the problem has a fixed output length of 10, it is also possible to carry out this task using 10 different softmax units to generate the 10 characters of the output. But one advantage of the attention model is that each part of the output (such as the month) knows it needs to depend only on a small part of the input (the characters in the input giving the month). We can  visualize what each part of the output is looking at which part of the input.\n",
    "\n",
    "Consider the task of translating \"Saturday 9 May 2018\" to \"2018-05-09\". If we visualize the computed $\\alpha^{\\langle t, t' \\rangle}$ we get this: \n",
    "\n",
    "<img src=\"images/date_attention.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 8**: Full Attention Map</center></caption>\n",
    "\n",
    "Notice how the output ignores the \"Saturday\" portion of the input. None of the output timesteps are paying much attention to that portion of the input. We also see that 9 has been translated as 09 and May has been correctly translated into 05, with the output paying attention to the parts of the input it needs to to make the translation. The year mostly requires it to pay attention to the input's \"18\" in order to generate \"2018.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrP893IFh3Mv"
   },
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Obtener los pesos de atención de la red\n",
    "\n",
    "Ahora vamos a visualizar los valores de la atención en su red. Vamos a propagar un ejemplo a través de la red, a continuación, visualizar los valores de $\\alpha^{\\langle t, t' \\rangle}$. \n",
    "\n",
    "Para averiguar dónde se encuentran los valores de la atención, vamos a empezar por imprimir un resumen del modelo .\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "<a name='3-1'></a>\n",
    "### 3.1 - Getting the Attention Weights From the Network\n",
    "\n",
    "Lets now visualize the attention values in your network. We'll propagate an example through the network, then visualize the values of $\\alpha^{\\langle t, t' \\rangle}$. \n",
    "\n",
    "To figure out where the attention values are located, let's start by printing a summary of the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53826,
     "status": "ok",
     "timestamp": 1612468551078,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "RfiLrfKIh3Mv",
    "outputId": "b6690603-209c-40d7-f352-235a689d1aea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 30, 37)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[18][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[10][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[11][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[12][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[13][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[14][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[15][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[16][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[17][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[18][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[19][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30, 10)       1290        concatenate[10][0]               \n",
      "                                                                 concatenate[11][0]               \n",
      "                                                                 concatenate[12][0]               \n",
      "                                                                 concatenate[13][0]               \n",
      "                                                                 concatenate[14][0]               \n",
      "                                                                 concatenate[15][0]               \n",
      "                                                                 concatenate[16][0]               \n",
      "                                                                 concatenate[17][0]               \n",
      "                                                                 concatenate[18][0]               \n",
      "                                                                 concatenate[19][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 1)        11          dense[10][0]                     \n",
      "                                                                 dense[11][0]                     \n",
      "                                                                 dense[12][0]                     \n",
      "                                                                 dense[13][0]                     \n",
      "                                                                 dense[14][0]                     \n",
      "                                                                 dense[15][0]                     \n",
      "                                                                 dense[16][0]                     \n",
      "                                                                 dense[17][0]                     \n",
      "                                                                 dense[18][0]                     \n",
      "                                                                 dense[19][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_1[10][0]                   \n",
      "                                                                 dense_1[11][0]                   \n",
      "                                                                 dense_1[12][0]                   \n",
      "                                                                 dense_1[13][0]                   \n",
      "                                                                 dense_1[14][0]                   \n",
      "                                                                 dense_1[15][0]                   \n",
      "                                                                 dense_1[16][0]                   \n",
      "                                                                 dense_1[17][0]                   \n",
      "                                                                 dense_1[18][0]                   \n",
      "                                                                 dense_1[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 64)        0           attention_weights[10][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[11][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[12][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[13][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[14][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[15][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[16][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[17][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[18][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[19][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 64), (None,  33024       dot[10][0]                       \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot[11][0]                       \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[10][2]                      \n",
      "                                                                 dot[12][0]                       \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[11][2]                      \n",
      "                                                                 dot[13][0]                       \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[12][2]                      \n",
      "                                                                 dot[14][0]                       \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[13][2]                      \n",
      "                                                                 dot[15][0]                       \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[14][2]                      \n",
      "                                                                 dot[16][0]                       \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[15][2]                      \n",
      "                                                                 dot[17][0]                       \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[16][2]                      \n",
      "                                                                 dot[18][0]                       \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[17][2]                      \n",
      "                                                                 dot[19][0]                       \n",
      "                                                                 lstm[18][0]                      \n",
      "                                                                 lstm[18][2]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11)           715         lstm[10][0]                      \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[18][0]                      \n",
      "                                                                 lstm[19][0]                      \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbcprBCPh3Mv"
   },
   "source": [
    "Navega por la salida de `model.summary()` anterior. Puedes ver que la capa llamada `attention_weights` emite las `alphas` de forma (m, 30, 1) antes de que `dot_2` calcule el vector de contexto para cada paso de tiempo $t = 0, \\ldots, T_y-1$. Vamos a obtener los pesos de atención de esta capa.\n",
    "\n",
    "La función `attention_map()` saca los valores de atención de tu modelo y los traza.\n",
    "\n",
    "**Nota: Somos conscientes de que puedes encontrarte con un error al ejecutar la celda de abajo a pesar de una implementación válida para el Ejercicio 2 - `modelf` anterior. Si obtienes el error, por favor infórmalo en este [Tema](https://discourse.deeplearning.ai/t/error-in-optional-ungraded-part-of-neural-machine-translation-w3a1/1096) en [Discourse](https://discourse.deeplearning.ai) ya que nos ayudará a mejorar nuestro contenido. \n",
    "\n",
    "Si no te has unido a nuestra comunidad de Discourse, puedes hacerlo haciendo clic en el enlace: http://bit.ly/dls-discourse\n",
    "\n",
    "Y no te preocupes por el error, no afectará a la calificación de esta tarea.\n",
    "\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "Navigate through the output of `model.summary()` above. You can see that the layer named `attention_weights` outputs the `alphas` of shape (m, 30, 1) before `dot_2` computes the context vector for every time step $t = 0, \\ldots, T_y-1$. Let's get the attention weights from this layer.\n",
    "\n",
    "The function `attention_map()` pulls out the attention values from your model and plots them.\n",
    "\n",
    "**Note**: We are aware that you might run into an error running the cell below despite a valid implementation for Exercise 2 - `modelf` above. If  you get the error kindly report it on this [Topic](https://discourse.deeplearning.ai/t/error-in-optional-ungraded-part-of-neural-machine-translation-w3a1/1096) on [Discourse](https://discourse.deeplearning.ai) as it'll help us improve our content. \n",
    "\n",
    "If you haven’t joined our Discourse community you can do so by clicking on the link: http://bit.ly/dls-discourse\n",
    "\n",
    "And don’t worry about the error, it will not affect the grading for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x612 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQ3qbIjqh3Mx"
   },
   "source": [
    "En el gráfico generado puedes observar los valores de los pesos de atención para cada carácter de la salida predicha. Examine este gráfico y compruebe que los lugares en los que la red presta atención tienen sentido para usted.\n",
    "\n",
    "En la aplicación de traducción de fechas, observará que la mayor parte del tiempo la atención ayuda a predecir el año, y no tiene mucho impacto en la predicción del día o del mes.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "On the generated plot you can observe the values of the attention weights for each character of the predicted output. Examine this plot and check that the places where the network is paying attention makes sense to you.\n",
    "\n",
    "In the date translation application, you will observe that most of the time attention helps predict the year, and doesn't have much impact on predicting the day or month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkpGu1Jkh3Mx"
   },
   "source": [
    "### ¡Felicidades!\n",
    "\n",
    "\n",
    "Has llegado al final de esta tarea \n",
    "\n",
    "#### Esto es lo que debes recordar\n",
    "\n",
    "- Los modelos de traducción automática pueden utilizarse para mapear de una secuencia a otra. Son útiles no sólo para traducir idiomas humanos (como francés->inglés), sino también para tareas como la traducción de formatos de fecha. \n",
    "- Un mecanismo de atención permite a una red centrarse en las partes más relevantes de la entrada al producir una parte específica de la salida. \n",
    "- Una red que utiliza un mecanismo de atención puede traducir de entradas de longitud $T_x$ a salidas de longitud $T_y$, donde $T_x$ y $T_y$ pueden ser diferentes. \n",
    "- Se pueden visualizar los pesos de atención $\\alpha^{\\langle t,t' \\rangle}$ para ver a qué presta atención la red mientras genera cada salida.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "### Congratulations!\n",
    "\n",
    "\n",
    "You have come to the end of this assignment \n",
    "\n",
    "#### Here's what you should remember\n",
    "\n",
    "- Machine translation models can be used to map from one sequence to another. They are useful not just for translating human languages (like French->English) but also for tasks like date format translation. \n",
    "- An attention mechanism allows a network to focus on the most relevant parts of the input when producing a specific part of the output. \n",
    "- A network using an attention mechanism can translate from inputs of length $T_x$ to outputs of length $T_y$, where $T_x$ and $T_y$ can be different. \n",
    "- You can visualize attention weights $\\alpha^{\\langle t,t' \\rangle}$ to see what the network is paying attention to while generating each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaKA2u4uh3My"
   },
   "source": [
    "Enhorabuena por haber terminado esta tarea. Ahora es capaz de implementar un modelo de atención y utilizarlo para aprender mapeos complejos de una secuencia a otra. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "Congratulations on finishing this assignment! You are now able to implement an attention model and use it to learn complex mappings from one sequence to another. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Solution_Neural_machine_translation_with_attention_v4a.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "DLSC5W3-1A"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
