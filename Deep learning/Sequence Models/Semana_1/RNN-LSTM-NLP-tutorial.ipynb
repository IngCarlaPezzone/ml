{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del Lenguaje Natural \n",
    "El Procesamiento del Lenguaje Natural (PNL) es una disciplina de la informática que se ocupa de la comunicación entre los lenguajes naturales (humanos) y los lenguajes informáticos. Un ejemplo común de PNL es algo como el corrector ortográfico o el autocompletado. Esencialmente, la PNL es el campo que se centra en cómo los ordenadores pueden entender y/o procesar los lenguajes naturales/humanos. \n",
    "\n",
    "### Redes Neuronales Recurrentes\n",
    "\n",
    "En este tutorial introduciremos un nuevo tipo de red neuronal que es mucho más capaz de procesar datos secuenciales como texto o caracteres, llamada **red neuronal recurrente** (RNN para abreviar). \n",
    "\n",
    "Aprenderemos a utilizar una red neuronal recurrente para hacer lo siguiente:\n",
    "- Análisis de Sentimientos\n",
    "- Generación de caracteres \n",
    "\n",
    "Las RNN son complejas y se presentan en muchas formas diferentes, por lo que en este tutorial nos centraremos en cómo funcionan y en el tipo de problemas para los que son más adecuadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de la secuencia\n",
    "En los tutoriales anteriores nos centramos en datos que podíamos representar como un punto de datos estático donde la noción de tiempo o paso era irrelevante. Tomemos por ejemplo nuestros datos de imagen, era simplemente un tensor de forma (ancho, alto, canales). Esos datos no cambian ni les importa la noción de tiempo. \n",
    "\n",
    "En este tutorial veremos las secuencias de texto y aprenderemos cómo podemos codificarlas de forma significativa. A diferencia de las imágenes, los datos secuenciales, como las largas cadenas de texto, los patrones climáticos, los vídeos y, en realidad, cualquier cosa en la que la noción de paso o tiempo sea relevante, necesita ser procesada y manejada de una manera especial. \n",
    "\n",
    "¿Pero qué quiero decir con secuencias y por qué los datos de texto son una secuencia? Bueno, esa es una buena pregunta. Dado que los datos textuales contienen muchas palabras que se suceden en un orden muy específico y significativo, tenemos que ser capaces de seguir la pista de cada palabra y de cuándo aparece en los datos. Codificar simplemente, por ejemplo, un párrafo entero de texto en un punto de datos no nos daría una imagen muy significativa de los datos y sería muy difícil hacer algo con ellos. Por eso tratamos el texto como una secuencia y procesamos una palabra cada vez. Seguiremos la pista de dónde aparece cada una de esas palabras y utilizaremos esa información para intentar comprender el significado de los trozos de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificación del texto\n",
    "Como sabemos, los modelos de aprendizaje automático y las redes neuronales no aceptan datos de texto en bruto como entrada. Esto significa que debemos codificar de alguna manera nuestros datos textuales en valores numéricos que nuestros modelos puedan entender. Hay muchas formas de hacerlo y a continuación veremos algunos ejemplos. \n",
    "\n",
    "Antes de entrar en los diferentes métodos de codificación/preprocesamiento, entendamos la información que podemos obtener de los datos textuales observando las siguientes dos críticas de películas.\n",
    "\n",
    "Pensé que la película iba a ser mala, pero en realidad fue increíble.\n",
    "\n",
    "Pensé que la película iba a ser increíble, pero en realidad era mala\".\n",
    "\n",
    "Aunque estas dos frases son muy parecidas, sabemos que tienen significados muy diferentes. Esto se debe a la **ordenación** de las palabras, una propiedad muy importante de los datos textuales.\n",
    "\n",
    "Ahora, tenlo en cuenta mientras consideramos algunas formas diferentes de codificar nuestros datos textuales.\n",
    "\n",
    "### Bolsa de palabras\n",
    "La primera y más sencilla forma de codificar nuestros datos es utilizar algo llamado **bolsa de palabras**. Se trata de una técnica bastante sencilla en la que cada palabra de una frase se codifica con un número entero y se arroja a una colección que no mantiene el orden de las palabras pero sí la frecuencia. Echa un vistazo a la siguiente función de python que codifica una cadena de texto en una bolsa de palabras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # global word_encoding\n",
    "    vocab = {}  # asigna una palabra a un número entero que la representa\n",
    "    word_encoding = 1\n",
    "    # crear una lista de todas las palabras en el texto, bien asumir que no hay gramática en nuestro texto para este ejemplo\n",
    "    words = text.lower().split(\" \") \n",
    "    # almacena todas las codificaciones y su frecuencia\n",
    "    bag = {} \n",
    "\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            encoding = vocab[word]  # obtener la codificación del vocabulario\n",
    "        else:\n",
    "            vocab[word]     = word_encoding\n",
    "            encoding        = word_encoding\n",
    "            word_encoding  += 1\n",
    "    \n",
    "        #if encoding in bag:\n",
    "        #    bag[encoding] += 1\n",
    "        #else:\n",
    "        #    bag[encoding] = 1\n",
    "        bag[encoding] = bag.get(encoding,0)+1\n",
    "  \n",
    "    return bag , vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}, {'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9})\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "bag = bag_of_words(text)\n",
    "print(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta no es realmente la forma en que lo haríamos en la práctica, pero espero que te dé una idea de cómo funciona la bolsa de palabras. Observa que hemos perdido el orden de aparición de las palabras. De hecho, veamos cómo funciona esta codificación para las dos frases que mostramos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabla(titulo,bolsa):\n",
    "    print(titulo+ \" :\")\n",
    "    print(\"+\"+\"-\"*41+\"+\")\n",
    "    print(f\"|{'palabra':^20}| {'frecuencia':^10} | {'index':5} |\")\n",
    "    print(\"+\"+\"-\"*41+\"+\")\n",
    "    for palabra, index in bolsa[1].items():\n",
    "        print(f\"|{palabra:^20}| {bolsa[0][index]:^10} | {index:>5} |\")\n",
    "    print(\"+\"+\"-\"*41+\"+\")\n",
    "\n",
    "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
    "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
    "\n",
    "pos_bag = bag_of_words(positive_review)\n",
    "neg_bag = bag_of_words(negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive :\n",
      "+-----------------------------------------+\n",
      "|      palabra       | frecuencia | index |\n",
      "+-----------------------------------------+\n",
      "|         i          |     1      |     1 |\n",
      "|      thought       |     1      |     2 |\n",
      "|        the         |     1      |     3 |\n",
      "|       movie        |     1      |     4 |\n",
      "|        was         |     2      |     5 |\n",
      "|       going        |     1      |     6 |\n",
      "|         to         |     1      |     7 |\n",
      "|         be         |     1      |     8 |\n",
      "|        bad         |     1      |     9 |\n",
      "|        but         |     1      |    10 |\n",
      "|         it         |     1      |    11 |\n",
      "|      actually      |     1      |    12 |\n",
      "|      amazing       |     1      |    13 |\n",
      "+-----------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "tabla(\"Positive\", pos_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative :\n",
      "+-----------------------------------------+\n",
      "|      palabra       | frecuencia | index |\n",
      "+-----------------------------------------+\n",
      "|         i          |     1      |     1 |\n",
      "|      thought       |     1      |     2 |\n",
      "|        the         |     1      |     3 |\n",
      "|       movie        |     1      |     4 |\n",
      "|        was         |     2      |     5 |\n",
      "|       going        |     1      |     6 |\n",
      "|         to         |     1      |     7 |\n",
      "|         be         |     1      |     8 |\n",
      "|      amazing       |     1      |     9 |\n",
      "|        but         |     1      |    10 |\n",
      "|         it         |     1      |    11 |\n",
      "|      actually      |     1      |    12 |\n",
      "|        bad         |     1      |    13 |\n",
      "+-----------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "tabla(\"Negative\", neg_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que, aunque estas frases tienen un significado muy diferente, están codificadas exactamente de la misma manera. Obviamente, esto no va a funcionar. Veamos otros métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de enteros\n",
    "La siguiente técnica que veremos se llama **codificación de enteros**. Se trata de representar cada palabra o carácter de una frase como un único número entero y mantener el orden de estas palabras. Esto debería solucionar el problema que vimos antes de perder el orden de las palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(text):\n",
    "    vocab = {}  \n",
    "    word_encoding = 1\n",
    "\n",
    "    words = text.lower().split(\" \") \n",
    "    encoding = [] \n",
    "    print(\"+\"+\"-\"*31+\"+\")\n",
    "    print(f\"|{'Palabra':^20} | {'Codigo':^8}|\")\n",
    "    print(\"+\"+\"-\"*31+\"+\")\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            code = vocab[word]  \n",
    "            encoding.append(code) \n",
    "        else:\n",
    "            vocab[word] = word_encoding\n",
    "            encoding.append(word_encoding)\n",
    "            code = vocab[word] \n",
    "            word_encoding += 1\n",
    "            \n",
    "        print(f\"|{word:^20} | {code:^8}|\")\n",
    "    print(\"+\"+\"-\"*31+\"+\")\n",
    "    return encoding , vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|      Palabra        |  Codigo |\n",
      "+-------------------------------+\n",
      "|        this         |    1    |\n",
      "|         is          |    2    |\n",
      "|         a           |    3    |\n",
      "|        test         |    4    |\n",
      "|         to          |    5    |\n",
      "|        see          |    6    |\n",
      "|         if          |    7    |\n",
      "|        this         |    1    |\n",
      "|        test         |    4    |\n",
      "|        will         |    8    |\n",
      "|        work         |    9    |\n",
      "|         is          |    2    |\n",
      "|         is          |    2    |\n",
      "|        test         |    4    |\n",
      "|         a           |    3    |\n",
      "|         a           |    3    |\n",
      "+-------------------------------+\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "encoding = one_hot_encoding(text)\n",
    "#print(encoding)\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:\n",
      "+-------------------------------+\n",
      "|      Palabra        |  Codigo |\n",
      "+-------------------------------+\n",
      "|         i           |    1    |\n",
      "|      thought        |    2    |\n",
      "|        the          |    3    |\n",
      "|       movie         |    4    |\n",
      "|        was          |    5    |\n",
      "|       going         |    6    |\n",
      "|         to          |    7    |\n",
      "|         be          |    8    |\n",
      "|        bad          |    9    |\n",
      "|        but          |    10   |\n",
      "|         it          |    11   |\n",
      "|        was          |    5    |\n",
      "|      actually       |    12   |\n",
      "|      amazing        |    13   |\n",
      "+-------------------------------+\n",
      "Negative:\n",
      "+-------------------------------+\n",
      "|      Palabra        |  Codigo |\n",
      "+-------------------------------+\n",
      "|         i           |    1    |\n",
      "|      thought        |    2    |\n",
      "|        the          |    3    |\n",
      "|       movie         |    4    |\n",
      "|        was          |    5    |\n",
      "|       going         |    6    |\n",
      "|         to          |    7    |\n",
      "|         be          |    8    |\n",
      "|      amazing        |    9    |\n",
      "|        but          |    10   |\n",
      "|         it          |    11   |\n",
      "|        was          |    5    |\n",
      "|      actually       |    12   |\n",
      "|        bad          |    13   |\n",
      "+-------------------------------+\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
    "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
    "\n",
    "print(\"Positive:\")\n",
    "pos_encode = one_hot_encoding(positive_review)\n",
    "print(\"Negative:\")\n",
    "neg_encode = one_hot_encoding(negative_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucho mejor, ahora llevamos la cuenta del orden de las palabras y podemos saber dónde ocurre cada una. Pero esto todavía tiene algunos problemas. Lo ideal sería que, al codificar las palabras, las palabras similares tuvieran etiquetas similares y las palabras diferentes tuvieran etiquetas muy diferentes. Por ejemplo, las palabras feliz y alegre deberían tener etiquetas muy parecidas para que podamos determinar que son similares. Mientras que palabras como horrible y asombroso deberían tener etiquetas muy diferentes. El método que hemos visto anteriormente no podrá hacer algo así por nosotros. Esto podría significar que el modelo tendrá un tiempo muy difícil para determinar si dos palabras son similares o no, lo que podría resultar en algunos impactos de rendimiento bastante drásticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Por suerte, existe un tercer método que es muy superior, la **incrustación de palabras**. Este método mantiene intacto el orden de las palabras y codifica palabras similares con etiquetas muy parecidas. Intenta no sólo codificar la frecuencia y el orden de las palabras, sino también el significado de esas palabras en la frase. Codifica cada palabra como un vector denso que representa su contexto en la frase.\n",
    "\n",
    "A diferencia de las técnicas anteriores, las incrustaciones de palabras se aprenden observando muchos ejemplos de entrenamiento diferentes. Puede añadir lo que se llama una *capa de incrustación* al principio de su modelo y, mientras éste se entrena, su capa de incrustación aprenderá las incrustaciones correctas de las palabras. También puedes utilizar capas de incrustación preentrenadas.\n",
    "\n",
    "Esta es la técnica que utilizaremos para nuestros ejemplos y su implementación se mostrará más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Redes neuronales recurrentes (RNN)\n",
    "Ahora que hemos aprendido un poco sobre cómo podemos codificar el texto es el momento de sumergirnos en las redes neuronales recurrentes. Hasta este punto hemos estado utilizando algo llamado **redes neuronales de avance**. Esto significa simplemente que todos nuestros datos se alimentan hacia adelante (todos a la vez) de izquierda a derecha a través de la red. Esto estuvo bien para los problemas que consideramos antes, pero no funcionará muy bien para procesar texto. Después de todo, ni siquiera nosotros (los humanos) procesamos el texto de una sola vez. Leemos palabra por palabra, de izquierda a derecha, y mantenemos un registro del significado actual de la frase para poder entender el significado de la siguiente palabra. Esto es exactamente lo que hace una red neuronal recurrente. Cuando decimos red neuronal recurrente lo que realmente queremos decir es una red que contiene un bucle. Una RNN procesa una palabra a la vez mientras mantiene una memoria interna de lo que ya ha visto. Esto le permitirá tratar las palabras de forma diferente en función de su orden en una frase y construir lentamente una comprensión de toda la entrada, una palabra cada vez.\n",
    "\n",
    "Por eso tratamos nuestros datos de texto como una secuencia. Para que podamos pasar una palabra a la vez a la RNN.\n",
    "\n",
    "Veamos cómo podría ser una capa recurrente.\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "*Fuente: https://colah.github.io/posts/2015-08-Understanding-LSTMs/*\n",
    "\n",
    "\n",
    "Definamos qué significan todas estas variables antes de entrar en la explicación.\n",
    "\n",
    "**h<sub>t</sub>** Salida en un paso de tiempo t\n",
    "\n",
    "**x<sub>t</sub>** Entrada  en un paso de tiempo t\n",
    "\n",
    "**A** Capa Recurrente (bucle)\n",
    "\n",
    "Lo que su diagrama trata de ilustrar es que una capa recurrente procesa las palabras o la entrada de una en una en combinación con la salida de la iteración anterior. Así, a medida que avanzamos en la secuencia de entrada, construimos una comprensión más compleja del texto en su conjunto.\n",
    "\n",
    "Lo que acabamos de ver se llama **capa RNN simple**. Puede ser eficaz en el procesamiento de secuencias de texto más cortas para problemas sencillos, pero tiene muchas desventajas asociadas. Uno de ellos es el hecho de que, a medida que las secuencias de texto se hacen más largas, es cada vez más difícil para la red entender el texto correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSTM\n",
    "La capa que hemos analizado en profundidad anteriormente se llama *RNN simple*. Sin embargo, existen otras capas recurrentes (capas que contienen un bucle) que funcionan mucho mejor que una capa RNN simple. De la que hablaremos aquí se llama LSTM (Long Short-Term Memory). Esta capa funciona de forma muy similar a la capa RNN simple, pero añade una forma de acceder a las entradas de cualquier paso de tiempo en el pasado. Mientras que en nuestra capa RNN simple las entradas de los pasos de tiempo anteriores desaparecían gradualmente a medida que avanzábamos en la entrada. Con una LSTM tenemos una estructura de datos de memoria a largo plazo que almacena todas las entradas vistas anteriormente, así como cuándo las vimos. Esto nos permite acceder a cualquier valor anterior que queramos en cualquier momento. Esto añade complejidad a nuestra red y le permite descubrir más relaciones útiles entre las entradas y el momento en que aparecen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las redes neuronales recurrentes tienen la forma de una cadena de módulos repetidos de red neuronal. En RNN estándar, este módulo repetitivo tendrá una estructura muy simple, como una sola capa de tanh.\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los LSTM también tienen esta estructura similar a una cadena, pero el módulo repetitivo tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, hay cuatro que interactúan de una manera muy especial.\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorrido paso a paso de LSTM\n",
    "\n",
    "#### Primer Paso\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
    "\n",
    "El primer paso en nuestro LSTM es decidir qué información vamos a desechar del estado de la celda. Esta decisión la toma una capa sigmoidea llamada \"capa de puerta de olvido\". mira $h_{t - 1}$ y $X_{t} $, y genera un número entre 0 y 1 para cada número en el estado de la celda $C_{t - 1}$. Si $f_{t}$ es 1 representa \"guardar esto por completo\", mientras que un 0 representa \"deshacerse completamente de esto\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segundo Paso\n",
    "\n",
    "El siguiente paso es decidir qué nueva información vamos a almacenar en el estado de la celda. Esto tiene dos partes. Primero, una capa sigmoidea llamada \"capa de puerta de entrada\" decide qué valores actualizaremos. A continuación, una capa $\\tanh$ crea un vector de nuevos valores candidatos, $\\tilde{C}_{t}$, que podría agregarse al estado. En el siguiente paso, combinaremos estos dos para crear una actualización del estado.\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tercer Paso\n",
    "\n",
    "Multiplicamos el estado anterior por $f_{t}$ , olvidando las cosas que decidimos olvidar antes. Luego agregamos $i_{t} ∗ \\tilde{C}_{t}$. Estos son los nuevos valores candidatos, escalados por cuánto decidimos actualizar cada valor de estado.\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuarto Paso\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png) \n",
    "\n",
    "Finalmente, tenemos que decidir qué vamos a generar. Esta salida se basará en nuestro estado de celda, pero será una versión filtrada. Primero, ejecutamos una capa sigmoidea que decide qué partes del estado de la celda vamos a generar. Luego, ponemos el estado de la celda a través de $\\tanh$ ( para empujar los valores a estar entre −1 y 1 ) y multiplíquelo por la salida de la puerta sigmoidea, de modo que solo emitamos las partes que decidimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Variantes de la memoria a largo plazo\n",
    "Lo que he descrito hasta ahora es un LSTM bastante normal. Pero no todos los LSTM son iguales a los anteriores. De hecho, parece que casi todos los documentos que involucran LSTM usan una versión ligeramente diferente. Las diferencias son menores, pero vale la pena mencionar algunas de ellas.\n",
    "\n",
    "Una variante popular de LSTM, presentada por Gers & Schmidhuber , agrega \"conexiones de mirilla\". Esto significa que dejamos que las capas de la puerta miren el estado de la celda.\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png)\n",
    "\n",
    "Otra variación es utilizar puertas de entrada y de olvido acopladas. En lugar de decidir por separado qué olvidar y qué debemos agregar nueva información, tomamos esas decisiones juntos. Solo olvidamos cuando vamos a ingresar algo en su lugar. Solo ingresamos nuevos valores al estado cuando olvidamos algo más antiguo.\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png)\n",
    "\n",
    "Una variación un poco más dramática del LSTM es la Unidad Recurrente Cerrada, o GRU, presentada por Cho, et al. (2014) . Combina las puertas de entrada y de olvido en una sola \"puerta de actualización\". También fusiona el estado de la celda y el estado oculto, y realiza algunos otros cambios. El modelo resultante es más simple que los modelos LSTM estándar y se ha vuelto cada vez más popular.\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis del sentimiento\n",
    "Y ahora es el momento de ver una red neuronal recurrente en acción. Para este ejemplo, vamos a hacer algo llamado análisis de sentimiento.\n",
    "\n",
    "La definición formal de este término de Wikipedia es la siguiente:\n",
    "\n",
    "*el proceso de identificar y categorizar computacionalmente las opiniones expresadas en un texto, especialmente para determinar si la actitud del escritor hacia un tema particular, producto, etc. es positiva, negativa o neutral.*\n",
    "\n",
    "El ejemplo que utilizaremos aquí es la clasificación de las críticas de cine como positivas, negativas o neutras.\n",
    "\n",
    "*Esta guía está basada en el siguiente tutorial de tensorflow: https://www.tensorflow.org/tutorials/text/text_classification_rnn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos de críticas de películas\n",
    "Empezamos cargando el conjunto de datos de críticas de películas de IMDB de keras. Este conjunto de datos contiene 25.000 críticas de IMDB donde cada una ya está preprocesada y tiene una etiqueta como positiva o negativa. Cada crítica está codificada por enteros que representan lo común que es una palabra en todo el conjunto de datos. Por ejemplo, una palabra codificada con el número entero 3 significa que es la tercera palabra más común en el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "MAXLEN     = 250\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n",
      "17473536/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
     ]
    }
   ],
   "source": [
    "# Veamos una revisión\n",
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más preprocesamiento\n",
    "Si echamos un vistazo a algunas de nuestras revisiones cargadas, nos daremos cuenta de que tienen diferentes longitudes. Esto es un problema. No podemos pasar datos de diferente longitud a nuestra red neuronal. Por lo tanto, debemos hacer que cada reseña tenga la misma longitud. Para ello seguiremos el siguiente procedimiento\n",
    "- si la reseña tiene más de 250 palabras, se recortan las palabras sobrantes\n",
    "- si la reseña tiene menos de 250 palabras, añadir la cantidad necesaria de 0's para que sea igual a 250.\n",
    "\n",
    "Por suerte para nosotros keras tiene una función que puede hacer esto por nosotros:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data  = sequence.pad_sequences(test_data,  MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo\n",
    "Ahora es el momento de crear el modelo. Utilizaremos una capa de incrustación de palabras como primera capa de nuestro modelo y añadiremos después una capa LSTM que se alimenta de un nodo denso para obtener nuestro sentimiento predicho. \n",
    "\n",
    "32 representa la dimensión de salida de los vectores generados por la capa de incrustación. Podemos cambiar este valor si queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          2834688   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "Ahora es el momento de compilar y entrenar el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 35s 52ms/step - loss: 0.4373 - acc: 0.7983 - val_loss: 0.2946 - val_acc: 0.8846\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.2401 - acc: 0.9090 - val_loss: 0.3154 - val_acc: 0.8610\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 34s 55ms/step - loss: 0.1837 - acc: 0.9337 - val_loss: 0.2854 - val_acc: 0.8796\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.1520 - acc: 0.9452 - val_loss: 0.5194 - val_acc: 0.8248\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.1263 - acc: 0.9548 - val_loss: 0.3043 - val_acc: 0.8902\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.1100 - acc: 0.9632 - val_loss: 0.3056 - val_acc: 0.8884\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0934 - acc: 0.9685 - val_loss: 0.3248 - val_acc: 0.8812\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.0803 - acc: 0.9733 - val_loss: 0.3155 - val_acc: 0.8868\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0695 - acc: 0.9780 - val_loss: 0.5675 - val_acc: 0.8520\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0614 - acc: 0.9808 - val_loss: 0.3562 - val_acc: 0.8704\n"
     ]
    }
   ],
   "source": [
    "model.compile( loss      = \"binary_crossentropy\",\n",
    "               optimizer = \"rmsprop\",\n",
    "               metrics   = ['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y evaluaremos el modelo en nuestros datos de entrenamiento para ver su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 13ms/step - loss: 0.4405 - acc: 0.8433\n",
      "[0.4405488669872284, 0.8433200120925903]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haciendo predicciones\n",
    "Ahora vamos a utilizar nuestra red para hacer predicciones sobre nuestras propias reseñas. \n",
    "\n",
    "Como nuestras opiniones están codificadas, tenemos que convertir cualquier opinión que escribamos en ese formato para que la red pueda entenderla. Para ello, cargamos las codificaciones del conjunto de datos y las utilizamos para codificar nuestros propios datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "1654784/1641221 [==============================] - 0s 0us/step\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "# vamos a hacer una función de decodificación\n",
    "\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "\n",
    "    return text[:-1]\n",
    "  \n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred         = np.zeros((1,250))\n",
    "    pred[0]      = encoded_text\n",
    "    result       = model.predict(pred) \n",
    "    print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7291384]\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
    "predict(positive_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20502672]\n"
     ]
    }
   ],
   "source": [
    "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador de juegos RNN\n",
    "\n",
    "Ahora es el momento de uno de los ejemplos más geniales que hemos visto hasta ahora. Vamos a utilizar una RNN para generar una obra de teatro. Simplemente mostraremos a la RNN un ejemplo de algo que queremos que recree y ella aprenderá a escribir una versión de la misma por sí misma. Haremos esto utilizando un modelo de predicción de caracteres que tomará como entrada una secuencia de longitud variable y predecirá el siguiente carácter. Podemos usar el modelo muchas veces seguidas con la salida de la última predicción como entrada para la siguiente llamada para generar una secuencia.\n",
    "\n",
    "\n",
    "*Esta guía se basa en lo siguiente: https://www.tensorflow.org/tutorials/text/text_generation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Para este ejemplo, sólo necesitamos una pieza de datos de entrenamiento. De hecho, podemos escribir nuestro propio poema u obra de teatro y pasarlo a la red para el entrenamiento si queremos. Sin embargo, para facilitar las cosas, utilizaremos un extracto de una obra de teatro de Shakesphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n",
      "1130496/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para ejucuar en la colab de google\n",
    "from google.colab import files\n",
    "path_to_file = list(files.upload().keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/emi/.keras/datasets/shakespeare.txt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos donde guardo el archivo\n",
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitud del archivo: 1115394 caracteres\n"
     ]
    }
   ],
   "source": [
    "# leemos el archivo y mostramops la longitud en caracteres\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print ('longitud del archivo: {} caracteres'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imprimimos los primeros 250 caracteres\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodificación\n",
    "Como este texto no está codificado todavía, tenemos que hacerlo nosotros mismos. Vamos a codificar cada carácter único como un entero diferente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "# Creación de un mapeo de caracteres únicos a índices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
       "       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char = np.array(vocab)\n",
    "idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una funcion lambda que recibe un texto y devuelve un array con los valores en enteros)\n",
    "text_to_int = lambda text : np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, ..., 45,  8,  0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: First Citizen\n",
      "Encode: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# veamos cómo se codifica parte de nuestro texto\n",
    "print(\"Texto:\", text[:13])\n",
    "print(\"Encode:\", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y aquí haremos una función que pueda convertir nuestros valores numéricos en texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "def int_to_text(ints):\n",
    "    # probamos si ints es un tensor de tensorflow lo tranformamos a un array de numpy\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de ejemplos de entrenamiento\n",
    "Recuerde que nuestra tarea es alimentar al modelo con una secuencia y hacer que nos devuelva el siguiente carácter. Esto significa que tenemos que dividir nuestros datos de texto de arriba en muchas secuencias más cortas que podamos pasar al modelo como ejemplos de entrenamiento. \n",
    "\n",
    "Los ejemplos de entrenamiento que prepararemos utilizarán una secuencia *longitud_de_secuencia* como entrada y una secuencia *longitud_de_secuencia* como salida, donde esa secuencia es la secuencia original desplazada una letra a la derecha. Por ejemplo:\n",
    "\n",
    "``Entrada: Infierno | salida: ello```\n",
    "\n",
    "Nuestro primer paso será crear una secuencia de caracteres a partir de nuestros datos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100  # longitud de la secuencia para un ejemplo de entrenamiento\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# # Crear ejemplos de formación / objetivos\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, podemos utilizar el método de lotes para convertir este flujo de caracteres en lotes de la longitud deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que utilizar estas secuencias de longitud 101 y dividirlas en entrada y salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):  # para el primer ejemplo: hello\n",
    "    input_text  = chunk[:-1]  # hell\n",
    "    target_text = chunk[1:]  # ello\n",
    "    return input_text, target_text  # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target)  # utilizamos map para aplicar la función anterior a cada entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59  1]\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(1):\n",
    "    print(x.numpy())\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(y.numpy())\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, tenemos que hacer lotes de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Tamaño del buffer para barajar el conjunto de datos\n",
    "# (TF data está diseñado para trabajar con secuencias posiblemente infinitas,\n",
    "# por lo que no intenta barajar toda la secuencia en memoria. En su lugar,\n",
    "# mantiene un buffer en el que baraja los elementos).\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el modelo\n",
    "Ahora es el momento de construir el modelo. Utilizaremos una capa de incrustación, una LSTM y una capa densa que contiene un nodo para cada carácter único en nuestros datos de entrenamiento. La capa densa nos dará una distribución de probabilidad sobre todos los nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(vocab_size, \n",
    "                                      embedding_dim,\n",
    "                                      batch_input_shape = [batch_size, None]),\n",
    "            tf.keras.layers.LSTM(rnn_units,\n",
    "                                 # sreturn_sequences: Booleano. Si se devuelve la última salida. en la secuencia de salida, \n",
    "                                 # o la secuencia completa. Por defecto: Falso.\n",
    "                                 return_sequences      = True,\n",
    "                                 # stateful: Booleano (por defecto Falso). \n",
    "                                 # Si es True, el último estado de cada muestra de índice i en un lote se utilizará \n",
    "                                 # como estado inicial para la muestra de índice i en el siguiente lote.\n",
    "                                 stateful              = True,\n",
    "                                 # recurrent_initializer: Inicializador para la matriz de pesos de recurrent_kernel, \n",
    "                                 # utilizada para la transformación lineal del estado recurrente. Por defecto: ortogonal\n",
    "                                 recurrent_initializer = 'glorot_uniform'),\n",
    "            tf.keras.layers.Dense(vocab_size)\n",
    "                                ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de una función de pérdida\n",
    "Ahora vamos a crear nuestra propia función de pérdida para este problema. Esto se debe a que nuestro modelo producirá un tensor con forma de (64, longitud_de_secuencia, 65) que representa la distribución de probabilidad de cada carácter en cada paso de tiempo para cada secuencia del lote.   \n",
    "\n",
    "Sin embargo, antes de hacerlo, echemos un vistazo a un ejemplo de entrada y a la salida de nuestro modelo no entrenado. Esto es para que podamos entender lo que el modelo nos está dando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    # pide a nuestro modelo una predicción sobre nuestro primer lote de datos de entrenamiento (64 entradas)\n",
    "    example_batch_predictions = model(input_example_batch)  \n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[-3.12169059e-03  6.64685853e-04 -1.41554140e-03 ...  1.01210969e-03\n",
      "    8.10748525e-03  1.59163424e-03]\n",
      "  [-7.11657712e-03  1.86954718e-03 -3.77903634e-05 ...  3.40952212e-03\n",
      "    2.41800584e-03  3.94266006e-03]\n",
      "  [-4.16120281e-03  3.97378253e-03 -2.25786888e-03 ...  6.56198943e-03\n",
      "    3.02954996e-03  4.16458864e-03]\n",
      "  ...\n",
      "  [-6.92722108e-03 -3.01308464e-03  4.83981986e-03 ...  5.85018471e-03\n",
      "   -3.67472740e-03 -2.18911609e-03]\n",
      "  [-4.60554799e-03  7.14544265e-04  1.13847316e-03 ...  2.15948443e-03\n",
      "    1.29446713e-03  1.05620315e-03]\n",
      "  [ 3.61024926e-04  2.83821439e-03  1.90595165e-05 ...  3.26792034e-03\n",
      "   -4.66392934e-03  3.79081373e-03]]\n",
      "\n",
      " [[ 1.81365060e-04 -8.49255826e-04  1.62916854e-02 ... -6.68749213e-03\n",
      "   -6.36514416e-03 -1.14551960e-02]\n",
      "  [ 1.54794916e-03  1.39831216e-03  1.63167082e-02 ... -2.22772593e-03\n",
      "   -8.82198103e-03 -2.06356961e-03]\n",
      "  [-3.22013395e-03  7.26652099e-03  1.60768647e-02 ... -1.58064463e-03\n",
      "   -3.37851094e-03 -5.82813472e-03]\n",
      "  ...\n",
      "  [-9.97425243e-03  1.04191713e-02  3.54056573e-03 ...  8.47379444e-04\n",
      "   -4.22164914e-04 -1.13726920e-03]\n",
      "  [-8.45036376e-03  1.06852958e-02  2.72673508e-03 ... -3.47187091e-03\n",
      "    3.52387410e-03 -2.56070774e-03]\n",
      "  [-7.36896787e-03  9.27761197e-03  1.80939632e-03 ... -6.77374564e-03\n",
      "    7.12292315e-03 -3.47719900e-03]]\n",
      "\n",
      " [[ 1.68981764e-03  1.21153574e-02  8.78932513e-03 ... -5.84992813e-04\n",
      "    2.34845420e-03 -8.59444868e-03]\n",
      "  [-8.24252260e-04  1.02680027e-02  3.90740950e-03 ... -1.85707177e-04\n",
      "   -1.56583043e-03 -4.97061433e-03]\n",
      "  [-6.54388731e-03  8.20847601e-03  5.00994921e-03 ...  1.85796164e-03\n",
      "   -5.25720511e-03 -1.80968200e-03]\n",
      "  ...\n",
      "  [-6.24287594e-03  1.04052294e-02  4.47362568e-03 ...  3.39232781e-03\n",
      "   -1.24471472e-03 -3.14673688e-03]\n",
      "  [-9.83043760e-03  9.87477042e-03  5.10301068e-03 ...  5.40488819e-03\n",
      "   -4.43855813e-03 -2.77005462e-03]\n",
      "  [-4.83075157e-03  5.16760815e-03  1.17337629e-02 ...  1.41526177e-03\n",
      "   -7.42727192e-03 -8.13351292e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.29963085e-02  6.26425305e-03  7.72459479e-03 ...  8.16564448e-03\n",
      "   -1.58083588e-02 -6.06904272e-04]\n",
      "  [-1.58142559e-02  1.06125521e-02  6.54110312e-03 ...  7.83217326e-03\n",
      "   -1.53547097e-02 -2.73203244e-03]\n",
      "  [-1.51261147e-02  1.07594570e-02  8.81336257e-03 ...  6.02002256e-05\n",
      "   -1.46163655e-02  3.97982541e-04]\n",
      "  ...\n",
      "  [-6.77649816e-03  7.38571491e-03 -4.77026217e-04 ...  3.51876079e-04\n",
      "    4.33114031e-03  4.47235070e-03]\n",
      "  [-6.31198613e-03  7.20657781e-03 -4.08149231e-03 ...  1.77254691e-03\n",
      "    1.42893335e-03  5.13955718e-03]\n",
      "  [-9.88071878e-03  6.36490714e-03 -1.93719170e-03 ...  4.35844297e-03\n",
      "   -1.90771976e-03  5.99354226e-03]]\n",
      "\n",
      " [[-8.40793364e-03  8.12794268e-03 -2.04044569e-04 ...  3.88553762e-03\n",
      "   -1.82959170e-03 -1.13835046e-02]\n",
      "  [-7.46557536e-03  8.94986838e-03  3.26945446e-04 ... -1.15652231e-03\n",
      "    3.16988071e-03 -9.30567831e-03]\n",
      "  [-8.28083046e-03  6.15786575e-03 -1.25504390e-03 ... -3.45457951e-03\n",
      "    3.47656803e-03 -4.17911448e-03]\n",
      "  ...\n",
      "  [-1.17576765e-02  5.03440760e-03  9.48385801e-03 ... -2.52144225e-03\n",
      "    9.77638410e-04 -3.65612819e-03]\n",
      "  [-9.52866022e-03  7.77416490e-03  8.88594892e-03 ... -5.37839718e-03\n",
      "    5.51512884e-03 -9.75468010e-03]\n",
      "  [-6.64665503e-03  6.72768941e-03  6.86174445e-03 ... -5.73784485e-03\n",
      "    3.97481676e-03 -1.37310307e-02]]\n",
      "\n",
      " [[-1.51673099e-03  1.21526327e-02 -6.59992453e-04 ... -8.65446962e-03\n",
      "    1.01053575e-02 -1.91715267e-02]\n",
      "  [-2.48919940e-04  9.30370670e-03  4.54870146e-03 ... -5.38597815e-03\n",
      "    1.08396122e-02 -9.21269692e-03]\n",
      "  [ 6.36854488e-03  1.04631148e-02  7.39032915e-03 ... -4.29952005e-03\n",
      "    4.11138684e-03 -5.28236991e-03]\n",
      "  ...\n",
      "  [ 2.33001891e-03  1.18362615e-02 -4.98676905e-04 ...  6.21442217e-03\n",
      "    4.04954702e-03  3.25725810e-03]\n",
      "  [ 7.29307707e-04  1.25443712e-02 -2.24991818e-04 ...  2.24925624e-03\n",
      "    7.16662593e-03 -3.03846132e-03]\n",
      "  [-1.01763359e-03  1.18318060e-02  2.53598299e-03 ... -5.59672620e-03\n",
      "    2.53930315e-03 -9.80873127e-04]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Podemos ver que la predicción es un array de 64 arrays, uno por cada entrada del lote\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-3.1216906e-03  6.6468585e-04 -1.4155414e-03 ...  1.0121097e-03\n",
      "   8.1074852e-03  1.5916342e-03]\n",
      " [-7.1165771e-03  1.8695472e-03 -3.7790363e-05 ...  3.4095221e-03\n",
      "   2.4180058e-03  3.9426601e-03]\n",
      " [-4.1612028e-03  3.9737825e-03 -2.2578689e-03 ...  6.5619894e-03\n",
      "   3.0295500e-03  4.1645886e-03]\n",
      " ...\n",
      " [-6.9272211e-03 -3.0130846e-03  4.8398199e-03 ...  5.8501847e-03\n",
      "  -3.6747274e-03 -2.1891161e-03]\n",
      " [-4.6055480e-03  7.1454427e-04  1.1384732e-03 ...  2.1594844e-03\n",
      "   1.2944671e-03  1.0562032e-03]\n",
      " [ 3.6102493e-04  2.8382144e-03  1.9059516e-05 ...  3.2679203e-03\n",
      "  -4.6639293e-03  3.7908137e-03]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# examinemos una predicción\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "# observe que esta es una matriz 2d de longitud 100, donde cada matriz interior es la predicción para el siguiente carácter en cada paso de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-0.00312169  0.00066469 -0.00141554 -0.00551986  0.00704249  0.01112877\n",
      " -0.00523458  0.00175228  0.00127487 -0.00229272 -0.01128815  0.00097445\n",
      " -0.00437749 -0.00355143  0.00077764 -0.00708889 -0.00251785 -0.00164459\n",
      "  0.00573261 -0.00639464  0.00461333 -0.00145566 -0.0065794   0.00724377\n",
      "  0.00153553  0.00665463  0.00233206 -0.00682848 -0.00813593  0.01568121\n",
      "  0.00250032  0.00017275 -0.00796703  0.00607542 -0.00515135 -0.00674238\n",
      "  0.0146931  -0.00952782 -0.01442668  0.00839407  0.00714136  0.00648044\n",
      "  0.00127929 -0.00141731 -0.00330456  0.00417646  0.0023957   0.00606689\n",
      "  0.00856928  0.01402924  0.00154698  0.00868561 -0.00942474 -0.01210114\n",
      " -0.01619386  0.00139606  0.01041866  0.01015501  0.00956712  0.00808504\n",
      " -0.00159678  0.00201729  0.00101211  0.00810749  0.00159163], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# y finalmente veremos una predicción en el primer paso de tiempo\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# y  por supuesto, sus 65 valores que representan la probabilidad de que cada carácter ocurra a continuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-$iMt;KMZRDAJf!ZX OdUPQ.mqotf?UQguxkVtzT-WY$pZEpX.TV.WUTMljTh!qUfrfZoV'\\nKw-U:H,NUl-FRSPWaSeAgh:C3.hv\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si queremos determinar el carácter predicho, necesitamos muestrear la distribución de salida (elegir un valor basado en la probabilidad)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# ahora podemos remodelar esa matriz y convertir todos los enteros en números para ver los caracteres reales\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars  # y esto es lo que predijo el modelo para la secuencia de entrenamiento 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que ahora tenemos que crear una función de pérdida que pueda comparar esa salida con la salida esperada y darnos algún valor numérico que represente lo cerca que estaban las dos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilación del modelo\n",
    "En este punto podemos pensar en nuestro problema como un problema de clasificación en el que el modelo predice la probabilidad de que cada letra única sea la siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de puntos de control (checkpoints)\n",
    "Ahora vamos a configurar nuestro modelo para que guarde los puntos de control mientras se entrena. Esto nos permitirá cargar nuestro modelo desde un punto de control y continuar entrenándolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Nombre del checkpoint\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                filepath          = checkpoint_prefix,\n",
    "                                save_weights_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "Por último, empezaremos a entrenar el modelo. \n",
    "Puede correr en una colab con GPU  \n",
    "**Si esto tarda un poco vaya a Runtime > Change Runtime Type y elija \"GPU\" en acelerador de hardware.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "172/172 [==============================] - 687s 4s/step - loss: 2.6396\n",
      "Epoch 2/50\n",
      "172/172 [==============================] - 643s 4s/step - loss: 1.9299\n",
      "Epoch 3/50\n",
      "172/172 [==============================] - 610s 4s/step - loss: 1.6740\n",
      "Epoch 4/50\n",
      "172/172 [==============================] - 615s 4s/step - loss: 1.5307\n",
      "Epoch 5/50\n",
      "172/172 [==============================] - 623s 4s/step - loss: 1.4443\n",
      "Epoch 6/50\n",
      "172/172 [==============================] - 609s 4s/step - loss: 1.3868\n",
      "Epoch 7/50\n",
      "172/172 [==============================] - 606s 4s/step - loss: 1.3417\n",
      "Epoch 8/50\n",
      "172/172 [==============================] - 598s 3s/step - loss: 1.3037\n",
      "Epoch 9/50\n",
      "172/172 [==============================] - 602s 3s/step - loss: 1.2685\n",
      "Epoch 10/50\n",
      "172/172 [==============================] - 603s 3s/step - loss: 1.2353\n",
      "Epoch 11/50\n",
      "172/172 [==============================] - 626s 4s/step - loss: 1.2026\n",
      "Epoch 12/50\n",
      "172/172 [==============================] - 621s 4s/step - loss: 1.1695\n",
      "Epoch 13/50\n",
      "172/172 [==============================] - 609s 4s/step - loss: 1.1364\n",
      "Epoch 14/50\n",
      "172/172 [==============================] - 619s 4s/step - loss: 1.1016\n",
      "Epoch 15/50\n",
      "172/172 [==============================] - 600s 3s/step - loss: 1.0650\n",
      "Epoch 16/50\n",
      "172/172 [==============================] - 625s 4s/step - loss: 1.0277\n",
      "Epoch 17/50\n",
      "172/172 [==============================] - 610s 4s/step - loss: 0.9885\n",
      "Epoch 18/50\n",
      "172/172 [==============================] - 632s 4s/step - loss: 0.9499\n",
      "Epoch 19/50\n",
      "172/172 [==============================] - 625s 4s/step - loss: 0.9125\n",
      "Epoch 20/50\n",
      "172/172 [==============================] - 608s 4s/step - loss: 0.8737\n",
      "Epoch 21/50\n",
      "172/172 [==============================] - 628s 4s/step - loss: 0.8365\n",
      "Epoch 22/50\n",
      "172/172 [==============================] - 595s 3s/step - loss: 0.8006\n",
      "Epoch 23/50\n",
      "172/172 [==============================] - 594s 3s/step - loss: 0.7671\n",
      "Epoch 24/50\n",
      "172/172 [==============================] - 601s 3s/step - loss: 0.7356\n",
      "Epoch 25/50\n",
      "172/172 [==============================] - 599s 3s/step - loss: 0.7055\n",
      "Epoch 26/50\n",
      "172/172 [==============================] - 594s 3s/step - loss: 0.6786\n",
      "Epoch 27/50\n",
      "172/172 [==============================] - 607s 4s/step - loss: 0.6543\n",
      "Epoch 28/50\n",
      "172/172 [==============================] - 630s 4s/step - loss: 0.6324\n",
      "Epoch 29/50\n",
      "172/172 [==============================] - 634s 4s/step - loss: 0.6103\n",
      "Epoch 30/50\n",
      "172/172 [==============================] - 633s 4s/step - loss: 0.5923\n",
      "Epoch 31/50\n",
      "172/172 [==============================] - 609s 4s/step - loss: 0.5745\n",
      "Epoch 32/50\n",
      "172/172 [==============================] - 629s 4s/step - loss: 0.5581\n",
      "Epoch 33/50\n",
      "172/172 [==============================] - 616s 4s/step - loss: 0.5440\n",
      "Epoch 34/50\n",
      "172/172 [==============================] - 624s 4s/step - loss: 0.5324\n",
      "Epoch 35/50\n",
      "172/172 [==============================] - 598s 3s/step - loss: 0.5201\n",
      "Epoch 36/50\n",
      "172/172 [==============================] - 634s 4s/step - loss: 0.5102\n",
      "Epoch 37/50\n",
      "172/172 [==============================] - 622s 4s/step - loss: 0.5004\n",
      "Epoch 38/50\n",
      "172/172 [==============================] - 618s 4s/step - loss: 0.4929\n",
      "Epoch 39/50\n",
      "172/172 [==============================] - 623s 4s/step - loss: 0.4855\n",
      "Epoch 40/50\n",
      "172/172 [==============================] - 640s 4s/step - loss: 0.4757\n",
      "Epoch 41/50\n",
      "172/172 [==============================] - 775s 4s/step - loss: 0.4691\n",
      "Epoch 42/50\n",
      "172/172 [==============================] - 660s 4s/step - loss: 0.4639\n",
      "Epoch 43/50\n",
      "172/172 [==============================] - 597s 3s/step - loss: 0.4585\n",
      "Epoch 44/50\n",
      "172/172 [==============================] - 590s 3s/step - loss: 0.4539\n",
      "Epoch 45/50\n",
      "172/172 [==============================] - 587s 3s/step - loss: 0.4475\n",
      "Epoch 46/50\n",
      "172/172 [==============================] - 591s 3s/step - loss: 0.4434\n",
      "Epoch 47/50\n",
      "172/172 [==============================] - 654s 4s/step - loss: 0.4393\n",
      "Epoch 48/50\n",
      "172/172 [==============================] - 708s 4s/step - loss: 0.4360\n",
      "Epoch 49/50\n",
      "172/172 [==============================] - 735s 4s/step - loss: 0.4342\n",
      "Epoch 50/50\n",
      "172/172 [==============================] - 743s 4s/step - loss: 0.4299\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "Reconstruiremos el modelo a partir de un punto de control utilizando un batch_size de 1 para que podamos alimentar un trozo de texto al modelo y que éste haga una predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que el modelo ha terminado de entrenar, podemos encontrar el **último punto de control** que almacena los pesos del modelo utilizando la siguiente línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de texto\n",
    "Ahora podemos utilizar la encantadora función proporcionada por tensorflow para generar algo de texto utilizando cualquier cadena de inicio que queramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Paso de evaluación (generar texto usando el modelo aprendido)\n",
    "    # Número de caracteres a generar\n",
    "    num_generate = 800\n",
    "\n",
    "    # Convertir nuestra cadena de inicio en números (vectorización)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # lista vacia para texto generado\n",
    "    text_generated = []\n",
    "\n",
    "    # Las temperaturas bajas dan como resultado un texto más predecible.\n",
    "    # Las temperaturas más altas dan como resultado un texto más sorprendente.\n",
    "    # Experimente para encontrar la mejor configuración.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # eliminar la dimensión del lote\n",
    "    \n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # utilizando una distribución categórica para predecir el carácter devuelto por el modelo\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pasamos el carácter predicho como la siguiente entrada al modelo\n",
    "        # junto con el estado oculto anterior\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type a starting string:  be or not to be\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be or not to be done:\n",
      "Shall, what most I, being over-proud intentle proud;\n",
      "For by the English personall pluck\n",
      "Ejoubted our law cameling; though we would wish\n",
      "Your country's brown, a ring form on's blood, be it thankil,\n",
      "That young Prince Edward marries Warwick's daughter.\n",
      "\n",
      "KING EDWARD IV:\n",
      "An oack, to say to one immaker: shall I send\n",
      "Dis this for ill scope be wakent good awhile! would I,\n",
      "Take thou on every time what should not\n",
      "beat you that act of it?\n",
      "\n",
      "Second Capulet:\n",
      "'Sicil the other, there it is berefactors\n",
      "Are crack'd for't: both you, as I hear, must I be glass,\n",
      "I then cast out again and that sayst do in piench my mother,\n",
      "How doth the portion and suffer looking sla, whereof thy love, or or?\n",
      "I might commend me to thy living lior\n",
      "Is as a man divine and myself,\n",
      "And not against his eased before all hearts b\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes\n",
    "\n",
    "1. “Text Classification with an RNN &nbsp;: &nbsp; TensorFlow Core.” TensorFlow, www.tensorflow.org/tutorials/text/text_classification_rnn.\n",
    "2. “Text Generation with an RNN &nbsp;: &nbsp; TensorFlow Core.” TensorFlow, www.tensorflow.org/tutorials/text/text_generation.\n",
    "3. “Understanding LSTM Networks.” Understanding LSTM Networks -- Colah's Blog, https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n",
    "4. Chollet François. Deep Learning with Python. Manning Publications Co., 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
